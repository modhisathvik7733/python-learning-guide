{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Matrix Plots in Seaborn\n",
    "\n",
    "## Goal\n",
    "Learn matrix-style visualizations:\n",
    "- **Heatmaps** for correlation matrices and confusion matrices\n",
    "- **Clustermaps** for hierarchical clustering\n",
    "- **Annotations + masking** (e.g., hide upper triangle for correlations)\n",
    "- **Centered diverging colormaps** for values like correlations (\u22121 to 1)\n",
    "- **Robust color scaling** to reduce sensitivity to outliers\n",
    "\n",
    "## Quick cheat sheet\n",
    "| Task | Function | Key params |\n",
    "|---|---|---|\n",
    "| Correlation matrix | `sns.heatmap()` | `annot`, `fmt`, `cmap`, `center`, `vmin/vmax`, `mask` |\n",
    "| Confusion matrix | `sns.heatmap()` | `annot`, `fmt`, `cmap`, `cbar=False`, axis labels |\n",
    "| Hierarchical clustering | `sns.clustermap()` | `standard_scale`, `z_score`, `method`, `metric`, `cmap` |\n",
    "| Mask upper triangle | `mask=np.triu(...)` | Combine with `center=0` for correlations |\n",
    "| Robust color scale | `robust=True` | Uses robust quantiles for color limits |\n",
    "\n",
    "## When to use what\n",
    "- Use **heatmap** when row/column order is meaningful or fixed.\n",
    "- Use **clustermap** when you want to discover **groups** of similar rows/columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style='white', context='notebook')\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Synthetic dataset (real-world style): e-commerce KPIs\n",
    "We will generate a dataset that looks like weekly e-commerce performance:\n",
    "- Acquisition: `ad_spend`, `sessions`\n",
    "- Funnel: `add_to_cart_rate`, `checkout_rate`, `conversion_rate`\n",
    "- Business outcomes: `orders`, `aov` (avg order value), `revenue`, `return_rate`\n",
    "\n",
    "This dataset is synthetic but designed to have plausible relationships:\n",
    "- More spend \u2192 more sessions\n",
    "- Better funnel rates \u2192 higher conversion \u2192 more orders \u2192 more revenue\n",
    "- Very high return rate reduces net revenue quality\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a synthetic e-commerce KPI dataset (weekly)\n",
    "n = 120  # weeks\n",
    "\n",
    "# Marketing + traffic\n",
    "ad_spend = np.random.lognormal(mean=9.0, sigma=0.35, size=n)  # in currency units\n",
    "sessions = (ad_spend / 20 + np.random.normal(0, 1200, size=n)).clip(2000, None)\n",
    "\n",
    "# Funnel rates (bounded 0..1)\n",
    "add_to_cart_rate = np.clip(0.06 + 0.000002 * sessions + np.random.normal(0, 0.01, n), 0.02, 0.25)\n",
    "checkout_rate = np.clip(0.35 + 0.25 * add_to_cart_rate + np.random.normal(0, 0.03, n), 0.10, 0.80)\n",
    "conversion_rate = np.clip(0.008 + 0.045 * add_to_cart_rate + 0.020 * checkout_rate + np.random.normal(0, 0.003, n), 0.002, 0.08)\n",
    "\n",
    "# Outcomes\n",
    "orders = (sessions * conversion_rate + np.random.normal(0, 25, n)).clip(10, None)\n",
    "\n",
    "# Average order value influenced by discounting (not modeled explicitly) + some noise\n",
    "# (add some correlation with checkout_rate as a proxy for checkout UX)\n",
    "aov = np.clip(900 + 600 * checkout_rate + np.random.normal(0, 120, n), 300, 2500)\n",
    "\n",
    "revenue = orders * aov\n",
    "\n",
    "# Return rate: mildly correlated with higher AOV + random seasonality\n",
    "season = np.sin(np.linspace(0, 6*np.pi, n))\n",
    "return_rate = np.clip(0.10 + 0.00002 * (aov - 900) + 0.03 * (season > 0) + np.random.normal(0, 0.02, n), 0.02, 0.35)\n",
    "\n",
    "# Add a few extreme outliers to demonstrate robust scaling\n",
    "outlier_weeks = np.random.choice(np.arange(n), size=4, replace=False)\n",
    "revenue[outlier_weeks] *= np.random.uniform(2.5, 4.5, size=4)\n",
    "\n",
    "kpi = pd.DataFrame({\n",
    "    'ad_spend': ad_spend,\n",
    "    'sessions': sessions,\n",
    "    'add_to_cart_rate': add_to_cart_rate,\n",
    "    'checkout_rate': checkout_rate,\n",
    "    'conversion_rate': conversion_rate,\n",
    "    'orders': orders,\n",
    "    'aov': aov,\n",
    "    'revenue': revenue,\n",
    "    'return_rate': return_rate,\n",
    "})\n",
    "\n",
    "kpi.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Basic checks\n",
    "print(kpi.shape)\n",
    "print(kpi.describe().round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Heatmaps: correlation matrix\n",
    "A correlation heatmap summarizes how variables move together.\n",
    "\n",
    "### Best practices for correlations\n",
    "- Use a **diverging colormap** centered at 0.\n",
    "- Set `vmin=-1` and `vmax=1` for correlations.\n",
    "- Use **masking** to show only the lower triangle (avoids duplicate info).\n",
    "- `annot=True` is great for small matrices; for large matrices it can be cluttered.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr = kpi.corr(numeric_only=True)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    cmap='vlag',\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    linewidths=0.5\n",
    ")\n",
    "plt.title('Correlation heatmap (annotated, centered at 0)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking the upper triangle\n",
    "Masking makes correlation heatmaps cleaner by removing mirrored duplicates.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Mask upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    mask=mask,\n",
    "    cmap='vlag',\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    linewidths=0.5,\n",
    "    square=True\n",
    ")\n",
    "plt.title('Correlation heatmap (lower triangle only)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Center & robust scaling\n",
    "\n",
    "### `center=0`\n",
    "For diverging data like correlations, set `center=0` so:\n",
    "- negative values map to one side (e.g., blue)\n",
    "- positive values map to the other side (e.g., red)\n",
    "\n",
    "### `robust=True`\n",
    "If a matrix contains extreme values/outliers, `robust=True` uses robust quantiles to set color limits.\n",
    "\n",
    "In our synthetic data, we injected outlier weeks in revenue; this can skew distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compare robust vs non-robust on a covariance-like matrix\n",
    "# Create a \"scaled revenue\" feature to amplify outlier impact\n",
    "kpi2 = kpi.copy()\n",
    "kpi2['revenue_scaled'] = kpi2['revenue'] / 1e6\n",
    "\n",
    "corr2 = kpi2[['ad_spend','sessions','orders','aov','revenue_scaled','return_rate']].corr()\n",
    "mask2 = np.triu(np.ones_like(corr2, dtype=bool))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6), constrained_layout=True)\n",
    "\n",
    "sns.heatmap(\n",
    "    corr2,\n",
    "    mask=mask2,\n",
    "    cmap='vlag',\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title('Correlation (fixed vmin/vmax)')\n",
    "\n",
    "sns.heatmap(\n",
    "    corr2,\n",
    "    mask=mask2,\n",
    "    cmap='vlag',\n",
    "    center=0,\n",
    "    robust=True,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title('Correlation (robust=True)')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Heatmaps: confusion matrix (classification)\n",
    "A confusion matrix is common in real-world ML evaluation (e.g., churn prediction, fraud detection).\n",
    "\n",
    "### Example scenario\n",
    "A model predicts whether a customer will **churn** (`1`) or not (`0`).\n",
    "\n",
    "We will create synthetic ground-truth labels and predicted labels, then plot a confusion matrix.\n",
    "\n",
    "Tip: For confusion matrices, a **sequential colormap** (e.g., `Blues`) is usually better than diverging.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Synthetic churn labels\n",
    "m = 1200\n",
    "true_churn = (np.random.rand(m) < 0.22).astype(int)  # 22% churn\n",
    "\n",
    "# A model with some skill: higher recall than random, but not perfect\n",
    "# Generate predicted probabilities with some separation\n",
    "scores = np.where(true_churn == 1,\n",
    "                  np.random.beta(4, 3, size=m),   # churners have higher scores\n",
    "                  np.random.beta(2, 6, size=m))   # non-churners have lower scores\n",
    "\n",
    "threshold = 0.45\n",
    "pred_churn = (scores >= threshold).astype(int)\n",
    "\n",
    "# Confusion matrix counts\n",
    "# rows = true, cols = predicted\n",
    "cm = pd.crosstab(\n",
    "    pd.Series(true_churn, name='True'),\n",
    "    pd.Series(pred_churn, name='Predicted')\n",
    ")\n",
    "\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot confusion matrix with annotations\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    cbar=False,\n",
    "    linewidths=0.5\n",
    ")\n",
    "plt.title('Confusion matrix heatmap (synthetic churn)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Clustermaps (`clustermap`)\n",
    "`clustermap()` performs hierarchical clustering and reorders rows/columns to group similar patterns.\n",
    "\n",
    "### When to use\n",
    "- Feature correlation blocks\n",
    "- Customer/product cohorts (rows) vs behaviors (columns)\n",
    "\n",
    "### Key parameters\n",
    "- `standard_scale=1`: standardize columns (good when features are on different scales)\n",
    "- `z_score=0`: z-score rows\n",
    "- `method`: linkage method (`'average'`, `'complete'`, `'ward'`)\n",
    "- `metric`: distance metric (`'euclidean'`, `'correlation'`)\n",
    "\n",
    "Important: `clustermap` creates its own figure (similar to `lmplot`).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a cohort-style dataset: product categories vs weekly metrics\n",
    "categories = ['Electronics', 'Fashion', 'Home', 'Beauty', 'Sports', 'Books']\n",
    "weeks = pd.date_range('2024-01-01', periods=24, freq='W')\n",
    "\n",
    "rows = []\n",
    "for cat in categories:\n",
    "    cat_factor = {\n",
    "        'Electronics': (1.3, 1.15),\n",
    "        'Fashion': (1.0, 0.95),\n",
    "        'Home': (0.9, 1.05),\n",
    "        'Beauty': (0.85, 1.10),\n",
    "        'Sports': (1.05, 1.00),\n",
    "        'Books': (0.7, 0.90),\n",
    "    }[cat]\n",
    "    spend_factor, conv_factor = cat_factor\n",
    "\n",
    "    for w in weeks:\n",
    "        base_spend = np.random.lognormal(8.5, 0.25) * spend_factor\n",
    "        base_sessions = (base_spend / 18 + np.random.normal(0, 900)).clip(1500, None)\n",
    "        base_conv = np.clip(np.random.normal(0.018, 0.004) * conv_factor, 0.005, 0.05)\n",
    "        base_aov = np.clip(np.random.normal(1200, 220) * (1.15 if cat == 'Electronics' else 0.95), 300, 3500)\n",
    "        base_orders = base_sessions * base_conv\n",
    "        base_revenue = base_orders * base_aov\n",
    "\n",
    "        rows.append({\n",
    "            'category': cat,\n",
    "            'week': w,\n",
    "            'ad_spend': base_spend,\n",
    "            'sessions': base_sessions,\n",
    "            'conversion_rate': base_conv,\n",
    "            'aov': base_aov,\n",
    "            'revenue': base_revenue,\n",
    "        })\n",
    "\n",
    "cohort = pd.DataFrame(rows)\n",
    "\n",
    "# Aggregate by category (mean over weeks)\n",
    "cohort_mean = cohort.groupby('category')[['ad_spend','sessions','conversion_rate','aov','revenue']].mean()\n",
    "cohort_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Clustermap on category-level KPIs\n",
    "# standard_scale=1 standardizes each column so clustering isn't dominated by high-magnitude features like revenue\n",
    "sns.clustermap(\n",
    "    cohort_mean,\n",
    "    standard_scale=1,\n",
    "    method='average',\n",
    "    metric='euclidean',\n",
    "    cmap='mako',\n",
    "    linewidths=0.5,\n",
    "    figsize=(8, 6)\n",
    ")\n",
    "plt.suptitle('Clustermap: product category KPIs (standardized by column)', y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice set\n",
    "Use `kpi` and `cohort_mean` from above.\n",
    "\n",
    "### A) Correlation heatmaps\n",
    "1. Create a correlation matrix heatmap for `kpi` with `annot=False` and `linewidths=0.3`.\n",
    "2. Mask the upper triangle.\n",
    "3. Try a different diverging colormap (e.g., `coolwarm`) and compare.\n",
    "\n",
    "### B) Confusion matrix\n",
    "4. Change the churn threshold to `0.55`. Recompute the confusion matrix and replot.\n",
    "5. Compute and print precision and recall from the confusion matrix (challenge).\n",
    "\n",
    "### C) Clustering\n",
    "6. Try `metric='correlation'` and compare clusters.\n",
    "7. Try `method='complete'`.\n",
    "\n",
    "### Challenge\n",
    "8. Write a function `plot_corr_triangle(df)` that:\n",
    "   - computes correlations\n",
    "   - masks the upper triangle\n",
    "   - plots a centered diverging heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Practice template (fill TODOs)\n",
    "\n",
    "# TODO A1-A3\n",
    "\n",
    "# TODO B4\n",
    "\n",
    "# TODO B5 (challenge): precision, recall\n",
    "\n",
    "# TODO C6-C7\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "def plot_corr_triangle(df: pd.DataFrame, **kwargs: Any):\n",
    "    # TODO: implement masked correlation heatmap\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference links\n",
    "- Seaborn heatmap API: https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
    "- Seaborn clustermap API: https://seaborn.pydata.org/generated/seaborn.clustermap.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}