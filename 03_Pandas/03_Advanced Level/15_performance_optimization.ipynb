{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcd15e5b",
   "metadata": {},
   "source": [
    "# Performance Optimization in Pandas\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Performance Optimization** = Making Pandas operations faster and more memory-efficient\n",
    "\n",
    "### Why Optimize?\n",
    "\n",
    "```\n",
    "‚ùå Slow & Memory-Hungry:\n",
    "- Minutes to hours for operations\n",
    "- Out of memory errors\n",
    "- Poor user experience\n",
    "\n",
    "‚úÖ Optimized:\n",
    "- Seconds instead of minutes\n",
    "- 50-90% less memory\n",
    "- Handle larger datasets\n",
    "- Better scalability\n",
    "```\n",
    "\n",
    "### What We'll Learn\n",
    "\n",
    "**1. Memory Optimization** üíæ\n",
    "- Efficient data types (int8 vs int64)\n",
    "- Categorical data\n",
    "- Memory profiling\n",
    "- Reducing DataFrame size\n",
    "\n",
    "**2. Vectorization** ‚ö°\n",
    "- Avoid loops (apply vs vectorized)\n",
    "- NumPy operations\n",
    "- Built-in methods\n",
    "- When to use what\n",
    "\n",
    "**3. Efficient Operations** üöÄ\n",
    "- Query vs boolean indexing\n",
    "- eval() for expressions\n",
    "- Efficient groupby\n",
    "- Index optimization\n",
    "\n",
    "**4. Large Data Handling** üìä\n",
    "- Chunk processing\n",
    "- Column selection (usecols)\n",
    "- Data type specification\n",
    "- Sampling strategies\n",
    "\n",
    "**5. Advanced Techniques** üéØ\n",
    "- Parallel processing\n",
    "- Numba acceleration\n",
    "- Sparse data structures\n",
    "- Copy vs inplace\n",
    "\n",
    "### Performance Principles\n",
    "\n",
    "```\n",
    "1. Vectorize > Apply > Loop\n",
    "2. Right data types = Less memory\n",
    "3. Sorted index = Fast selection\n",
    "4. Read only what you need\n",
    "5. Process in chunks if needed\n",
    "```\n",
    "\n",
    "### Speed Comparison (Typical)\n",
    "\n",
    "```\n",
    "Vectorized:      0.001s  ‚ö°‚ö°‚ö° (1000x faster)\n",
    "Built-in apply:  0.1s    ‚ö°‚ö°\n",
    "Custom apply:    1s      ‚ö°\n",
    "For loop:        10s     üêå (10,000x slower!)\n",
    "```\n",
    "\n",
    "### What You'll Master\n",
    "\n",
    "1. ‚úÖ Memory profiling and optimization\n",
    "2. ‚úÖ Choosing optimal data types\n",
    "3. ‚úÖ Vectorization techniques\n",
    "4. ‚úÖ Efficient filtering and selection\n",
    "5. ‚úÖ Large file reading strategies\n",
    "6. ‚úÖ Chunked processing\n",
    "7. ‚úÖ Index optimization\n",
    "8. ‚úÖ Parallel processing basics\n",
    "9. ‚úÖ Performance measurement\n",
    "10. ‚úÖ Best practices for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76f8681a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported\n",
      "Pandas version: 2.2.3\n",
      "NumPy version: 2.1.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from functools import wraps\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "# Timer decorator for measuring performance\n",
    "def timer(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(f\"{func.__name__}: {end - start:.4f}s\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e7fc61",
   "metadata": {},
   "source": [
    "## 1. Memory Optimization\n",
    "\n",
    "### Data Type Sizes\n",
    "\n",
    "```python\n",
    "# Integer types\n",
    "int8:    -128 to 127                  (1 byte)\n",
    "int16:   -32,768 to 32,767            (2 bytes)\n",
    "int32:   -2B to 2B                    (4 bytes)\n",
    "int64:   -9 quintillion to 9 quintillion (8 bytes) ‚Üê Default\n",
    "\n",
    "# Float types\n",
    "float16: 5 digits precision           (2 bytes)\n",
    "float32: 7 digits precision           (4 bytes)\n",
    "float64: 15 digits precision          (8 bytes) ‚Üê Default\n",
    "\n",
    "# Other\n",
    "bool:    True/False                   (1 byte)\n",
    "category: Depends on # unique values  (Usually 1-2 bytes + overhead)\n",
    "```\n",
    "\n",
    "### Automatic Downcasting\n",
    "\n",
    "```python\n",
    "def optimize_dtypes(df):\n",
    "    # Integers\n",
    "    for col in df.select_dtypes(include=['int']).columns:\n",
    "        col_min = df[col].min()\n",
    "        col_max = df[col].max()\n",
    "        \n",
    "        if col_min >= 0:\n",
    "            if col_max < 255:\n",
    "                df[col] = df[col].astype('uint8')\n",
    "            elif col_max < 65535:\n",
    "                df[col] = df[col].astype('uint16')\n",
    "            elif col_max < 4294967295:\n",
    "                df[col] = df[col].astype('uint32')\n",
    "        else:\n",
    "            if col_min > -128 and col_max < 127:\n",
    "                df[col] = df[col].astype('int8')\n",
    "            elif col_min > -32768 and col_max < 32767:\n",
    "                df[col] = df[col].astype('int16')\n",
    "            elif col_min > -2147483648 and col_max < 2147483647:\n",
    "                df[col] = df[col].astype('int32')\n",
    "    \n",
    "    # Floats\n",
    "    for col in df.select_dtypes(include=['float']).columns:\n",
    "        df[col] = df[col].astype('float32')\n",
    "    \n",
    "    return df\n",
    "```\n",
    "\n",
    "### Memory Profiling\n",
    "\n",
    "```python\n",
    "# Check memory usage\n",
    "df.info(memory_usage='deep')  # Detailed memory info\n",
    "df.memory_usage(deep=True)    # Per column\n",
    "\n",
    "# Total memory\n",
    "df.memory_usage(deep=True).sum() / 1024**2  # MB\n",
    "```\n",
    "\n",
    "### Categorical Conversion\n",
    "\n",
    "```python\n",
    "# When to use categorical\n",
    "unique_ratio = df['col'].nunique() / len(df)\n",
    "\n",
    "if unique_ratio < 0.5:  # Less than 50% unique\n",
    "    df['col'] = df['col'].astype('category')\n",
    "```\n",
    "\n",
    "### Memory Savings Example\n",
    "\n",
    "```\n",
    "Before optimization:\n",
    "int64:    8 bytes √ó 1M rows = 8 MB\n",
    "float64:  8 bytes √ó 1M rows = 8 MB\n",
    "object:   50 bytes √ó 1M rows = 50 MB\n",
    "Total: 66 MB\n",
    "\n",
    "After optimization:\n",
    "int8:     1 byte √ó 1M rows = 1 MB     (87% savings)\n",
    "float32:  4 bytes √ó 1M rows = 4 MB    (50% savings)\n",
    "category: 2 bytes √ó 1M rows = 2 MB    (96% savings)\n",
    "Total: 7 MB (89% savings!)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f5e5e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MEMORY OPTIMIZATION ===\n",
      "\n",
      "Example 1: Integer data type comparison\n",
      "\n",
      "1M integers (0-100):\n",
      "  int64:  7.63 MB\n",
      "  int8:   0.95 MB\n",
      "  Savings: 87.5%\n",
      "\n",
      "======================================================================\n",
      "Example 2: Float precision impact\n",
      "\n",
      "1M floats:\n",
      "  float64: 7.63 MB\n",
      "  float32: 3.81 MB\n",
      "  Savings: 50.0%\n",
      "\n",
      "======================================================================\n",
      "Example 3: String vs Categorical\n",
      "\n",
      "1M strings (5 unique values):\n",
      "  object:    47.68 MB\n",
      "  category:  0.95 MB\n",
      "  Savings: 98.0%\n",
      "\n",
      "======================================================================\n",
      "Example 4: Automatic dtype optimization\n",
      "\n",
      "Before optimization:\n",
      "age           int64\n",
      "score       float64\n",
      "category     object\n",
      "dtype: object\n",
      "Memory: 644.66 KB\n",
      "\n",
      "After optimization:\n",
      "age            uint8\n",
      "score        float32\n",
      "category    category\n",
      "dtype: object\n",
      "Memory: 58.97 KB\n",
      "\n",
      "Savings: 90.9%\n"
     ]
    }
   ],
   "source": [
    "print(\"=== MEMORY OPTIMIZATION ===\\n\")\n",
    "\n",
    "# Example 1: Data type impact\n",
    "print(\"Example 1: Integer data type comparison\\n\")\n",
    "n = 1_000_000\n",
    "values = np.random.randint(0, 100, n)\n",
    "\n",
    "df_int64 = pd.DataFrame({'value': values})\n",
    "df_int8 = pd.DataFrame({'value': values.astype('int8')})\n",
    "\n",
    "mem_int64 = df_int64.memory_usage(deep=True).sum() / 1024**2\n",
    "mem_int8 = df_int8.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "print(f\"1M integers (0-100):\")\n",
    "print(f\"  int64:  {mem_int64:.2f} MB\")\n",
    "print(f\"  int8:   {mem_int8:.2f} MB\")\n",
    "print(f\"  Savings: {(1 - mem_int8/mem_int64)*100:.1f}%\")\n",
    "print()\n",
    "\n",
    "# Example 2: Float precision\n",
    "print(\"=\"*70)\n",
    "print(\"Example 2: Float precision impact\\n\")\n",
    "float_values = np.random.randn(n)\n",
    "\n",
    "df_float64 = pd.DataFrame({'value': float_values})\n",
    "df_float32 = pd.DataFrame({'value': float_values.astype('float32')})\n",
    "\n",
    "mem_float64 = df_float64.memory_usage(deep=True).sum() / 1024**2\n",
    "mem_float32 = df_float32.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "print(f\"1M floats:\")\n",
    "print(f\"  float64: {mem_float64:.2f} MB\")\n",
    "print(f\"  float32: {mem_float32:.2f} MB\")\n",
    "print(f\"  Savings: {(1 - mem_float32/mem_float64)*100:.1f}%\")\n",
    "print()\n",
    "\n",
    "# Example 3: Categorical for strings\n",
    "print(\"=\"*70)\n",
    "print(\"Example 3: String vs Categorical\\n\")\n",
    "categories = ['A', 'B', 'C', 'D', 'E']\n",
    "strings = np.random.choice(categories, n)\n",
    "\n",
    "df_object = pd.DataFrame({'category': strings})\n",
    "df_category = pd.DataFrame({'category': pd.Categorical(strings)})\n",
    "\n",
    "mem_object = df_object.memory_usage(deep=True).sum() / 1024**2\n",
    "mem_category = df_category.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "print(f\"1M strings (5 unique values):\")\n",
    "print(f\"  object:    {mem_object:.2f} MB\")\n",
    "print(f\"  category:  {mem_category:.2f} MB\")\n",
    "print(f\"  Savings: {(1 - mem_category/mem_object)*100:.1f}%\")\n",
    "print()\n",
    "\n",
    "# Example 4: Automatic optimization function\n",
    "print(\"=\"*70)\n",
    "print(\"Example 4: Automatic dtype optimization\\n\")\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    \"\"\"Optimize DataFrame data types\"\"\"\n",
    "    for col in df.select_dtypes(include=['int']).columns:\n",
    "        col_min = df[col].min()\n",
    "        col_max = df[col].max()\n",
    "        \n",
    "        if col_min >= 0:\n",
    "            if col_max < 255:\n",
    "                df[col] = df[col].astype('uint8')\n",
    "            elif col_max < 65535:\n",
    "                df[col] = df[col].astype('uint16')\n",
    "        else:\n",
    "            if col_min > -128 and col_max < 127:\n",
    "                df[col] = df[col].astype('int8')\n",
    "            elif col_min > -32768 and col_max < 32767:\n",
    "                df[col] = df[col].astype('int16')\n",
    "    \n",
    "    for col in df.select_dtypes(include=['float']).columns:\n",
    "        df[col] = df[col].astype('float32')\n",
    "    \n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        num_unique = df[col].nunique()\n",
    "        if num_unique / len(df) < 0.5:\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create test DataFrame\n",
    "df_test = pd.DataFrame({\n",
    "    'age': np.random.randint(0, 100, 10000),\n",
    "    'score': np.random.randn(10000),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 10000)\n",
    "})\n",
    "\n",
    "print(\"Before optimization:\")\n",
    "print(df_test.dtypes)\n",
    "mem_before = df_test.memory_usage(deep=True).sum() / 1024\n",
    "print(f\"Memory: {mem_before:.2f} KB\")\n",
    "\n",
    "df_optimized = optimize_dtypes(df_test.copy())\n",
    "print(\"\\nAfter optimization:\")\n",
    "print(df_optimized.dtypes)\n",
    "mem_after = df_optimized.memory_usage(deep=True).sum() / 1024\n",
    "print(f\"Memory: {mem_after:.2f} KB\")\n",
    "print(f\"\\nSavings: {(1 - mem_after/mem_before)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a952a65",
   "metadata": {},
   "source": [
    "## 2. Vectorization - Avoid Loops!\n",
    "\n",
    "### Performance Hierarchy\n",
    "\n",
    "```\n",
    "‚ö°‚ö°‚ö° FASTEST (100-1000x)\n",
    "1. Built-in Pandas methods\n",
    "   df['result'] = df['col1'] + df['col2']\n",
    "\n",
    "2. NumPy vectorized operations\n",
    "   df['result'] = np.sqrt(df['col'])\n",
    "\n",
    "‚ö°‚ö° FAST (10-100x)\n",
    "3. Pandas built-in functions\n",
    "   df['result'] = df['col'].str.upper()\n",
    "\n",
    "‚ö° SLOW (baseline)\n",
    "4. apply() with built-in functions\n",
    "   df['result'] = df['col'].apply(lambda x: x**2)\n",
    "\n",
    "üêå VERY SLOW (10-1000x slower)\n",
    "5. apply() with custom functions\n",
    "   df['result'] = df.apply(complex_function, axis=1)\n",
    "\n",
    "üêåüêå EXTREMELY SLOW\n",
    "6. For loops (iterrows, itertuples)\n",
    "   for idx, row in df.iterrows():\n",
    "```\n",
    "\n",
    "### Vectorization Examples\n",
    "\n",
    "**‚ùå Loop (Slow)**\n",
    "```python\n",
    "result = []\n",
    "for i in range(len(df)):\n",
    "    result.append(df.loc[i, 'a'] + df.loc[i, 'b'])\n",
    "df['result'] = result\n",
    "```\n",
    "\n",
    "**‚ùå iterrows (Slow)**\n",
    "```python\n",
    "result = []\n",
    "for idx, row in df.iterrows():\n",
    "    result.append(row['a'] + row['b'])\n",
    "df['result'] = result\n",
    "```\n",
    "\n",
    "**‚úÖ Vectorized (Fast)**\n",
    "```python\n",
    "df['result'] = df['a'] + df['b']\n",
    "```\n",
    "\n",
    "### Common Operations\n",
    "\n",
    "**Arithmetic**\n",
    "```python\n",
    "# ‚ùå Slow\n",
    "df['result'] = df['col'].apply(lambda x: x * 2 + 5)\n",
    "\n",
    "# ‚úÖ Fast\n",
    "df['result'] = df['col'] * 2 + 5\n",
    "```\n",
    "\n",
    "**Conditional Logic**\n",
    "```python\n",
    "# ‚ùå Slow\n",
    "df['category'] = df['value'].apply(\n",
    "    lambda x: 'High' if x > 100 else 'Low'\n",
    ")\n",
    "\n",
    "# ‚úÖ Fast\n",
    "df['category'] = np.where(df['value'] > 100, 'High', 'Low')\n",
    "\n",
    "# ‚úÖ Fast (multiple conditions)\n",
    "conditions = [\n",
    "    df['value'] > 100,\n",
    "    df['value'] > 50,\n",
    "    df['value'] > 0\n",
    "]\n",
    "choices = ['High', 'Medium', 'Low']\n",
    "df['category'] = np.select(conditions, choices, default='None')\n",
    "```\n",
    "\n",
    "**String Operations**\n",
    "```python\n",
    "# ‚ùå Slow\n",
    "df['upper'] = df['name'].apply(lambda x: x.upper())\n",
    "\n",
    "# ‚úÖ Fast\n",
    "df['upper'] = df['name'].str.upper()\n",
    "```\n",
    "\n",
    "**Math Operations**\n",
    "```python\n",
    "# ‚ùå Slow\n",
    "df['sqrt'] = df['value'].apply(lambda x: x**0.5)\n",
    "\n",
    "# ‚úÖ Fast\n",
    "df['sqrt'] = np.sqrt(df['value'])\n",
    "```\n",
    "\n",
    "### When apply() is OK\n",
    "\n",
    "```python\n",
    "# Complex logic not vectorizable\n",
    "def complex_calculation(row):\n",
    "    # Multiple column interactions\n",
    "    # Complex conditions\n",
    "    # External API calls\n",
    "    return result\n",
    "\n",
    "# Use apply when truly needed\n",
    "df['result'] = df.apply(complex_calculation, axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "740de5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VECTORIZATION PERFORMANCE ===\n",
      "\n",
      "Example 1: Arithmetic operations\n",
      "\n",
      "Loop:       0.6336s\n",
      "iterrows:   1.2246s\n",
      "apply:      0.2777s\n",
      "Vectorized: 0.0007s\n",
      "\n",
      "Speedup: 866x faster!\n",
      "\n",
      "======================================================================\n",
      "Example 2: Conditional operations\n",
      "\n",
      "apply:      0.0103s\n",
      "np.where:   0.0065s\n",
      "\n",
      "Speedup: 2x faster!\n",
      "\n",
      "======================================================================\n",
      "Example 3: Multiple conditions\n",
      "\n",
      "apply:      0.0130s\n",
      "np.select:  0.0083s\n",
      "\n",
      "Speedup: 2x faster!\n",
      "\n",
      "======================================================================\n",
      "Example 4: String operations\n",
      "\n",
      "apply:      0.0094s\n",
      "str.upper:  0.0080s\n",
      "\n",
      "Speedup: 1x faster!\n",
      "\n",
      "======================================================================\n",
      "Example 5: Math operations\n",
      "\n",
      "apply:      0.0148s\n",
      "np.sqrt:    0.0003s\n",
      "\n",
      "Speedup: 43x faster!\n"
     ]
    }
   ],
   "source": [
    "print(\"=== VECTORIZATION PERFORMANCE ===\\n\")\n",
    "\n",
    "# Create test data\n",
    "n = 100_000\n",
    "df = pd.DataFrame({\n",
    "    'a': np.random.randint(0, 100, n),\n",
    "    'b': np.random.randint(0, 100, n),\n",
    "    'value': np.random.randn(n),\n",
    "    'name': np.random.choice(['alice', 'bob', 'charlie'], n)\n",
    "})\n",
    "\n",
    "# Example 1: Simple arithmetic\n",
    "print(\"Example 1: Arithmetic operations\\n\")\n",
    "\n",
    "# Method 1: Loop (slowest)\n",
    "start = time.time()\n",
    "result = []\n",
    "for i in range(len(df)):\n",
    "    result.append(df.loc[i, 'a'] + df.loc[i, 'b'])\n",
    "df['result_loop'] = result\n",
    "time_loop = time.time() - start\n",
    "print(f\"Loop:       {time_loop:.4f}s\")\n",
    "\n",
    "# Method 2: iterrows\n",
    "start = time.time()\n",
    "result = []\n",
    "for idx, row in df.iterrows():\n",
    "    result.append(row['a'] + row['b'])\n",
    "df['result_iterrows'] = result\n",
    "time_iterrows = time.time() - start\n",
    "print(f\"iterrows:   {time_iterrows:.4f}s\")\n",
    "\n",
    "# Method 3: apply\n",
    "start = time.time()\n",
    "df['result_apply'] = df.apply(lambda row: row['a'] + row['b'], axis=1)\n",
    "time_apply = time.time() - start\n",
    "print(f\"apply:      {time_apply:.4f}s\")\n",
    "\n",
    "# Method 4: Vectorized\n",
    "start = time.time()\n",
    "df['result_vectorized'] = df['a'] + df['b']\n",
    "time_vec = time.time() - start\n",
    "print(f\"Vectorized: {time_vec:.4f}s\")\n",
    "\n",
    "print(f\"\\nSpeedup: {time_loop/time_vec:.0f}x faster!\")\n",
    "print()\n",
    "\n",
    "# Example 2: Conditional logic\n",
    "print(\"=\"*70)\n",
    "print(\"Example 2: Conditional operations\\n\")\n",
    "\n",
    "# Method 1: apply\n",
    "start = time.time()\n",
    "df['category_apply'] = df['value'].apply(\n",
    "    lambda x: 'Positive' if x > 0 else 'Negative'\n",
    ")\n",
    "time_apply = time.time() - start\n",
    "print(f\"apply:      {time_apply:.4f}s\")\n",
    "\n",
    "# Method 2: np.where (vectorized)\n",
    "start = time.time()\n",
    "df['category_where'] = np.where(df['value'] > 0, 'Positive', 'Negative')\n",
    "time_where = time.time() - start\n",
    "print(f\"np.where:   {time_where:.4f}s\")\n",
    "\n",
    "print(f\"\\nSpeedup: {time_apply/time_where:.0f}x faster!\")\n",
    "print()\n",
    "\n",
    "# Example 3: Multiple conditions\n",
    "print(\"=\"*70)\n",
    "print(\"Example 3: Multiple conditions\\n\")\n",
    "\n",
    "# Method 1: apply with if-elif-else\n",
    "def categorize_apply(x):\n",
    "    if x > 1:\n",
    "        return 'High'\n",
    "    elif x > 0:\n",
    "        return 'Medium'\n",
    "    elif x > -1:\n",
    "        return 'Low'\n",
    "    else:\n",
    "        return 'Very Low'\n",
    "\n",
    "start = time.time()\n",
    "df['level_apply'] = df['value'].apply(categorize_apply)\n",
    "time_apply = time.time() - start\n",
    "print(f\"apply:      {time_apply:.4f}s\")\n",
    "\n",
    "# Method 2: np.select (vectorized)\n",
    "start = time.time()\n",
    "conditions = [\n",
    "    df['value'] > 1,\n",
    "    df['value'] > 0,\n",
    "    df['value'] > -1\n",
    "]\n",
    "choices = ['High', 'Medium', 'Low']\n",
    "df['level_select'] = np.select(conditions, choices, default='Very Low')\n",
    "time_select = time.time() - start\n",
    "print(f\"np.select:  {time_select:.4f}s\")\n",
    "\n",
    "print(f\"\\nSpeedup: {time_apply/time_select:.0f}x faster!\")\n",
    "print()\n",
    "\n",
    "# Example 4: String operations\n",
    "print(\"=\"*70)\n",
    "print(\"Example 4: String operations\\n\")\n",
    "\n",
    "# Method 1: apply\n",
    "start = time.time()\n",
    "df['name_upper_apply'] = df['name'].apply(lambda x: x.upper())\n",
    "time_apply = time.time() - start\n",
    "print(f\"apply:      {time_apply:.4f}s\")\n",
    "\n",
    "# Method 2: str accessor (vectorized)\n",
    "start = time.time()\n",
    "df['name_upper_str'] = df['name'].str.upper()\n",
    "time_str = time.time() - start\n",
    "print(f\"str.upper:  {time_str:.4f}s\")\n",
    "\n",
    "print(f\"\\nSpeedup: {time_apply/time_str:.0f}x faster!\")\n",
    "print()\n",
    "\n",
    "# Example 5: Mathematical operations\n",
    "print(\"=\"*70)\n",
    "print(\"Example 5: Math operations\\n\")\n",
    "\n",
    "# Create positive values for sqrt\n",
    "df['positive'] = np.abs(df['value']) + 1\n",
    "\n",
    "# Method 1: apply\n",
    "start = time.time()\n",
    "df['sqrt_apply'] = df['positive'].apply(lambda x: x**0.5)\n",
    "time_apply = time.time() - start\n",
    "print(f\"apply:      {time_apply:.4f}s\")\n",
    "\n",
    "# Method 2: NumPy (vectorized)\n",
    "start = time.time()\n",
    "df['sqrt_numpy'] = np.sqrt(df['positive'])\n",
    "time_numpy = time.time() - start\n",
    "print(f\"np.sqrt:    {time_numpy:.4f}s\")\n",
    "\n",
    "print(f\"\\nSpeedup: {time_apply/time_numpy:.0f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ff9027",
   "metadata": {},
   "source": [
    "## 3. Efficient Operations\n",
    "\n",
    "### Query vs Boolean Indexing\n",
    "\n",
    "```python\n",
    "# ‚ùå Slower for complex conditions\n",
    "df[(df['age'] > 25) & (df['city'] == 'NYC') & (df['score'] > 80)]\n",
    "\n",
    "# ‚úÖ Often faster (especially large DataFrames)\n",
    "df.query('age > 25 and city == \"NYC\" and score > 80')\n",
    "```\n",
    "\n",
    "### eval() for Complex Expressions\n",
    "\n",
    "```python\n",
    "# ‚ùå Slower (creates intermediate arrays)\n",
    "df['result'] = df['a'] + df['b'] * df['c'] - df['d'] / df['e']\n",
    "\n",
    "# ‚úÖ Faster (no intermediate arrays)\n",
    "df['result'] = df.eval('a + b * c - d / e')\n",
    "```\n",
    "\n",
    "### Efficient GroupBy\n",
    "\n",
    "```python\n",
    "# ‚úÖ Use built-in aggregations\n",
    "df.groupby('category')['value'].sum()      # Fast\n",
    "df.groupby('category')['value'].mean()     # Fast\n",
    "\n",
    "# ‚ùå Custom aggregation (slower)\n",
    "df.groupby('category')['value'].apply(custom_func)\n",
    "\n",
    "# ‚úÖ Multiple aggregations at once\n",
    "df.groupby('category').agg({\n",
    "    'value': ['sum', 'mean', 'count']\n",
    "})\n",
    "```\n",
    "\n",
    "### Index Optimization\n",
    "\n",
    "```python\n",
    "# ‚úÖ Sort index for fast slicing\n",
    "df = df.sort_index()\n",
    "df.loc['A':'Z']  # Fast with sorted index\n",
    "\n",
    "# ‚úÖ Set index for frequent lookups\n",
    "df = df.set_index('id')  # If you filter by 'id' often\n",
    "df.loc[12345]  # Fast lookup\n",
    "```\n",
    "\n",
    "### Copy vs Inplace\n",
    "\n",
    "```python\n",
    "# ‚ùå Creates copy (uses more memory)\n",
    "df_new = df.drop('col', axis=1)\n",
    "\n",
    "# ‚ö†Ô∏è Inplace (modifies original)\n",
    "df.drop('col', axis=1, inplace=True)\n",
    "\n",
    "# Note: inplace is being deprecated in many methods\n",
    "# Better to be explicit:\n",
    "df = df.drop('col', axis=1)\n",
    "```\n",
    "\n",
    "### Efficient Merging\n",
    "\n",
    "```python\n",
    "# ‚úÖ Set index before merge if merging multiple times\n",
    "df1 = df1.set_index('key')\n",
    "df2 = df2.set_index('key')\n",
    "result = df1.join(df2)  # Faster than merge\n",
    "\n",
    "# ‚úÖ Specify merge columns explicitly\n",
    "pd.merge(df1, df2, on='key', how='inner')  # Explicit is fast\n",
    "```\n",
    "\n",
    "### Method Chaining\n",
    "\n",
    "```python\n",
    "# ‚úÖ Efficient chaining\n",
    "result = (df\n",
    "    .query('age > 25')\n",
    "    .groupby('city')['sales']\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd17c72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EFFICIENT OPERATIONS ===\n",
      "\n",
      "Example 1: query() vs boolean indexing\n",
      "\n",
      "Boolean:  0.0050s\n",
      "query():  0.0058s\n",
      "\n",
      "Results match: True\n",
      "Boolean is 1.1x faster\n",
      "\n",
      "======================================================================\n",
      "Example 2: eval() for expressions\n",
      "\n",
      "Standard: 0.0006s\n",
      "eval():   0.0016s\n",
      "\n",
      "eval() is 0.4x faster\n",
      "\n",
      "======================================================================\n",
      "Example 3: GroupBy performance\n",
      "\n",
      "Single agg:    0.0042s\n",
      "Separate aggs: 0.0122s\n",
      "Combined agg:  0.0054s\n",
      "\n",
      "Combined is 2.3x faster than separate\n",
      "\n",
      "======================================================================\n",
      "Example 4: Index optimization\n",
      "\n",
      "Unsorted index: 0.4978s (100 selections)\n",
      "Sorted index:   0.0076s (100 selections)\n",
      "\n",
      "Sorted index is 65.6x faster\n",
      "\n",
      "======================================================================\n",
      "Example 5: Efficient method chaining\n",
      "\n",
      "Chained operation result:\n",
      "city\n",
      "LA         74.45\n",
      "NYC        74.39\n",
      "Chicago    74.30\n",
      "Name: score, dtype: float64\n",
      "\n",
      "Time: 0.0074s\n"
     ]
    }
   ],
   "source": [
    "print(\"=== EFFICIENT OPERATIONS ===\\n\")\n",
    "\n",
    "# Create test data\n",
    "n = 100_000\n",
    "df = pd.DataFrame({\n",
    "    'age': np.random.randint(20, 70, n),\n",
    "    'city': np.random.choice(['NYC', 'LA', 'Chicago'], n),\n",
    "    'score': np.random.randint(50, 100, n),\n",
    "    'a': np.random.randn(n),\n",
    "    'b': np.random.randn(n),\n",
    "    'c': np.random.randn(n),\n",
    "    'd': np.random.randn(n),\n",
    "    'e': np.random.randn(n) + 1  # Avoid division by zero\n",
    "})\n",
    "\n",
    "# Example 1: query() vs boolean indexing\n",
    "print(\"Example 1: query() vs boolean indexing\\n\")\n",
    "\n",
    "# Boolean indexing\n",
    "start = time.time()\n",
    "result1 = df[(df['age'] > 25) & (df['city'] == 'NYC') & (df['score'] > 80)]\n",
    "time_bool = time.time() - start\n",
    "print(f\"Boolean:  {time_bool:.4f}s\")\n",
    "\n",
    "# query()\n",
    "start = time.time()\n",
    "result2 = df.query('age > 25 and city == \"NYC\" and score > 80')\n",
    "time_query = time.time() - start\n",
    "print(f\"query():  {time_query:.4f}s\")\n",
    "\n",
    "print(f\"\\nResults match: {len(result1) == len(result2)}\")\n",
    "if time_bool > time_query:\n",
    "    print(f\"query() is {time_bool/time_query:.1f}x faster\")\n",
    "else:\n",
    "    print(f\"Boolean is {time_query/time_bool:.1f}x faster\")\n",
    "print()\n",
    "\n",
    "# Example 2: eval() for complex expressions\n",
    "print(\"=\"*70)\n",
    "print(\"Example 2: eval() for expressions\\n\")\n",
    "\n",
    "# Standard operation\n",
    "start = time.time()\n",
    "df['result_standard'] = df['a'] + df['b'] * df['c'] - df['d'] / df['e']\n",
    "time_standard = time.time() - start\n",
    "print(f\"Standard: {time_standard:.4f}s\")\n",
    "\n",
    "# eval()\n",
    "start = time.time()\n",
    "df['result_eval'] = df.eval('a + b * c - d / e')\n",
    "time_eval = time.time() - start\n",
    "print(f\"eval():   {time_eval:.4f}s\")\n",
    "\n",
    "print(f\"\\neval() is {time_standard/time_eval:.1f}x faster\")\n",
    "print()\n",
    "\n",
    "# Example 3: Efficient groupby\n",
    "print(\"=\"*70)\n",
    "print(\"Example 3: GroupBy performance\\n\")\n",
    "\n",
    "# Single aggregation\n",
    "start = time.time()\n",
    "result1 = df.groupby('city')['score'].sum()\n",
    "time1 = time.time() - start\n",
    "print(f\"Single agg:    {time1:.4f}s\")\n",
    "\n",
    "# Multiple separate aggregations\n",
    "start = time.time()\n",
    "sum_result = df.groupby('city')['score'].sum()\n",
    "mean_result = df.groupby('city')['score'].mean()\n",
    "count_result = df.groupby('city')['score'].count()\n",
    "time2 = time.time() - start\n",
    "print(f\"Separate aggs: {time2:.4f}s\")\n",
    "\n",
    "# Combined aggregation (efficient)\n",
    "start = time.time()\n",
    "result3 = df.groupby('city')['score'].agg(['sum', 'mean', 'count'])\n",
    "time3 = time.time() - start\n",
    "print(f\"Combined agg:  {time3:.4f}s\")\n",
    "\n",
    "print(f\"\\nCombined is {time2/time3:.1f}x faster than separate\")\n",
    "print()\n",
    "\n",
    "# Example 4: Index optimization\n",
    "print(\"=\"*70)\n",
    "print(\"Example 4: Index optimization\\n\")\n",
    "\n",
    "# Create DataFrame with random order\n",
    "df_unsorted = df.copy()\n",
    "df_unsorted.index = np.random.permutation(df.index)\n",
    "\n",
    "# Selection without sorted index\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    _ = df_unsorted.loc[df_unsorted['city'] == 'NYC']\n",
    "time_unsorted = time.time() - start\n",
    "print(f\"Unsorted index: {time_unsorted:.4f}s (100 selections)\")\n",
    "\n",
    "# Set and sort index\n",
    "df_sorted = df.set_index('city').sort_index()\n",
    "\n",
    "# Selection with sorted index\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    _ = df_sorted.loc['NYC']\n",
    "time_sorted = time.time() - start\n",
    "print(f\"Sorted index:   {time_sorted:.4f}s (100 selections)\")\n",
    "\n",
    "print(f\"\\nSorted index is {time_unsorted/time_sorted:.1f}x faster\")\n",
    "print()\n",
    "\n",
    "# Example 5: Method chaining\n",
    "print(\"=\"*70)\n",
    "print(\"Example 5: Efficient method chaining\\n\")\n",
    "\n",
    "# Chained operations\n",
    "start = time.time()\n",
    "result = (df\n",
    "    .query('age > 30')\n",
    "    .groupby('city')['score']\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "time_chain = time.time() - start\n",
    "\n",
    "print(\"Chained operation result:\")\n",
    "print(result)\n",
    "print(f\"\\nTime: {time_chain:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1642a906",
   "metadata": {},
   "source": [
    "## 4. Large Data Handling\n",
    "\n",
    "### Reading Large Files Efficiently\n",
    "\n",
    "**1. Specify dtypes**\n",
    "```python\n",
    "# ‚ùå Pandas infers dtypes (slow, memory-hungry)\n",
    "df = pd.read_csv('large.csv')\n",
    "\n",
    "# ‚úÖ Specify dtypes upfront\n",
    "dtypes = {\n",
    "    'id': 'int32',\n",
    "    'value': 'float32',\n",
    "    'category': 'category'\n",
    "}\n",
    "df = pd.read_csv('large.csv', dtype=dtypes)\n",
    "```\n",
    "\n",
    "**2. Select only needed columns**\n",
    "```python\n",
    "# ‚ùå Read all columns\n",
    "df = pd.read_csv('large.csv')  # 50 columns\n",
    "\n",
    "# ‚úÖ Read only what you need\n",
    "df = pd.read_csv('large.csv', usecols=['id', 'name', 'value'])\n",
    "```\n",
    "\n",
    "**3. Parse dates efficiently**\n",
    "```python\n",
    "# ‚ùå Parse after reading\n",
    "df = pd.read_csv('large.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# ‚úÖ Parse during reading\n",
    "df = pd.read_csv('large.csv', parse_dates=['date'])\n",
    "```\n",
    "\n",
    "### Chunked Processing\n",
    "\n",
    "**Process in chunks**\n",
    "```python\n",
    "# Process 10,000 rows at a time\n",
    "chunk_size = 10_000\n",
    "results = []\n",
    "\n",
    "for chunk in pd.read_csv('large.csv', chunksize=chunk_size):\n",
    "    # Process chunk\n",
    "    processed = chunk[chunk['value'] > 100]\n",
    "    results.append(processed)\n",
    "\n",
    "# Combine results\n",
    "df = pd.concat(results, ignore_index=True)\n",
    "```\n",
    "\n",
    "**Aggregate while chunking**\n",
    "```python\n",
    "# Calculate sum without loading all data\n",
    "total = 0\n",
    "\n",
    "for chunk in pd.read_csv('large.csv', chunksize=10_000):\n",
    "    total += chunk['value'].sum()\n",
    "\n",
    "print(f\"Total: {total}\")\n",
    "```\n",
    "\n",
    "### Sampling\n",
    "\n",
    "```python\n",
    "# ‚úÖ Random sample for development\n",
    "df = pd.read_csv('large.csv', \n",
    "                 skiprows=lambda i: i > 0 and np.random.random() > 0.01)\n",
    "# Reads ~1% of rows\n",
    "\n",
    "# ‚úÖ First N rows for testing\n",
    "df = pd.read_csv('large.csv', nrows=1000)\n",
    "```\n",
    "\n",
    "### Compression\n",
    "\n",
    "```python\n",
    "# Read compressed files (auto-detected)\n",
    "df = pd.read_csv('data.csv.gz')  # gzip\n",
    "df = pd.read_csv('data.csv.bz2')  # bzip2\n",
    "df = pd.read_csv('data.csv.zip')  # zip\n",
    "\n",
    "# Save with compression\n",
    "df.to_csv('data.csv.gz', compression='gzip')\n",
    "```\n",
    "\n",
    "### Alternative Formats\n",
    "\n",
    "```python\n",
    "# Parquet (fast, compressed, preserves dtypes)\n",
    "df.to_parquet('data.parquet')  # Save\n",
    "df = pd.read_parquet('data.parquet')  # Load\n",
    "\n",
    "# Feather (very fast, for temporary storage)\n",
    "df.to_feather('data.feather')\n",
    "df = pd.read_feather('data.feather')\n",
    "\n",
    "# Pickle (preserves everything, Python-only)\n",
    "df.to_pickle('data.pkl')\n",
    "df = pd.read_pickle('data.pkl')\n",
    "```\n",
    "\n",
    "### Format Comparison\n",
    "\n",
    "```\n",
    "Format      Speed    Size    Cross-Lang  Best For\n",
    "-------     -----    ----    ----------  --------\n",
    "CSV         Slow     Large   ‚úÖ          Sharing, human-readable\n",
    "Parquet     Fast     Small   ‚úÖ          Production, archiving\n",
    "Feather     Fastest  Medium  ‚úÖ          Temporary, inter-process\n",
    "Pickle      Fast     Medium  ‚ùå          Python-only, quick save\n",
    "HDF5        Fast     Small   ‚úÖ          Large datasets, append\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc2eb9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LARGE DATA HANDLING ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/sfvddbbx4tg14yxq7trqmhyw0000gn/T/ipykernel_20755/1014934650.py:10: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  'date': pd.date_range('2020-01-01', periods=n, freq='T')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created large_sample.csv (1M rows)\n",
      "\n",
      "Example 1: Optimized CSV reading\n",
      "\n",
      "Default read:\n",
      "  Time: 0.50s\n",
      "  Memory: 135.4 MB\n",
      "\n",
      "Optimized read:\n",
      "  Time: 1.76s\n",
      "  Memory: 18.1 MB\n",
      "\n",
      "Memory savings: 86.6%\n",
      "\n",
      "======================================================================\n",
      "Example 2: Column selection (usecols)\n",
      "\n",
      "All columns (5):\n",
      "  Time: 0.46s\n",
      "  Memory: 135.4 MB\n",
      "\n",
      "Only 2 columns:\n",
      "  Time: 0.18s\n",
      "  Memory: 15.3 MB\n",
      "\n",
      "Memory savings: 88.7%\n",
      "Speed improvement: 2.6x\n",
      "\n",
      "======================================================================\n",
      "Example 3: Chunked processing\n",
      "\n",
      "Processing chunk 1: 49788 rows kept\n",
      "\n",
      "Processed 1000000 rows in 0.48s\n",
      "Result: 499053 rows (filtered)\n",
      "\n",
      "======================================================================\n",
      "Example 4: Chunk aggregation\n",
      "\n",
      "Total sum: 499,645,967\n",
      "Mean: 499.65\n",
      "Time: 0.59s\n",
      "\n",
      "======================================================================\n",
      "Example 5: Sampling strategies\n",
      "\n",
      "First 10K rows: 0.0055s\n",
      "Random 1% sample: 0.4565s (9968 rows)\n",
      "\n",
      "======================================================================\n",
      "Example 6: File format performance\n",
      "\n",
      "Format     Write      Read       Size (MB) \n",
      "----------------------------------------\n",
      "CSV        0.273      0.048      4.9       \n",
      "Parquet    2.533      3.308      2.6       \n",
      "Pickle     0.004      0.002      3.2       \n",
      "\n",
      "‚úÖ Cleanup complete\n"
     ]
    }
   ],
   "source": [
    "print(\"=== LARGE DATA HANDLING ===\\n\")\n",
    "\n",
    "# Create sample large CSV for demonstration\n",
    "n = 1_000_000\n",
    "sample_data = pd.DataFrame({\n",
    "    'id': range(n),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], n),\n",
    "    'value': np.random.randn(n),\n",
    "    'amount': np.random.randint(0, 1000, n),\n",
    "    'date': pd.date_range('2020-01-01', periods=n, freq='T')\n",
    "})\n",
    "sample_data.to_csv('large_sample.csv', index=False)\n",
    "print(\"‚úÖ Created large_sample.csv (1M rows)\\n\")\n",
    "\n",
    "# Example 1: Default vs optimized reading\n",
    "print(\"Example 1: Optimized CSV reading\\n\")\n",
    "\n",
    "# Default reading\n",
    "start = time.time()\n",
    "df_default = pd.read_csv('large_sample.csv')\n",
    "time_default = time.time() - start\n",
    "mem_default = df_default.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"Default read:\")\n",
    "print(f\"  Time: {time_default:.2f}s\")\n",
    "print(f\"  Memory: {mem_default:.1f} MB\")\n",
    "\n",
    "# Optimized reading\n",
    "start = time.time()\n",
    "df_optimized = pd.read_csv('large_sample.csv',\n",
    "                           dtype={'id': 'int32',\n",
    "                                  'category': 'category',\n",
    "                                  'value': 'float32',\n",
    "                                  'amount': 'int16'},\n",
    "                           parse_dates=['date'])\n",
    "time_optimized = time.time() - start\n",
    "mem_optimized = df_optimized.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"\\nOptimized read:\")\n",
    "print(f\"  Time: {time_optimized:.2f}s\")\n",
    "print(f\"  Memory: {mem_optimized:.1f} MB\")\n",
    "\n",
    "print(f\"\\nMemory savings: {(1 - mem_optimized/mem_default)*100:.1f}%\")\n",
    "print()\n",
    "\n",
    "# Example 2: Select only needed columns\n",
    "print(\"=\"*70)\n",
    "print(\"Example 2: Column selection (usecols)\\n\")\n",
    "\n",
    "# Read all columns\n",
    "start = time.time()\n",
    "df_all = pd.read_csv('large_sample.csv')\n",
    "time_all = time.time() - start\n",
    "mem_all = df_all.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"All columns (5):\")\n",
    "print(f\"  Time: {time_all:.2f}s\")\n",
    "print(f\"  Memory: {mem_all:.1f} MB\")\n",
    "\n",
    "# Read only 2 columns\n",
    "start = time.time()\n",
    "df_subset = pd.read_csv('large_sample.csv', usecols=['id', 'value'])\n",
    "time_subset = time.time() - start\n",
    "mem_subset = df_subset.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"\\nOnly 2 columns:\")\n",
    "print(f\"  Time: {time_subset:.2f}s\")\n",
    "print(f\"  Memory: {mem_subset:.1f} MB\")\n",
    "\n",
    "print(f\"\\nMemory savings: {(1 - mem_subset/mem_all)*100:.1f}%\")\n",
    "print(f\"Speed improvement: {time_all/time_subset:.1f}x\")\n",
    "print()\n",
    "\n",
    "# Example 3: Chunked processing\n",
    "print(\"=\"*70)\n",
    "print(\"Example 3: Chunked processing\\n\")\n",
    "\n",
    "# Process in chunks and filter\n",
    "chunk_size = 100_000\n",
    "results = []\n",
    "\n",
    "start = time.time()\n",
    "for i, chunk in enumerate(pd.read_csv('large_sample.csv', chunksize=chunk_size)):\n",
    "    # Filter chunk\n",
    "    filtered = chunk[chunk['amount'] > 500]\n",
    "    results.append(filtered)\n",
    "    if i == 0:\n",
    "        print(f\"Processing chunk {i+1}: {len(filtered)} rows kept\")\n",
    "\n",
    "df_filtered = pd.concat(results, ignore_index=True)\n",
    "time_chunk = time.time() - start\n",
    "\n",
    "print(f\"\\nProcessed {len(sample_data)} rows in {time_chunk:.2f}s\")\n",
    "print(f\"Result: {len(df_filtered)} rows (filtered)\")\n",
    "print()\n",
    "\n",
    "# Example 4: Aggregation without loading all data\n",
    "print(\"=\"*70)\n",
    "print(\"Example 4: Chunk aggregation\\n\")\n",
    "\n",
    "# Calculate sum and count without loading full DataFrame\n",
    "total_sum = 0\n",
    "total_count = 0\n",
    "\n",
    "start = time.time()\n",
    "for chunk in pd.read_csv('large_sample.csv', chunksize=100_000):\n",
    "    total_sum += chunk['amount'].sum()\n",
    "    total_count += len(chunk)\n",
    "\n",
    "mean_value = total_sum / total_count\n",
    "time_agg = time.time() - start\n",
    "\n",
    "print(f\"Total sum: {total_sum:,.0f}\")\n",
    "print(f\"Mean: {mean_value:.2f}\")\n",
    "print(f\"Time: {time_agg:.2f}s\")\n",
    "print()\n",
    "\n",
    "# Example 5: Sampling\n",
    "print(\"=\"*70)\n",
    "print(\"Example 5: Sampling strategies\\n\")\n",
    "\n",
    "# Read first N rows\n",
    "start = time.time()\n",
    "df_head = pd.read_csv('large_sample.csv', nrows=10_000)\n",
    "time_head = time.time() - start\n",
    "print(f\"First 10K rows: {time_head:.4f}s\")\n",
    "\n",
    "# Read random sample (~1%)\n",
    "start = time.time()\n",
    "df_sample = pd.read_csv('large_sample.csv',\n",
    "                        skiprows=lambda i: i > 0 and np.random.random() > 0.01)\n",
    "time_sample = time.time() - start\n",
    "print(f\"Random 1% sample: {time_sample:.4f}s ({len(df_sample)} rows)\")\n",
    "print()\n",
    "\n",
    "# Example 6: File format comparison\n",
    "print(\"=\"*70)\n",
    "print(\"Example 6: File format performance\\n\")\n",
    "\n",
    "# Prepare smaller dataset for format comparison\n",
    "df_format = sample_data.head(100_000)\n",
    "\n",
    "# CSV\n",
    "start = time.time()\n",
    "df_format.to_csv('test.csv', index=False)\n",
    "time_csv_write = time.time() - start\n",
    "start = time.time()\n",
    "_ = pd.read_csv('test.csv')\n",
    "time_csv_read = time.time() - start\n",
    "import os\n",
    "size_csv = os.path.getsize('test.csv') / 1024**2\n",
    "\n",
    "# Parquet\n",
    "start = time.time()\n",
    "df_format.to_parquet('test.parquet')\n",
    "time_parquet_write = time.time() - start\n",
    "start = time.time()\n",
    "_ = pd.read_parquet('test.parquet')\n",
    "time_parquet_read = time.time() - start\n",
    "size_parquet = os.path.getsize('test.parquet') / 1024**2\n",
    "\n",
    "# Pickle\n",
    "start = time.time()\n",
    "df_format.to_pickle('test.pkl')\n",
    "time_pickle_write = time.time() - start\n",
    "start = time.time()\n",
    "_ = pd.read_pickle('test.pkl')\n",
    "time_pickle_read = time.time() - start\n",
    "size_pickle = os.path.getsize('test.pkl') / 1024**2\n",
    "\n",
    "print(f\"{'Format':<10} {'Write':<10} {'Read':<10} {'Size (MB)':<10}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'CSV':<10} {time_csv_write:<10.3f} {time_csv_read:<10.3f} {size_csv:<10.1f}\")\n",
    "print(f\"{'Parquet':<10} {time_parquet_write:<10.3f} {time_parquet_read:<10.3f} {size_parquet:<10.1f}\")\n",
    "print(f\"{'Pickle':<10} {time_pickle_write:<10.3f} {time_pickle_read:<10.3f} {size_pickle:<10.1f}\")\n",
    "\n",
    "# Cleanup\n",
    "import os\n",
    "for f in ['large_sample.csv', 'test.csv', 'test.parquet', 'test.pkl']:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "print(\"\\n‚úÖ Cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c951b",
   "metadata": {},
   "source": [
    "## 5. Advanced Techniques\n",
    "\n",
    "### Parallel Processing\n",
    "\n",
    "**Using swifter (auto-parallelizes apply)**\n",
    "```python\n",
    "# Install: pip install swifter\n",
    "import swifter\n",
    "\n",
    "# ‚ùå Single-core apply\n",
    "df['result'] = df['col'].apply(complex_function)\n",
    "\n",
    "# ‚úÖ Multi-core apply\n",
    "df['result'] = df['col'].swifter.apply(complex_function)\n",
    "```\n",
    "\n",
    "**Using Dask (parallel Pandas)**\n",
    "```python\n",
    "# Install: pip install dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Convert to Dask DataFrame\n",
    "ddf = dd.from_pandas(df, npartitions=4)\n",
    "\n",
    "# Operations are parallelized\n",
    "result = ddf.groupby('category')['value'].mean().compute()\n",
    "```\n",
    "\n",
    "**Using multiprocessing**\n",
    "```python\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    # Your processing logic\n",
    "    return chunk[chunk['value'] > 0]\n",
    "\n",
    "# Split DataFrame\n",
    "chunks = np.array_split(df, 4)\n",
    "\n",
    "# Process in parallel\n",
    "with Pool(4) as pool:\n",
    "    results = pool.map(process_chunk, chunks)\n",
    "\n",
    "# Combine results\n",
    "df_result = pd.concat(results)\n",
    "```\n",
    "\n",
    "### Numba Acceleration\n",
    "\n",
    "```python\n",
    "# Install: pip install numba\n",
    "from numba import jit\n",
    "\n",
    "# ‚ùå Slow Python function\n",
    "def calculate_slow(arr):\n",
    "    result = np.zeros_like(arr)\n",
    "    for i in range(len(arr)):\n",
    "        result[i] = arr[i] ** 2 + arr[i] ** 0.5\n",
    "    return result\n",
    "\n",
    "# ‚úÖ JIT-compiled (10-100x faster)\n",
    "@jit(nopython=True)\n",
    "def calculate_fast(arr):\n",
    "    result = np.zeros_like(arr)\n",
    "    for i in range(len(arr)):\n",
    "        result[i] = arr[i] ** 2 + arr[i] ** 0.5\n",
    "    return result\n",
    "\n",
    "df['result'] = calculate_fast(df['value'].values)\n",
    "```\n",
    "\n",
    "### Sparse Data Structures\n",
    "\n",
    "```python\n",
    "# When data is mostly zeros/NaN\n",
    "import pandas.arrays as arrays\n",
    "\n",
    "# Create sparse array\n",
    "sparse_arr = arrays.SparseArray([0, 0, 1, 0, 0, 2, 0, 0])\n",
    "df['sparse'] = sparse_arr\n",
    "\n",
    "# Huge memory savings for sparse data\n",
    "# Example: 1M rows, 99% zeros\n",
    "# Dense: 8 MB\n",
    "# Sparse: 0.08 MB (99% savings!)\n",
    "```\n",
    "\n",
    "### String Performance\n",
    "\n",
    "```python\n",
    "# ‚úÖ Use string dtype for better performance\n",
    "df['text'] = df['text'].astype('string')  # Pandas 1.0+\n",
    "\n",
    "# Benefits:\n",
    "# - Better memory usage\n",
    "# - Faster string operations\n",
    "# - Consistent NA handling\n",
    "```\n",
    "\n",
    "### Copy-on-Write (COW)\n",
    "\n",
    "```python\n",
    "# Enable COW mode (Pandas 2.0+)\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "# Benefits:\n",
    "# - Avoids unnecessary copies\n",
    "# - Better memory usage\n",
    "# - More predictable behavior\n",
    "```\n",
    "\n",
    "### Efficient String Operations\n",
    "\n",
    "```python\n",
    "# ‚úÖ Vectorized regex\n",
    "df['matches'] = df['text'].str.contains(r'pattern', regex=True)\n",
    "\n",
    "# ‚úÖ Extract groups\n",
    "df[['first', 'last']] = df['name'].str.extract(r'(\\w+)\\s(\\w+)')\n",
    "\n",
    "# ‚úÖ Replace efficiently\n",
    "df['clean'] = df['text'].str.replace(r'[^a-zA-Z]', '', regex=True)\n",
    "```\n",
    "\n",
    "### Window Operations\n",
    "\n",
    "```python\n",
    "# Rolling operations (efficient C implementation)\n",
    "df['rolling_mean'] = df['value'].rolling(window=7).mean()\n",
    "df['rolling_sum'] = df['value'].rolling(window=7).sum()\n",
    "\n",
    "# Expanding operations\n",
    "df['cumulative_mean'] = df['value'].expanding().mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92b0ccab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ADVANCED TECHNIQUES ===\n",
      "\n",
      "Example 1: Numba acceleration\n",
      "\n",
      "Python loop:  1.2026s\n",
      "Numba JIT:    0.0010s\n",
      "\n",
      "Numba is 1191x faster!\n",
      "\n",
      "======================================================================\n",
      "Example 2: Sparse arrays for sparse data\n",
      "\n",
      "Dense array:  7.63 MB\n",
      "Sparse array: 0.57 MB\n",
      "\n",
      "Memory savings: 92.5%\n",
      "\n",
      "======================================================================\n",
      "Example 3: String dtype performance\n",
      "\n",
      "Object dtype:  5.33 MB\n",
      "String dtype:  5.33 MB\n",
      "\n",
      "Object upper:  0.0078s\n",
      "String upper:  0.0085s\n",
      "\n",
      "======================================================================\n",
      "Example 4: Efficient window operations\n",
      "\n",
      "Rolling mean (window=100): 0.0140s\n",
      "Expanding mean:            0.0099s\n",
      "\n",
      "======================================================================\n",
      "Example 5: Vectorized string operations\n",
      "\n",
      "Extract username:  0.0059s\n",
      "Pattern matching:  0.0034s\n",
      "Replace pattern:   0.0044s\n",
      "\n",
      "======================================================================\n",
      "Example 6: Copy-on-Write optimization\n",
      "\n",
      "Copy-on-Write mode: False\n",
      "\n",
      "COW Benefits:\n",
      "  - Avoids unnecessary data copies\n",
      "  - Reduces memory usage\n",
      "  - More predictable behavior\n",
      "\n",
      "To enable: pd.options.mode.copy_on_write = True\n"
     ]
    }
   ],
   "source": [
    "print(\"=== ADVANCED TECHNIQUES ===\\n\")\n",
    "\n",
    "# Example 1: Numba acceleration (if available)\n",
    "print(\"Example 1: Numba acceleration\\n\")\n",
    "\n",
    "try:\n",
    "    from numba import jit\n",
    "    \n",
    "    n = 1_000_000\n",
    "    arr = np.random.randn(n)\n",
    "    \n",
    "    # Regular Python function\n",
    "    def calculate_python(arr):\n",
    "        result = np.zeros_like(arr)\n",
    "        for i in range(len(arr)):\n",
    "            result[i] = arr[i] ** 2 + np.sqrt(np.abs(arr[i]))\n",
    "        return result\n",
    "    \n",
    "    # JIT-compiled version\n",
    "    @jit(nopython=True)\n",
    "    def calculate_numba(arr):\n",
    "        result = np.zeros_like(arr)\n",
    "        for i in range(len(arr)):\n",
    "            result[i] = arr[i] ** 2 + np.sqrt(np.abs(arr[i]))\n",
    "        return result\n",
    "    \n",
    "    # Warm up JIT\n",
    "    _ = calculate_numba(arr[:100])\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.time()\n",
    "    result_python = calculate_python(arr)\n",
    "    time_python = time.time() - start\n",
    "    print(f\"Python loop:  {time_python:.4f}s\")\n",
    "    \n",
    "    start = time.time()\n",
    "    result_numba = calculate_numba(arr)\n",
    "    time_numba = time.time() - start\n",
    "    print(f\"Numba JIT:    {time_numba:.4f}s\")\n",
    "    \n",
    "    print(f\"\\nNumba is {time_python/time_numba:.0f}x faster!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Numba not installed (pip install numba)\")\n",
    "    print(\"Skipping Numba example\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Example 2: Sparse arrays\n",
    "print(\"=\"*70)\n",
    "print(\"Example 2: Sparse arrays for sparse data\\n\")\n",
    "\n",
    "n = 1_000_000\n",
    "# Create mostly zero array (95% zeros)\n",
    "sparse_data = np.random.choice([0, 1, 2, 3], n, p=[0.95, 0.03, 0.01, 0.01])\n",
    "\n",
    "# Dense array\n",
    "df_dense = pd.DataFrame({'values': sparse_data})\n",
    "mem_dense = df_dense.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"Dense array:  {mem_dense:.2f} MB\")\n",
    "\n",
    "# Sparse array\n",
    "df_sparse = pd.DataFrame({\n",
    "    'values': pd.arrays.SparseArray(sparse_data)\n",
    "})\n",
    "mem_sparse = df_sparse.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"Sparse array: {mem_sparse:.2f} MB\")\n",
    "print(f\"\\nMemory savings: {(1 - mem_sparse/mem_dense)*100:.1f}%\")\n",
    "print()\n",
    "\n",
    "# Example 3: String dtype\n",
    "print(\"=\"*70)\n",
    "print(\"Example 3: String dtype performance\\n\")\n",
    "\n",
    "n = 100_000\n",
    "strings = ['text_' + str(i % 100) for i in range(n)]\n",
    "\n",
    "# Object dtype\n",
    "df_object = pd.DataFrame({'text': strings})\n",
    "mem_object = df_object.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"Object dtype:  {mem_object:.2f} MB\")\n",
    "\n",
    "# String dtype\n",
    "df_string = pd.DataFrame({'text': pd.array(strings, dtype='string')})\n",
    "mem_string = df_string.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"String dtype:  {mem_string:.2f} MB\")\n",
    "\n",
    "# Performance test\n",
    "start = time.time()\n",
    "_ = df_object['text'].str.upper()\n",
    "time_object = time.time() - start\n",
    "print(f\"\\nObject upper:  {time_object:.4f}s\")\n",
    "\n",
    "start = time.time()\n",
    "_ = df_string['text'].str.upper()\n",
    "time_string = time.time() - start\n",
    "print(f\"String upper:  {time_string:.4f}s\")\n",
    "print()\n",
    "\n",
    "# Example 4: Window operations\n",
    "print(\"=\"*70)\n",
    "print(\"Example 4: Efficient window operations\\n\")\n",
    "\n",
    "n = 1_000_000\n",
    "df_window = pd.DataFrame({\n",
    "    'value': np.random.randn(n)\n",
    "})\n",
    "\n",
    "# Rolling mean (efficient C implementation)\n",
    "start = time.time()\n",
    "df_window['rolling_mean'] = df_window['value'].rolling(window=100).mean()\n",
    "time_rolling = time.time() - start\n",
    "print(f\"Rolling mean (window=100): {time_rolling:.4f}s\")\n",
    "\n",
    "# Expanding mean\n",
    "start = time.time()\n",
    "df_window['expanding_mean'] = df_window['value'].expanding().mean()\n",
    "time_expanding = time.time() - start\n",
    "print(f\"Expanding mean:            {time_expanding:.4f}s\")\n",
    "print()\n",
    "\n",
    "# Example 5: Efficient regex operations\n",
    "print(\"=\"*70)\n",
    "print(\"Example 5: Vectorized string operations\\n\")\n",
    "\n",
    "n = 10_000\n",
    "emails = [f\"user{i}@example.com\" for i in range(n)]\n",
    "df_email = pd.DataFrame({'email': emails})\n",
    "\n",
    "# Extract username (vectorized)\n",
    "start = time.time()\n",
    "df_email['username'] = df_email['email'].str.extract(r'(.+)@')\n",
    "time_extract = time.time() - start\n",
    "print(f\"Extract username:  {time_extract:.4f}s\")\n",
    "\n",
    "# Check pattern (vectorized)\n",
    "start = time.time()\n",
    "df_email['is_valid'] = df_email['email'].str.contains(r'^[\\w.]+@[\\w.]+\\.[a-z]+$', regex=True)\n",
    "time_pattern = time.time() - start\n",
    "print(f\"Pattern matching:  {time_pattern:.4f}s\")\n",
    "\n",
    "# Replace (vectorized)\n",
    "start = time.time()\n",
    "df_email['domain'] = df_email['email'].str.replace(r'.+@', '', regex=True)\n",
    "time_replace = time.time() - start\n",
    "print(f\"Replace pattern:   {time_replace:.4f}s\")\n",
    "print()\n",
    "\n",
    "# Example 6: Copy-on-Write mode\n",
    "print(\"=\"*70)\n",
    "print(\"Example 6: Copy-on-Write optimization\\n\")\n",
    "\n",
    "# Check if COW is available (Pandas 2.0+)\n",
    "try:\n",
    "    original_cow = pd.options.mode.copy_on_write\n",
    "    print(f\"Copy-on-Write mode: {original_cow}\")\n",
    "    print(\"\\nCOW Benefits:\")\n",
    "    print(\"  - Avoids unnecessary data copies\")\n",
    "    print(\"  - Reduces memory usage\")\n",
    "    print(\"  - More predictable behavior\")\n",
    "    print(\"\\nTo enable: pd.options.mode.copy_on_write = True\")\n",
    "except AttributeError:\n",
    "    print(\"‚ö†Ô∏è Copy-on-Write not available (requires Pandas 2.0+)\")\n",
    "    print(\"Your version:\", pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672ce319",
   "metadata": {},
   "source": [
    "## 6. Best Practices & Optimization Checklist\n",
    "\n",
    "### Memory Optimization Checklist ‚úÖ\n",
    "\n",
    "```python\n",
    "# 1. Use optimal data types\n",
    "‚òê int64 ‚Üí int8/int16/int32 (if range allows)\n",
    "‚òê float64 ‚Üí float32 (if precision allows)\n",
    "‚òê object ‚Üí category (for repeated values)\n",
    "‚òê object ‚Üí string (for text data)\n",
    "\n",
    "# 2. Read efficiently\n",
    "‚òê Specify dtype in read_csv()\n",
    "‚òê Use usecols to read only needed columns\n",
    "‚òê Parse dates during reading\n",
    "‚òê Use chunksize for large files\n",
    "\n",
    "# 3. Remove unused data\n",
    "‚òê Drop unnecessary columns\n",
    "‚òê Filter early in pipeline\n",
    "‚òê Use sparse arrays for sparse data\n",
    "```\n",
    "\n",
    "### Speed Optimization Checklist ‚ö°\n",
    "\n",
    "```python\n",
    "# 1. Vectorize operations\n",
    "‚òê Use built-in Pandas/NumPy functions\n",
    "‚òê Avoid iterrows(), itertuples()\n",
    "‚òê Replace apply() with vectorized alternatives\n",
    "‚òê Use np.where() for conditionals\n",
    "‚òê Use np.select() for multiple conditions\n",
    "\n",
    "# 2. Optimize data access\n",
    "‚òê Sort index if slicing frequently\n",
    "‚òê Set index for frequent lookups\n",
    "‚òê Use query() for complex filters\n",
    "‚òê Use eval() for expressions\n",
    "\n",
    "# 3. Efficient groupby\n",
    "‚òê Use built-in aggregations (sum, mean)\n",
    "‚òê Combine multiple aggregations with agg()\n",
    "‚òê Avoid custom functions in groupby if possible\n",
    "```\n",
    "\n",
    "### Development Workflow üîÑ\n",
    "\n",
    "```python\n",
    "# 1. Start small\n",
    "df_sample = pd.read_csv('large.csv', nrows=1000)  # Test on small sample\n",
    "# Develop and debug on sample\n",
    "\n",
    "# 2. Profile performance\n",
    "%timeit operation  # Jupyter magic\n",
    "# Or use timer decorator\n",
    "\n",
    "# 3. Check memory\n",
    "df.info(memory_usage='deep')\n",
    "df.memory_usage(deep=True).sum() / 1024**2  # MB\n",
    "\n",
    "# 4. Optimize iteratively\n",
    "# - Identify bottlenecks\n",
    "# - Apply optimizations\n",
    "# - Measure improvements\n",
    "# - Repeat\n",
    "\n",
    "# 5. Scale up\n",
    "# Apply to full dataset once optimized\n",
    "```\n",
    "\n",
    "### Common Anti-Patterns ‚ùå\n",
    "\n",
    "```python\n",
    "# ‚ùå DON'T: Loop over rows\n",
    "for idx, row in df.iterrows():\n",
    "    df.loc[idx, 'new'] = row['a'] + row['b']\n",
    "\n",
    "# ‚úÖ DO: Vectorize\n",
    "df['new'] = df['a'] + df['b']\n",
    "\n",
    "# ‚ùå DON'T: Grow DataFrame in loop\n",
    "df = pd.DataFrame()\n",
    "for i in range(1000):\n",
    "    df = pd.concat([df, new_row])\n",
    "\n",
    "# ‚úÖ DO: Build list, then create DataFrame\n",
    "rows = []\n",
    "for i in range(1000):\n",
    "    rows.append(new_row)\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# ‚ùå DON'T: Use apply for simple operations\n",
    "df['result'] = df['value'].apply(lambda x: x * 2)\n",
    "\n",
    "# ‚úÖ DO: Use vectorized operations\n",
    "df['result'] = df['value'] * 2\n",
    "\n",
    "# ‚ùå DON'T: Read all columns if you need few\n",
    "df = pd.read_csv('large.csv')  # 50 columns\n",
    "df = df[['col1', 'col2']]  # Use only 2\n",
    "\n",
    "# ‚úÖ DO: Read only needed columns\n",
    "df = pd.read_csv('large.csv', usecols=['col1', 'col2'])\n",
    "\n",
    "# ‚ùå DON'T: Use object dtype for categories\n",
    "df['category'] = df['category'].astype('object')\n",
    "\n",
    "# ‚úÖ DO: Use category dtype\n",
    "df['category'] = df['category'].astype('category')\n",
    "```\n",
    "\n",
    "### Performance Targets üéØ\n",
    "\n",
    "```\n",
    "Data Size    Operations Should Be\n",
    "---------    --------------------\n",
    "< 10K rows   Instant (< 0.1s)\n",
    "< 100K       Fast (< 1s)\n",
    "< 1M         Reasonable (< 10s)\n",
    "> 1M         Consider chunking or Dask\n",
    "\n",
    "If slower, optimize!\n",
    "```\n",
    "\n",
    "### When to Use What\n",
    "\n",
    "```python\n",
    "# Vectorized operations\n",
    "When: Always try first\n",
    "Example: df['result'] = df['a'] + df['b']\n",
    "\n",
    "# np.where() / np.select()\n",
    "When: Conditional logic\n",
    "Example: df['category'] = np.where(df['value'] > 0, 'Pos', 'Neg')\n",
    "\n",
    "# apply() with built-in function\n",
    "When: Need to apply built-in function\n",
    "Example: df['result'] = df['col'].apply(len)\n",
    "\n",
    "# apply() with custom function\n",
    "When: Complex logic, can't vectorize\n",
    "Example: df['result'] = df.apply(complex_logic, axis=1)\n",
    "\n",
    "# Numba\n",
    "When: Custom numeric computations, need speed\n",
    "Example: @jit(nopython=True) def compute(arr): ...\n",
    "\n",
    "# Dask/parallel\n",
    "When: Very large datasets, embarrassingly parallel\n",
    "Example: ddf.groupby('key').agg('sum').compute()\n",
    "\n",
    "# Chunking\n",
    "When: Dataset doesn't fit in memory\n",
    "Example: for chunk in pd.read_csv(..., chunksize=10000):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf9b4e5",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "### Beginner Level (1-5)\n",
    "\n",
    "1. **Memory Profiling**\n",
    "   - Check memory usage of a DataFrame\n",
    "   - Identify which columns use most memory\n",
    "   - Calculate total memory in MB\n",
    "\n",
    "2. **Data Type Optimization**\n",
    "   - Convert int64 column (values 0-255) to int8\n",
    "   - Convert float64 to float32\n",
    "   - Convert object column to category\n",
    "\n",
    "3. **Vectorization Basics**\n",
    "   - Replace loop with vectorized arithmetic\n",
    "   - Use np.where() for conditional\n",
    "   - Time both approaches\n",
    "\n",
    "4. **Efficient Reading**\n",
    "   - Read CSV with dtype specification\n",
    "   - Use usecols to read subset\n",
    "   - Compare memory usage\n",
    "\n",
    "5. **String Operations**\n",
    "   - Replace apply() with str accessor\n",
    "   - Measure performance difference\n",
    "\n",
    "### Intermediate Level (6-10)\n",
    "\n",
    "6. **Automatic Optimization**\n",
    "   - Write function to optimize all dtypes\n",
    "   - Handle int, float, and object columns\n",
    "   - Report memory savings\n",
    "\n",
    "7. **Query vs Boolean**\n",
    "   - Implement same filter with both methods\n",
    "   - Benchmark on large DataFrame\n",
    "   - Compare performance\n",
    "\n",
    "8. **Chunked Processing**\n",
    "   - Read large CSV in chunks\n",
    "   - Filter each chunk\n",
    "   - Combine results\n",
    "\n",
    "9. **Efficient GroupBy**\n",
    "   - Compare custom vs built-in aggregation\n",
    "   - Use agg() for multiple aggregations\n",
    "   - Benchmark performance\n",
    "\n",
    "10. **eval() Usage**\n",
    "    - Create complex expression\n",
    "    - Implement with standard operations\n",
    "    - Implement with eval()\n",
    "    - Compare performance\n",
    "\n",
    "### Advanced Level (11-15)\n",
    "\n",
    "11. **File Format Comparison**\n",
    "    - Save DataFrame in CSV, Parquet, Pickle\n",
    "    - Compare write/read times\n",
    "    - Compare file sizes\n",
    "\n",
    "12. **Sparse Arrays**\n",
    "    - Create DataFrame with 90% zeros\n",
    "    - Compare dense vs sparse memory\n",
    "    - Perform operations on sparse\n",
    "\n",
    "13. **Parallel Processing**\n",
    "    - Split DataFrame into chunks\n",
    "    - Process chunks in parallel (multiprocessing)\n",
    "    - Compare with single-threaded\n",
    "\n",
    "14. **Index Optimization**\n",
    "    - Benchmark lookups without index\n",
    "    - Set and sort index\n",
    "    - Benchmark lookups with index\n",
    "    - Calculate speedup\n",
    "\n",
    "15. **Complete Optimization Pipeline**\n",
    "    - Read large CSV\n",
    "    - Optimize dtypes\n",
    "    - Vectorize operations\n",
    "    - Use efficient aggregations\n",
    "    - Save in optimal format\n",
    "\n",
    "### Challenge Problems (16-20)\n",
    "\n",
    "16. **Memory Budget Challenge**\n",
    "    - Given: 10M row DataFrame, 100MB memory limit\n",
    "    - Optimize to fit in budget\n",
    "    - Maintain functionality\n",
    "\n",
    "17. **Speed Challenge**\n",
    "    - Given: Slow data pipeline (1 minute)\n",
    "    - Apply all optimization techniques\n",
    "    - Target: < 5 seconds\n",
    "\n",
    "18. **Large File Processing**\n",
    "    - Process 1GB+ CSV file\n",
    "    - Don't load all into memory\n",
    "    - Perform aggregations\n",
    "    - Save results\n",
    "\n",
    "19. **Real-World Pipeline**\n",
    "    - Read from multiple CSVs\n",
    "    - Merge efficiently\n",
    "    - Apply transformations (vectorized)\n",
    "    - Aggregate results\n",
    "    - Optimize entire pipeline\n",
    "\n",
    "20. **Numba Integration**\n",
    "    - Implement custom function\n",
    "    - Compare Python, NumPy, Numba versions\n",
    "    - Integrate into Pandas workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c931a8f0",
   "metadata": {},
   "source": [
    "## Quick Reference Card\n",
    "\n",
    "### Memory Optimization\n",
    "\n",
    "```python\n",
    "# Check memory\n",
    "df.info(memory_usage='deep')\n",
    "df.memory_usage(deep=True).sum() / 1024**2  # MB\n",
    "\n",
    "# Optimize integers\n",
    "df['col'] = df['col'].astype('int8')    # -128 to 127\n",
    "df['col'] = df['col'].astype('int16')   # -32K to 32K\n",
    "df['col'] = df['col'].astype('int32')   # -2B to 2B\n",
    "\n",
    "# Optimize floats\n",
    "df['col'] = df['col'].astype('float32')  # 7 digits precision\n",
    "\n",
    "# Categoricals\n",
    "df['col'] = df['col'].astype('category')  # For repeated values\n",
    "\n",
    "# String dtype\n",
    "df['col'] = df['col'].astype('string')  # Better than object\n",
    "```\n",
    "\n",
    "### Vectorization\n",
    "\n",
    "```python\n",
    "# Arithmetic (always vectorized)\n",
    "df['result'] = df['a'] + df['b'] * df['c']\n",
    "\n",
    "# Conditionals\n",
    "df['cat'] = np.where(df['val'] > 0, 'Pos', 'Neg')\n",
    "\n",
    "# Multiple conditions\n",
    "conditions = [df['val'] > 100, df['val'] > 50]\n",
    "choices = ['High', 'Medium']\n",
    "df['cat'] = np.select(conditions, choices, default='Low')\n",
    "\n",
    "# String operations\n",
    "df['upper'] = df['text'].str.upper()\n",
    "df['len'] = df['text'].str.len()\n",
    "df['contains'] = df['text'].str.contains('pattern')\n",
    "\n",
    "# Math operations\n",
    "df['sqrt'] = np.sqrt(df['val'])\n",
    "df['log'] = np.log(df['val'])\n",
    "```\n",
    "\n",
    "### Efficient Reading\n",
    "\n",
    "```python\n",
    "# Specify dtypes\n",
    "df = pd.read_csv('file.csv', dtype={'id': 'int32', 'cat': 'category'})\n",
    "\n",
    "# Select columns\n",
    "df = pd.read_csv('file.csv', usecols=['col1', 'col2'])\n",
    "\n",
    "# Parse dates\n",
    "df = pd.read_csv('file.csv', parse_dates=['date'])\n",
    "\n",
    "# Chunked reading\n",
    "for chunk in pd.read_csv('file.csv', chunksize=10000):\n",
    "    process(chunk)\n",
    "\n",
    "# Sampling\n",
    "df = pd.read_csv('file.csv', nrows=1000)  # First 1000\n",
    "df = pd.read_csv('file.csv',  # ~1% sample\n",
    "                 skiprows=lambda i: i > 0 and np.random.random() > 0.01)\n",
    "```\n",
    "\n",
    "### Efficient Operations\n",
    "\n",
    "```python\n",
    "# Query (often faster)\n",
    "df.query('age > 25 and city == \"NYC\"')\n",
    "\n",
    "# eval() for expressions\n",
    "df['result'] = df.eval('a + b * c - d / e')\n",
    "\n",
    "# Efficient groupby\n",
    "df.groupby('cat')['val'].agg(['sum', 'mean', 'count'])\n",
    "\n",
    "# Sort index for slicing\n",
    "df = df.sort_index()\n",
    "df.loc['A':'Z']  # Fast\n",
    "\n",
    "# Set index for lookups\n",
    "df = df.set_index('id')\n",
    "df.loc[12345]  # Fast\n",
    "```\n",
    "\n",
    "### File Formats\n",
    "\n",
    "```python\n",
    "# Parquet (recommended for production)\n",
    "df.to_parquet('file.parquet')\n",
    "df = pd.read_parquet('file.parquet')\n",
    "\n",
    "# Feather (fast temporary storage)\n",
    "df.to_feather('file.feather')\n",
    "df = pd.read_feather('file.feather')\n",
    "\n",
    "# Pickle (Python-only)\n",
    "df.to_pickle('file.pkl')\n",
    "df = pd.read_pickle('file.pkl')\n",
    "\n",
    "# CSV with compression\n",
    "df.to_csv('file.csv.gz', compression='gzip')\n",
    "df = pd.read_csv('file.csv.gz')\n",
    "```\n",
    "\n",
    "### Performance Hierarchy\n",
    "\n",
    "```python\n",
    "# From fastest to slowest:\n",
    "1. Vectorized:           df['c'] = df['a'] + df['b']\n",
    "2. NumPy:                df['c'] = np.sqrt(df['a'])\n",
    "3. Built-in methods:     df['c'] = df['a'].str.upper()\n",
    "4. apply() with lambda:  df['c'] = df['a'].apply(lambda x: x**2)\n",
    "5. apply() custom:       df['c'] = df.apply(func, axis=1)\n",
    "6. itertuples():         for row in df.itertuples(): ...\n",
    "7. iterrows():           for idx, row in df.iterrows(): ...\n",
    "8. Loop with loc:        for i in range(len(df)): df.loc[i, ...]\n",
    "```\n",
    "\n",
    "### Benchmarking\n",
    "\n",
    "```python\n",
    "# Jupyter magic\n",
    "%timeit operation\n",
    "\n",
    "# Python timing\n",
    "import time\n",
    "start = time.time()\n",
    "operation\n",
    "print(f\"Time: {time.time() - start:.4f}s\")\n",
    "\n",
    "# Decorator\n",
    "from functools import wraps\n",
    "def timer(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        print(f\"{func.__name__}: {time.time()-start:.4f}s\")\n",
    "        return result\n",
    "    return wrapper\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a02564",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Optimization Principles üéØ\n",
    "\n",
    "**1. Right Data Types = Less Memory**\n",
    "- int64 ‚Üí int8/int16/int32 (87% savings)\n",
    "- float64 ‚Üí float32 (50% savings)\n",
    "- object ‚Üí category (up to 96% savings)\n",
    "- Use string dtype for text data\n",
    "\n",
    "**2. Vectorize Everything**\n",
    "- Built-in operations: 100-1000x faster\n",
    "- NumPy functions: 50-500x faster\n",
    "- Avoid loops: They are 10-1000x slower\n",
    "- Use np.where() and np.select()\n",
    "\n",
    "**3. Read Smart, Not Hard**\n",
    "- Specify dtypes upfront\n",
    "- Use usecols for column subset\n",
    "- Parse dates during reading\n",
    "- Use chunksize for large files\n",
    "\n",
    "**4. Efficient Operations**\n",
    "- query() often faster than boolean\n",
    "- eval() for complex expressions\n",
    "- Built-in groupby aggregations\n",
    "- Sort index for fast slicing\n",
    "\n",
    "**5. Right Format Matters**\n",
    "- Parquet: Production use (fast, small)\n",
    "- Feather: Temporary storage (fastest)\n",
    "- CSV: Sharing only (slowest, largest)\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Checklist ‚úÖ\n",
    "\n",
    "**Before optimization:**\n",
    "```\n",
    "‚òê Profile memory usage\n",
    "‚òê Identify bottlenecks\n",
    "‚òê Measure current performance\n",
    "```\n",
    "\n",
    "**Data types:**\n",
    "```\n",
    "‚òê Downcast integers\n",
    "‚òê Use float32 instead of float64\n",
    "‚òê Convert to category where appropriate\n",
    "‚òê Use string dtype for text\n",
    "```\n",
    "\n",
    "**Operations:**\n",
    "```\n",
    "‚òê Replace loops with vectorization\n",
    "‚òê Replace apply with built-in methods\n",
    "‚òê Use np.where/np.select for conditionals\n",
    "‚òê Use query() for complex filters\n",
    "‚òê Use eval() for expressions\n",
    "```\n",
    "\n",
    "**I/O:**\n",
    "```\n",
    "‚òê Specify dtypes in read_csv\n",
    "‚òê Use usecols to read subset\n",
    "‚òê Use chunksize for large files\n",
    "‚òê Save in Parquet format\n",
    "```\n",
    "\n",
    "**After optimization:**\n",
    "```\n",
    "‚òê Measure improvements\n",
    "‚òê Verify results unchanged\n",
    "‚òê Document optimizations\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Typical Improvements üìä\n",
    "\n",
    "```\n",
    "Optimization          Typical Speedup    Memory Savings\n",
    "----------------      ---------------    --------------\n",
    "Data types            N/A                50-90%\n",
    "Vectorization         10-1000x           N/A\n",
    "query() vs boolean    1.5-3x             N/A\n",
    "eval()                1.5-2x             Reduces temps\n",
    "Efficient reading     2-5x               50-90%\n",
    "Parquet vs CSV        5-10x              70-90%\n",
    "Index optimization    10-100x            N/A\n",
    "Chunking              Enables process    100% (no OOM)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Decision Tree üå≥\n",
    "\n",
    "**Is it slow?**\n",
    "```\n",
    "‚îú‚îÄ Using loops?\n",
    "‚îÇ  ‚îî‚îÄ Vectorize!\n",
    "‚îú‚îÄ Using apply()?\n",
    "‚îÇ  ‚îî‚îÄ Use built-in method or np.where\n",
    "‚îú‚îÄ Complex filter?\n",
    "‚îÇ  ‚îî‚îÄ Try query()\n",
    "‚îú‚îÄ Complex expression?\n",
    "‚îÇ  ‚îî‚îÄ Try eval()\n",
    "‚îî‚îÄ Still slow?\n",
    "   ‚îî‚îÄ Consider Numba or parallel\n",
    "```\n",
    "\n",
    "**Out of memory?**\n",
    "```\n",
    "‚îú‚îÄ Optimize data types first\n",
    "‚îú‚îÄ Read only needed columns\n",
    "‚îú‚îÄ Use category for repeated strings\n",
    "‚îú‚îÄ Process in chunks\n",
    "‚îî‚îÄ Use Parquet/Feather format\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Remember\n",
    "\n",
    "- üéØ **Measure first** - Don't optimize blindly\n",
    "- ‚ö° **Vectorize always** - Avoid loops at all costs\n",
    "- üíæ **Right dtypes** - Can save 50-90% memory\n",
    "- üìä **Use Parquet** - For production data storage\n",
    "- üîç **Profile often** - Find real bottlenecks\n",
    "- üìñ **Read smart** - Specify dtypes, select columns\n",
    "- üöÄ **Start small** - Test on samples\n",
    "- ‚úÖ **Verify results** - After optimization\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "After mastering performance optimization:\n",
    "1. **Dask** - For distributed computing\n",
    "2. **Vaex** - For out-of-core DataFrames\n",
    "3. **Polars** - Alternative fast DataFrame library\n",
    "4. **Arrow** - For zero-copy data sharing\n",
    "5. **GPU acceleration** - cuDF for NVIDIA GPUs\n",
    "\n",
    "---\n",
    "\n",
    "**Make Pandas Fast! ‚ö°üêº**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
