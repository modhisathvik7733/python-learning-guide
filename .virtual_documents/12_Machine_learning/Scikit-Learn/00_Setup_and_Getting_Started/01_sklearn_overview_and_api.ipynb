





from sklearn.datasets import load_iris, load_diabetes, load_wine, load_breast_cancer
from sklearn.datasets import make_classification, make_regression
import pandas as pd
import numpy as np

# Load classification dataset (Iris)
iris = load_iris()
print("=== IRIS DATASET (Classification) ===")
print(f"Features: {iris.data.shape}")
print(f"Targets: {iris.target.shape}")
print(f"Feature names: {iris.feature_names}")
print(f"Classes: {iris.target_names}")
print(f"Description: {iris.DESCR[:200]}...\n")

# Load regression dataset (Diabetes)
diabetes = load_diabetes()
print("\n=== DIABETES DATASET (Regression) ===")
print(f"Features: {diabetes.data.shape}")
print(f"Target (disease progression): {diabetes.target.shape}")
print(f"Feature names: {diabetes.feature_names}")


# Convert to DataFrame for easier exploration
iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)
iris_df['species'] = iris.target
iris_df['species_name'] = iris_df['species'].map(
    {i: name for i, name in enumerate(iris.target_names)}
)

print("First 5 rows:")
print(iris_df.head())
print("\nDataset statistics:")
print(iris_df.describe())
print("\nClass distribution:")
print(iris_df['species_name'].value_counts())





from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# Load Wine dataset
wine = load_wine()
X, y = wine.data, wine.target

print("Wine Dataset Info:")
print(f"Samples: {X.shape[0]}, Features: {X.shape[1]}")
print(f"Classes: {wine.target_names}")

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define multiple models
models = {
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),
    'SVM': SVC(random_state=42)
}

# Train and evaluate each model
print("\n" + "="*50)
print("MODEL COMPARISON")
print("="*50)

results = {}
for name, model in models.items():
    # Train
    model.fit(X_train_scaled, y_train)
    
    # Predict
    y_pred = model.predict(X_test_scaled)
    
    # Evaluate
    train_acc = model.score(X_train_scaled, y_train)
    test_acc = accuracy_score(y_test, y_pred)
    
    results[name] = {'train': train_acc, 'test': test_acc}
    
    print(f"\n{name}:")
    print(f"  Train Accuracy: {train_acc:.4f}")
    print(f"  Test Accuracy:  {test_acc:.4f}")

# Show best model
best_model = max(results, key=lambda x: results[x]['test'])
print(f"\nüèÜ Best Model: {best_model} (Test Acc: {results[best_model]['test']:.4f})")





from sklearn.datasets import load_diabetes
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Load dataset
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target

print("Diabetes Dataset Info:")
print(f"Samples: {X.shape[0]}, Features: {X.shape[1]}")
print(f"Target range: [{y.min():.1f}, {y.max():.1f}]")
print(f"Features: {diabetes.feature_names}")

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Define regression models
reg_models = {
    'Linear Regression': LinearRegression(),
    'Ridge (L2)': Ridge(alpha=1.0),
    'Lasso (L1)': Lasso(alpha=1.0),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)
}

print("\n" + "="*60)
print("REGRESSION MODEL COMPARISON")
print("="*60)

for name, model in reg_models.items():
    # Train
    model.fit(X_train, y_train)
    
    # Predict
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    
    # Evaluate
    train_r2 = r2_score(y_train, y_pred_train)
    test_r2 = r2_score(y_test, y_pred_test)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
    test_mae = mean_absolute_error(y_test, y_pred_test)
    
    print(f"\n{name}:")
    print(f"  Train R¬≤: {train_r2:.4f}")
    print(f"  Test R¬≤:  {test_r2:.4f}")
    print(f"  Test RMSE: {test_rmse:.2f}")
    print(f"  Test MAE:  {test_mae:.2f}")





from sklearn.model_selection import cross_val_score, cross_validate
from sklearn.datasets import load_breast_cancer

# Load breast cancer dataset
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

print("Breast Cancer Dataset:")
print(f"Samples: {X.shape[0]}, Features: {X.shape[1]}")
print(f"Classes: {cancer.target_names}")
print(f"Class distribution: {np.bincount(y)}")

# Scale data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Test multiple models with 5-fold CV
models_cv = {
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=5000),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(random_state=42)
}

print("\n" + "="*60)
print("5-FOLD CROSS-VALIDATION RESULTS")
print("="*60)

for name, model in models_cv.items():
    # Perform cross-validation
    cv_results = cross_validate(
        model, X_scaled, y, 
        cv=5, 
        scoring=['accuracy', 'precision', 'recall', 'f1'],
        return_train_score=True
    )
    
    print(f"\n{name}:")
    print(f"  Accuracy:  {cv_results['test_accuracy'].mean():.4f} (+/- {cv_results['test_accuracy'].std():.4f})")
    print(f"  Precision: {cv_results['test_precision'].mean():.4f} (+/- {cv_results['test_precision'].std():.4f})")
    print(f"  Recall:    {cv_results['test_recall'].mean():.4f} (+/- {cv_results['test_recall'].std():.4f})")
    print(f"  F1 Score:  {cv_results['test_f1'].mean():.4f} (+/- {cv_results['test_f1'].std():.4f})")





from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier

# Create a pipeline: Scale ‚Üí PCA ‚Üí Classify
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=0.95)),  # Keep 95% variance
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
])

# Load data
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train entire pipeline with one command
pipeline.fit(X_train, y_train)

# Predict
y_pred = pipeline.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print("Pipeline Results:")
print(f"Original features: {X_train.shape[1]}")
print(f"PCA components: {pipeline.named_steps['pca'].n_components_}")
print(f"Test Accuracy: {accuracy:.4f}")

# Access pipeline components
print(f"\nExplained variance ratio: {pipeline.named_steps['pca'].explained_variance_ratio_.sum():.4f}")





from sklearn.datasets import make_classification, make_regression, make_blobs
import matplotlib.pyplot as plt

# 1. Classification data
X_cls, y_cls = make_classification(
    n_samples=1000,
    n_features=20,
    n_informative=15,
    n_redundant=5,
    n_classes=3,
    random_state=42
)
print("Classification dataset:", X_cls.shape, y_cls.shape)

# 2. Regression data
X_reg, y_reg = make_regression(
    n_samples=1000,
    n_features=10,
    n_informative=8,
    noise=10,
    random_state=42
)
print("Regression dataset:", X_reg.shape, y_reg.shape)

# 3. Clustering data
X_clust, y_clust = make_blobs(
    n_samples=500,
    n_features=2,
    centers=4,
    cluster_std=1.0,
    random_state=42
)
print("Clustering dataset:", X_clust.shape)

# Visualize clustering data
plt.figure(figsize=(8, 6))
plt.scatter(X_clust[:, 0], X_clust[:, 1], c=y_clust, cmap='viridis', alpha=0.6)
plt.title('Synthetic Clustering Data')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Cluster')
plt.show()





from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

# Load data
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define parameter grid
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],
    'kernel': ['rbf', 'linear']
}

# Create GridSearchCV
grid_search = GridSearchCV(
    SVC(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

# Fit grid search
print("Searching for best parameters...")
grid_search.fit(X_train_scaled, y_train)

# Results
print(f"\nBest parameters: {grid_search.best_params_}")
print(f"Best cross-validation score: {grid_search.best_score_:.4f}")
print(f"Test set score: {grid_search.score(X_test_scaled, y_test):.4f}")

# Show top 5 configurations
results_df = pd.DataFrame(grid_search.cv_results_)
print("\nTop 5 configurations:")
print(results_df[['params', 'mean_test_score', 'rank_test_score']]
      .sort_values('rank_test_score')
      .head())






