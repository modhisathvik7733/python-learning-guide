


import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Suppress warnings for cleaner output
import warnings
warnings.filterwarnings('ignore')





# Create customer dataset
data = {
    'customer_id': range(1, 11),
    'city': ['Mumbai', 'Delhi', 'Mumbai', 'Bangalore', 'Delhi', 
             'Chennai', 'Mumbai', 'Bangalore', 'Chennai', 'Delhi'],
    'education': ['High School', 'Bachelor', 'Master', 'Bachelor', 'PhD',
                  'High School', 'Master', 'Bachelor', 'PhD', 'Master'],
    'product': ['Laptop', 'Phone', 'Tablet', 'Laptop', 'Phone',
                'Tablet', 'Laptop', 'Phone', 'Tablet', 'Laptop'],
    'satisfaction': ['Good', 'Excellent', 'Poor', 'Good', 'Excellent',
                     'Good', 'Excellent', 'Poor', 'Good', 'Excellent'],
    'purchased': [1, 1, 0, 1, 1, 0, 1, 0, 1, 1]  # Target variable
}

df = pd.DataFrame(data)
print("Original Dataset:")
print(df)
print("\nData Types:")
print(df.dtypes)
print("\nCategorical Columns:")
print(df.select_dtypes(include='object').columns.tolist())





# LabelEncoder for target variable
le = LabelEncoder()

# Example: Encode satisfaction levels
satisfaction = df['satisfaction'].values
print("Original:", satisfaction)

# Fit and transform
satisfaction_encoded = le.fit_transform(satisfaction)
print("Encoded: ", satisfaction_encoded)

# See the mapping
print("\nMapping:")
for i, label in enumerate(le.classes_):
    print(f"  {label:12s} → {i}")

# Inverse transform (get back original)
original_back = le.inverse_transform(satisfaction_encoded)
print("\nInverse transform:", original_back)

# ⚠️ Problem with LabelEncoder for features
print("\n" + "="*60)
print("WHY NOT USE FOR FEATURES?")
print("="*60)

cities = ['Mumbai', 'Delhi', 'Bangalore', 'Chennai']
le_city = LabelEncoder()
encoded_cities = le_city.fit_transform(cities)

print("\nCities encoded with LabelEncoder:")
for city, code in zip(cities, encoded_cities):
    print(f"  {city:12s} → {code}")

print("\n⚠️ Problem: Algorithm thinks Bangalore(0) < Chennai(1) < Delhi(2) < Mumbai(3)")
print("   But cities have NO inherent order!")
print("   Solution: Use OneHotEncoder for nominal features")





# Define custom order for education
education_order = ['High School', 'Bachelor', 'Master', 'PhD']

# OrdinalEncoder with custom order
oe = OrdinalEncoder(categories=[education_order])

# Reshape for sklearn (needs 2D array)
education_data = df[['education']]
print("Original Education:")
print(education_data.values.flatten())

# Fit and transform
education_encoded = oe.fit_transform(education_data)
print("\nEncoded (with order):")
print(education_encoded.flatten())

print("\nMapping (preserves order):")
for i, level in enumerate(education_order):
    print(f"  {i} ← {level}")

# Multiple ordinal columns
print("\n" + "="*60)
print("MULTIPLE ORDINAL COLUMNS")
print("="*60)

satisfaction_order = ['Poor', 'Good', 'Excellent']

# Create ordinal encoder for multiple columns
oe_multi = OrdinalEncoder(categories=[education_order, satisfaction_order])

# Encode both columns
ordinal_features = df[['education', 'satisfaction']]
ordinal_encoded = oe_multi.fit_transform(ordinal_features)

result_df = pd.DataFrame(
    ordinal_encoded,
    columns=['education_encoded', 'satisfaction_encoded']
)
result_df = pd.concat([ordinal_features.reset_index(drop=True), result_df], axis=1)

print("\nOriginal vs Encoded:")
print(result_df)





# OneHotEncoder for city (nominal feature)
ohe = OneHotEncoder(sparse_output=False)  # sparse_output=False for readable output

city_data = df[['city']]
print("Original Cities:")
print(city_data.values.flatten())

# Fit and transform
city_encoded = ohe.fit_transform(city_data)
print("\nOne-Hot Encoded Shape:", city_encoded.shape)
print("\nEncoded Matrix:")
print(city_encoded)

# Get feature names
feature_names = ohe.get_feature_names_out(['city'])
print("\nColumn Names:", feature_names)

# Create readable dataframe
city_encoded_df = pd.DataFrame(city_encoded, columns=feature_names)
city_encoded_df.insert(0, 'original_city', city_data.values)

print("\nReadable Format:")
print(city_encoded_df)


# OneHotEncoder for multiple columns
print("="*60)
print("ONE-HOT ENCODING MULTIPLE COLUMNS")
print("="*60)

# Select nominal features
nominal_features = df[['city', 'product']]
print("\nOriginal Data:")
print(nominal_features)

# Encode
ohe_multi = OneHotEncoder(sparse_output=False)
encoded = ohe_multi.fit_transform(nominal_features)

# Get column names
column_names = ohe_multi.get_feature_names_out(['city', 'product'])
print("\nGenerated Columns:", column_names)

# Create dataframe
encoded_df = pd.DataFrame(encoded, columns=column_names)
print("\nEncoded Result:")
print(encoded_df.head())
print(f"\nShape: {nominal_features.shape} → {encoded_df.shape}")





# Without drop='first'
ohe_full = OneHotEncoder(sparse_output=False)
city_full = ohe_full.fit_transform(df[['city']])

print("Without drop='first':")
print(f"Columns: {ohe_full.get_feature_names_out()}")
print(f"Shape: {city_full.shape}")
print(city_full[:3])

# With drop='first' (recommended for linear models)
ohe_drop = OneHotEncoder(sparse_output=False, drop='first')
city_drop = ohe_drop.fit_transform(df[['city']])

print("\nWith drop='first':")
print(f"Columns: {ohe_drop.get_feature_names_out()}")
print(f"Shape: {city_drop.shape}")
print(city_drop[:3])
print("\n✓ First category (Bangalore) dropped")
print("  If all columns are 0 → Bangalore")





# Training data
train_cities = np.array([['Mumbai'], ['Delhi'], ['Bangalore']])

# Fit encoder
ohe_unknown = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
ohe_unknown.fit(train_cities)

print("Trained on:", train_cities.flatten())
print("Categories learned:", ohe_unknown.categories_)

# Test data with unknown category
test_cities = np.array([['Mumbai'], ['Kolkata'], ['Delhi']])  # Kolkata is new!

print("\nTest data:", test_cities.flatten())

# Transform (without error)
test_encoded = ohe_unknown.transform(test_cities)
print("\nEncoded:")
print(test_encoded)
print("\n'Kolkata' (unknown) → all zeros")





# Original data
print("Original DataFrame:")
print(df[['city', 'product', 'satisfaction']].head())

# get_dummies - all categorical columns
df_dummies = pd.get_dummies(df[['city', 'product', 'satisfaction']])
print("\nAfter get_dummies():")
print(df_dummies.head())
print(f"\nShape: 3 columns → {df_dummies.shape[1]} columns")

# With drop_first
df_dummies_drop = pd.get_dummies(
    df[['city', 'product', 'satisfaction']], 
    drop_first=True
)
print("\nWith drop_first=True:")
print(f"Shape: {df_dummies_drop.shape[1]} columns")
print(df_dummies_drop.head())





# Create larger dataset
np.random.seed(42)
n_samples = 200

cities = np.random.choice(['Mumbai', 'Delhi', 'Bangalore', 'Chennai'], n_samples)
education = np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples)
product = np.random.choice(['Laptop', 'Phone', 'Tablet'], n_samples)
age = np.random.randint(20, 60, n_samples)
salary = np.random.randint(30000, 150000, n_samples)

# Target: purchased (influenced by salary and education)
purchased = ((salary > 80000) & (education != 'High School')).astype(int)
purchased = np.random.permutation(purchased)  # Add randomness

data_large = pd.DataFrame({
    'city': cities,
    'education': education,
    'product': product,
    'age': age,
    'salary': salary,
    'purchased': purchased
})

print("Dataset:")
print(data_large.head(10))
print(f"\nShape: {data_large.shape}")
print(f"Purchase rate: {data_large['purchased'].mean():.2%}")


# Separate features and target
X = data_large.drop('purchased', axis=1)
y = data_large['purchased']

# Split data FIRST
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

print("Training set:", X_train.shape)
print("Test set:    ", X_test.shape)

# Identify column types
nominal_cols = ['city', 'product']
ordinal_cols = ['education']
numeric_cols = ['age', 'salary']

print("\nColumn Types:")
print(f"  Nominal: {nominal_cols}")
print(f"  Ordinal: {ordinal_cols}")
print(f"  Numeric: {numeric_cols}")


# Encode nominal features (city, product)
ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
X_train_nominal = ohe.fit_transform(X_train[nominal_cols])
X_test_nominal = ohe.transform(X_test[nominal_cols])

nominal_feature_names = ohe.get_feature_names_out(nominal_cols)
print("Nominal features encoded:")
print(f"  Original: {nominal_cols}")
print(f"  Encoded:  {nominal_feature_names}")
print(f"  Shape: {X_train[nominal_cols].shape} → {X_train_nominal.shape}")

# Encode ordinal features (education)
education_order = ['High School', 'Bachelor', 'Master', 'PhD']
oe = OrdinalEncoder(categories=[education_order])
X_train_ordinal = oe.fit_transform(X_train[ordinal_cols])
X_test_ordinal = oe.transform(X_test[ordinal_cols])

print("\nOrdinal features encoded:")
print(f"  {ordinal_cols} with order: {education_order}")

# Get numeric features
X_train_numeric = X_train[numeric_cols].values
X_test_numeric = X_test[numeric_cols].values

# Combine all features
X_train_processed = np.hstack([
    X_train_nominal,
    X_train_ordinal,
    X_train_numeric
])

X_test_processed = np.hstack([
    X_test_nominal,
    X_test_ordinal,
    X_test_numeric
])

print("\nFinal processed features:")
print(f"  Training: {X_train_processed.shape}")
print(f"  Test:     {X_test_processed.shape}")


# Train Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_processed, y_train)

# Predictions
y_pred_train = rf.predict(X_train_processed)
y_pred_test = rf.predict(X_test_processed)

# Evaluate
train_acc = accuracy_score(y_train, y_pred_train)
test_acc = accuracy_score(y_test, y_pred_test)

print("="*60)
print("MODEL PERFORMANCE")
print("="*60)
print(f"Training Accuracy: {train_acc:.4f}")
print(f"Test Accuracy:     {test_acc:.4f}")

# Feature importance
all_feature_names = list(nominal_feature_names) + ordinal_cols + numeric_cols
feature_importance = pd.DataFrame({
    'feature': all_feature_names,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

print("\nTop 10 Important Features:")
print(feature_importance.head(10))








# Example: High cardinality feature
cities_many = ['City_' + str(i) for i in range(50)]  # 50 cities
city_sample = np.random.choice(cities_many, 100)

print(f"Number of unique cities: {len(np.unique(city_sample))}")
print("OneHotEncoding would create 50 columns!\n")

# Solution 1: Frequency Encoding
city_counts = pd.Series(city_sample).value_counts()
city_freq_encoded = pd.Series(city_sample).map(city_counts)

print("Frequency Encoding:")
print(f"Original: {city_sample[:5]}")
print(f"Encoded:  {city_freq_encoded[:5].values}")
print("Each city → its frequency count")

# Solution 2: Group rare categories
top_10_cities = city_counts.head(10).index
city_grouped = pd.Series(city_sample).apply(
    lambda x: x if x in top_10_cities else 'Other'
)

print(f"\nGrouping: 50 categories → {len(city_grouped.unique())} categories")
print(f"Top 10 cities kept, rest grouped as 'Other'")



