


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures, KBinsDiscretizer, FunctionTransformer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from datetime import datetime, timedelta

np.random.seed(42)





# Simple example: 2 features
X = np.array([
    [2, 3],
    [4, 5]
])

print("Original Features:")
print("[a, b]")
print(X)

# Create polynomial features (degree 2)
poly = PolynomialFeatures(degree=2, include_bias=True)
X_poly = poly.fit_transform(X)

print("\nPolynomial Features (degree=2):")
print("Feature names:", poly.get_feature_names_out(['a', 'b']))
print(X_poly)

# Verify calculations for first row [2, 3]
print("\nVerification for [2, 3]:")
print(f"  1:      {1}")
print(f"  a:      {2}")
print(f"  b:      {3}")
print(f"  aÂ²:     {2**2}")
print(f"  aÃ—b:    {2*3}")
print(f"  bÂ²:     {3**2}")


# Different parameters
X_sample = np.array([[2, 3]])

print("="*60)
print("POLYNOMIAL FEATURES - DIFFERENT CONFIGURATIONS")
print("="*60)

# Degree 3
poly_d3 = PolynomialFeatures(degree=3, include_bias=False)
X_d3 = poly_d3.fit_transform(X_sample)
print("\nDegree=3, include_bias=False:")
print(f"Features ({X_d3.shape[1]}): {poly_d3.get_feature_names_out(['a', 'b'])}")
print(X_d3)

# Only interaction features (no powers)
poly_interact = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
X_interact = poly_interact.fit_transform(X_sample)
print("\nDegree=2, interaction_only=True:")
print(f"Features ({X_interact.shape[1]}): {poly_interact.get_feature_names_out(['a', 'b'])}")
print(X_interact)
print("(Only a, b, aÃ—b - no aÂ², bÂ²)")





# Generate non-linear data
np.random.seed(42)
X = np.linspace(0, 10, 100).reshape(-1, 1)
y = 0.5 * X**2 + X + 2 + np.random.randn(100, 1) * 3

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Dataset: y = 0.5*xÂ² + x + 2 + noise")
print(f"Training samples: {X_train.shape[0]}")
print(f"Test samples: {X_test.shape[0]}")

# Compare: Linear vs Polynomial Regression
results = {}

for degree in [1, 2, 3, 5]:
    # Create polynomial features
    poly = PolynomialFeatures(degree=degree, include_bias=False)
    X_train_poly = poly.fit_transform(X_train)
    X_test_poly = poly.transform(X_test)
    
    # Train linear regression on polynomial features
    model = LinearRegression()
    model.fit(X_train_poly, y_train)
    
    # Evaluate
    train_score = model.score(X_train_poly, y_train)
    test_score = model.score(X_test_poly, y_test)
    
    results[degree] = {'train_r2': train_score, 'test_r2': test_score}
    
    print(f"\nDegree {degree}:")
    print(f"  Features: {X_train_poly.shape[1]}")
    print(f"  Train RÂ²: {train_score:.4f}")
    print(f"  Test RÂ²:  {test_score:.4f}")

# Plot results
plt.figure(figsize=(12, 4))

for i, degree in enumerate([1, 2, 5]):
    plt.subplot(1, 3, i+1)
    
    # Transform data
    poly = PolynomialFeatures(degree=degree, include_bias=False)
    X_train_poly = poly.fit_transform(X_train)
    
    # Fit model
    model = LinearRegression()
    model.fit(X_train_poly, y_train)
    
    # Plot
    X_plot = np.linspace(0, 10, 300).reshape(-1, 1)
    X_plot_poly = poly.transform(X_plot)
    y_plot = model.predict(X_plot_poly)
    
    plt.scatter(X_train, y_train, alpha=0.5, s=20, label='Training data')
    plt.plot(X_plot, y_plot, 'r-', linewidth=2, label=f'Degree {degree}')
    plt.title(f'Polynomial Degree {degree}\nTest RÂ² = {results[degree]["test_r2"]:.3f}')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.legend()
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\nðŸ’¡ Observation:")
print("  - Degree 1 (linear): Underfitting")
print("  - Degree 2: Good fit (matches true relationship)")
print("  - Degree 5: Might overfit (too flexible)")





# Sample data: ages
ages = np.array([18, 22, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80]).reshape(-1, 1)

print("Original Ages:")
print(ages.flatten())

# Strategy 1: Uniform (equal-width bins)
binner_uniform = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='uniform')
ages_uniform = binner_uniform.fit_transform(ages)

print("\nUniform Binning (4 bins):")
print("Bin edges:", binner_uniform.bin_edges_[0])
print("Binned:", ages_uniform.flatten())

# Strategy 2: Quantile (equal-frequency bins)
binner_quantile = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='quantile')
ages_quantile = binner_quantile.fit_transform(ages)

print("\nQuantile Binning (4 bins):")
print("Bin edges:", binner_quantile.bin_edges_[0])
print("Binned:", ages_quantile.flatten())

# Create comparison dataframe
comparison = pd.DataFrame({
    'Age': ages.flatten(),
    'Uniform_Bin': ages_uniform.flatten().astype(int),
    'Quantile_Bin': ages_quantile.flatten().astype(int)
})

print("\nComparison:")
print(comparison)


# Different encoding options
print("="*60)
print("BINNING ENCODING OPTIONS")
print("="*60)

sample_ages = np.array([22, 35, 48, 61]).reshape(-1, 1)
print(f"\nSample ages: {sample_ages.flatten()}")

# encode='ordinal' - Bin number
binner_ord = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')
binned_ord = binner_ord.fit_transform(sample_ages)
print("\nencode='ordinal':")
print(f"Result: {binned_ord.flatten()}")
print("Meaning: Which bin (0, 1, 2)")

# encode='onehot' - Binary columns
binner_ohe = KBinsDiscretizer(n_bins=3, encode='onehot', strategy='uniform')
binned_ohe = binner_ohe.fit_transform(sample_ages).toarray()
print("\nencode='onehot':")
print(binned_ohe)
print("Meaning: One-hot encoded bins")

# encode='onehot-dense' - Same as onehot but dense array
binner_dense = KBinsDiscretizer(n_bins=3, encode='onehot-dense', strategy='uniform')
binned_dense = binner_dense.fit_transform(sample_ages)
print("\nencode='onehot-dense':")
print(binned_dense)





# Sample salary data
salaries = pd.Series([25000, 35000, 45000, 55000, 65000, 75000, 85000, 95000, 120000, 150000])

print("Original Salaries:")
print(salaries.values)

# pd.cut - Fixed bin edges
bins_cut = pd.cut(salaries, bins=3, labels=['Low', 'Medium', 'High'])
print("\npd.cut (equal-width):")
print(bins_cut)

# pd.qcut - Quantile-based
bins_qcut = pd.qcut(salaries, q=3, labels=['Low', 'Medium', 'High'])
print("\npd.qcut (equal-frequency):")
print(bins_qcut)

# Custom bin edges
bins_custom = pd.cut(
    salaries, 
    bins=[0, 50000, 80000, 200000],
    labels=['Junior', 'Mid-level', 'Senior']
)
print("\nCustom bins:")
print(bins_custom)

# Value counts
print("\nDistribution:")
print(bins_custom.value_counts())





# Create sample date data
dates = pd.date_range(start='2025-01-01', end='2025-12-31', freq='7D')
df_dates = pd.DataFrame({'date': dates})

print("Original Dates:")
print(df_dates.head(10))
print(f"\nTotal dates: {len(df_dates)}")

# Extract date features
df_dates['year'] = df_dates['date'].dt.year
df_dates['month'] = df_dates['date'].dt.month
df_dates['day'] = df_dates['date'].dt.day
df_dates['day_of_week'] = df_dates['date'].dt.dayofweek  # Monday=0, Sunday=6
df_dates['day_name'] = df_dates['date'].dt.day_name()
df_dates['week_of_year'] = df_dates['date'].dt.isocalendar().week
df_dates['quarter'] = df_dates['date'].dt.quarter
df_dates['is_weekend'] = df_dates['day_of_week'].isin([5, 6]).astype(int)
df_dates['is_month_start'] = df_dates['date'].dt.is_month_start.astype(int)
df_dates['is_month_end'] = df_dates['date'].dt.is_month_end.astype(int)

print("\nExtracted Features:")
print(df_dates.head(10))

print("\nFeature Statistics:")
print(f"Unique months: {df_dates['month'].nunique()}")
print(f"Weekend dates: {df_dates['is_weekend'].sum()}")
print(f"Weekday dates: {(1-df_dates['is_weekend']).sum()}")


# Advanced date features
print("="*60)
print("ADVANCED DATE FEATURES")
print("="*60)

# Days since reference date
reference_date = pd.Timestamp('2025-01-01')
df_dates['days_since_start'] = (df_dates['date'] - reference_date).dt.days

# Days until end of year
end_of_year = pd.Timestamp('2025-12-31')
df_dates['days_until_end'] = (end_of_year - df_dates['date']).dt.days

# Cyclical encoding for month (preserves circular nature)
df_dates['month_sin'] = np.sin(2 * np.pi * df_dates['month'] / 12)
df_dates['month_cos'] = np.cos(2 * np.pi * df_dates['month'] / 12)

# Cyclical encoding for day of week
df_dates['dow_sin'] = np.sin(2 * np.pi * df_dates['day_of_week'] / 7)
df_dates['dow_cos'] = np.cos(2 * np.pi * df_dates['day_of_week'] / 7)

print("\nAdvanced Features:")
print(df_dates[[
    'date', 'days_since_start', 'days_until_end', 
    'month_sin', 'month_cos', 'dow_sin', 'dow_cos'
]].head(10))

print("\nðŸ’¡ Why Cyclical Encoding?")
print("  - December (12) and January (1) are close in time")
print("  - Standard encoding: |12 - 1| = 11 (far apart)")
print("  - Cyclical encoding: Captures circular relationship")





# Generate synthetic sales data
np.random.seed(42)
dates = pd.date_range(start='2024-01-01', end='2025-12-31', freq='D')
n_days = len(dates)

# Create sales with patterns
sales = (
    100 +  # Base sales
    30 * np.sin(2 * np.pi * np.arange(n_days) / 365) +  # Yearly seasonality
    20 * np.sin(2 * np.pi * np.arange(n_days) / 7) +    # Weekly pattern
    np.random.randn(n_days) * 10  # Noise
)

# Add weekend boost
day_of_week = pd.to_datetime(dates).dayofweek
weekend_boost = np.where(day_of_week.isin([5, 6]), 25, 0)
sales = sales + weekend_boost

# Create dataframe
df_sales = pd.DataFrame({'date': dates, 'sales': sales})

print("Sales Data Sample:")
print(df_sales.head(10))
print(f"\nDate range: {df_sales['date'].min()} to {df_sales['date'].max()}")
print(f"Average sales: ${df_sales['sales'].mean():.2f}")

# Extract features
df_sales['day_of_week'] = df_sales['date'].dt.dayofweek
df_sales['month'] = df_sales['date'].dt.month
df_sales['quarter'] = df_sales['date'].dt.quarter
df_sales['is_weekend'] = df_sales['day_of_week'].isin([5, 6]).astype(int)
df_sales['day_of_year'] = df_sales['date'].dt.dayofyear

# Cyclical features
df_sales['dow_sin'] = np.sin(2 * np.pi * df_sales['day_of_week'] / 7)
df_sales['dow_cos'] = np.cos(2 * np.pi * df_sales['day_of_week'] / 7)
df_sales['month_sin'] = np.sin(2 * np.pi * df_sales['month'] / 12)
df_sales['month_cos'] = np.cos(2 * np.pi * df_sales['month'] / 12)

# Prepare for modeling
feature_cols = ['day_of_week', 'month', 'quarter', 'is_weekend', 
                'day_of_year', 'dow_sin', 'dow_cos', 'month_sin', 'month_cos']
X = df_sales[feature_cols]
y = df_sales['sales']

# Train-test split (last 3 months for testing)
split_date = '2025-10-01'
train_mask = df_sales['date'] < split_date
test_mask = df_sales['date'] >= split_date

X_train, y_train = X[train_mask], y[train_mask]
X_test, y_test = X[test_mask], y[test_mask]

print(f"\nTraining samples: {len(X_train)}")
print(f"Test samples: {len(X_test)}")

# Train model
model = LinearRegression()
model.fit(X_train, y_train)

# Predictions
y_pred_train = model.predict(X_train)
y_pred_test = model.predict(X_test)

# Evaluate
train_r2 = r2_score(y_train, y_pred_train)
test_r2 = r2_score(y_test, y_pred_test)
test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))

print("\n" + "="*60)
print("MODEL PERFORMANCE")
print("="*60)
print(f"Train RÂ²: {train_r2:.4f}")
print(f"Test RÂ²:  {test_r2:.4f}")
print(f"Test RMSE: ${test_rmse:.2f}")

# Feature importance
feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'coefficient': model.coef_
}).sort_values('coefficient', key=abs, ascending=False)

print("\nFeature Importance (by coefficient magnitude):")
print(feature_importance)





# Example: House price features
house_data = pd.DataFrame({
    'area_sqft': [1000, 1500, 2000, 2500, 3000],
    'price': [200000, 280000, 350000, 420000, 480000],
    'bedrooms': [2, 3, 3, 4, 4],
    'age_years': [5, 10, 15, 20, 25]
})

print("Original Data:")
print(house_data)

# Create custom features
house_data['price_per_sqft'] = house_data['price'] / house_data['area_sqft']
house_data['sqft_per_bedroom'] = house_data['area_sqft'] / house_data['bedrooms']
house_data['log_price'] = np.log(house_data['price'])
house_data['age_squared'] = house_data['age_years'] ** 2

print("\nWith Engineered Features:")
print(house_data)

# Using FunctionTransformer
print("\n" + "="*60)
print("USING FunctionTransformer")
print("="*60)

# Log transformer
log_transformer = FunctionTransformer(np.log1p, validate=True)  # log1p = log(1+x)
prices = house_data[['price']].values
prices_log = log_transformer.fit_transform(prices)

print("\nLog transformation:")
print(f"Original: {prices.flatten()}")
print(f"Log(1+x): {prices_log.flatten()}")

# Square root transformer
sqrt_transformer = FunctionTransformer(np.sqrt, validate=True)
area = house_data[['area_sqft']].values
area_sqrt = sqrt_transformer.fit_transform(area)

print("\nSquare root transformation:")
print(f"Original: {area.flatten()}")
print(f"Sqrt:     {area_sqrt.flatten()}")

# Custom function
def price_category(X):
    """Categorize price: 0=low, 1=medium, 2=high"""
    return np.select(
        [X < 250000, X < 400000],
        [0, 1],
        default=2
    ).reshape(-1, 1)

category_transformer = FunctionTransformer(price_category, validate=True)
price_categories = category_transformer.fit_transform(prices)

print("\nCustom categorization:")
print(f"Prices:     {prices.flatten()}")
print(f"Categories: {price_categories.flatten()}")
print("  0 = Low (<$250k), 1 = Medium (<$400k), 2 = High")





# Example: Customer analytics
customer_data = pd.DataFrame({
    'total_purchases': [10, 25, 50, 100, 5],
    'total_spent': [500, 1200, 2500, 5000, 200],
    'days_active': [30, 90, 180, 365, 15],
    'returns': [1, 2, 3, 5, 1]
})

print("Original Customer Data:")
print(customer_data)

# Create interaction features
customer_data['avg_purchase_value'] = (
    customer_data['total_spent'] / customer_data['total_purchases']
)
customer_data['purchases_per_day'] = (
    customer_data['total_purchases'] / customer_data['days_active']
)
customer_data['return_rate'] = (
    customer_data['returns'] / customer_data['total_purchases']
)
customer_data['spending_per_day'] = (
    customer_data['total_spent'] / customer_data['days_active']
)

print("\nWith Interaction Features:")
print(customer_data.round(2))

print("\nðŸ’¡ These features reveal:")
print("  - avg_purchase_value: Customer spending power")
print("  - purchases_per_day: Engagement level")
print("  - return_rate: Product satisfaction")
print("  - spending_per_day: Revenue potential")



