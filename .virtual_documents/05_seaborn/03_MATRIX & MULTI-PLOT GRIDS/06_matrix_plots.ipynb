


# Setup
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

sns.set_theme(style='white', context='notebook')
np.random.seed(42)






# Create a synthetic e-commerce KPI dataset (weekly)
n = 120  # weeks

# Marketing + traffic
ad_spend = np.random.lognormal(mean=9.0, sigma=0.35, size=n)  # in currency units
sessions = (ad_spend / 20 + np.random.normal(0, 1200, size=n)).clip(2000, None)

# Funnel rates (bounded 0..1)
add_to_cart_rate = np.clip(0.06 + 0.000002 * sessions + np.random.normal(0, 0.01, n), 0.02, 0.25)
checkout_rate = np.clip(0.35 + 0.25 * add_to_cart_rate + np.random.normal(0, 0.03, n), 0.10, 0.80)
conversion_rate = np.clip(0.008 + 0.045 * add_to_cart_rate + 0.020 * checkout_rate + np.random.normal(0, 0.003, n), 0.002, 0.08)

# Outcomes
orders = (sessions * conversion_rate + np.random.normal(0, 25, n)).clip(10, None)

# Average order value influenced by discounting (not modeled explicitly) + some noise
# (add some correlation with checkout_rate as a proxy for checkout UX)
aov = np.clip(900 + 600 * checkout_rate + np.random.normal(0, 120, n), 300, 2500)

revenue = orders * aov

# Return rate: mildly correlated with higher AOV + random seasonality
season = np.sin(np.linspace(0, 6*np.pi, n))
return_rate = np.clip(0.10 + 0.00002 * (aov - 900) + 0.03 * (season > 0) + np.random.normal(0, 0.02, n), 0.02, 0.35)

# Add a few extreme outliers to demonstrate robust scaling
outlier_weeks = np.random.choice(np.arange(n), size=4, replace=False)
revenue[outlier_weeks] *= np.random.uniform(2.5, 4.5, size=4)

kpi = pd.DataFrame({
    'ad_spend': ad_spend,
    'sessions': sessions,
    'add_to_cart_rate': add_to_cart_rate,
    'checkout_rate': checkout_rate,
    'conversion_rate': conversion_rate,
    'orders': orders,
    'aov': aov,
    'revenue': revenue,
    'return_rate': return_rate,
})

kpi.head()



# Basic checks
print(kpi.shape)
print(kpi.describe().round(3))






# Correlation matrix
corr = kpi.corr(numeric_only=True)

plt.figure(figsize=(10, 7))
sns.heatmap(
    corr,
    cmap='vlag',
    center=0,
    vmin=-1,
    vmax=1,
    annot=True,
    fmt='.2f',
    linewidths=0.5
)
plt.title('Correlation heatmap (annotated, centered at 0)')
plt.show()






# Mask upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

plt.figure(figsize=(10, 7))
sns.heatmap(
    corr,
    mask=mask,
    cmap='vlag',
    center=0,
    vmin=-1,
    vmax=1,
    annot=True,
    fmt='.2f',
    linewidths=0.5,
    square=True
)
plt.title('Correlation heatmap (lower triangle only)')
plt.show()






# Compare robust vs non-robust on a covariance-like matrix
# Create a "scaled revenue" feature to amplify outlier impact
kpi2 = kpi.copy()
kpi2['revenue_scaled'] = kpi2['revenue'] / 1e6

corr2 = kpi2[['ad_spend','sessions','orders','aov','revenue_scaled','return_rate']].corr()
mask2 = np.triu(np.ones_like(corr2, dtype=bool))

fig, axes = plt.subplots(1, 2, figsize=(14, 6), constrained_layout=True)

sns.heatmap(
    corr2,
    mask=mask2,
    cmap='vlag',
    center=0,
    vmin=-1,
    vmax=1,
    annot=True,
    fmt='.2f',
    ax=axes[0]
)
axes[0].set_title('Correlation (fixed vmin/vmax)')

sns.heatmap(
    corr2,
    mask=mask2,
    cmap='vlag',
    center=0,
    robust=True,
    annot=True,
    fmt='.2f',
    ax=axes[1]
)
axes[1].set_title('Correlation (robust=True)')

plt.show()






# Synthetic churn labels
m = 1200
true_churn = (np.random.rand(m) < 0.22).astype(int)  # 22% churn

# A model with some skill: higher recall than random, but not perfect
# Generate predicted probabilities with some separation
scores = np.where(true_churn == 1,
                  np.random.beta(4, 3, size=m),   # churners have higher scores
                  np.random.beta(2, 6, size=m))   # non-churners have lower scores

threshold = 0.45
pred_churn = (scores >= threshold).astype(int)

# Confusion matrix counts
# rows = true, cols = predicted
cm = pd.crosstab(
    pd.Series(true_churn, name='True'),
    pd.Series(pred_churn, name='Predicted')
)

cm



# Plot confusion matrix with annotations
plt.figure(figsize=(6, 5))
sns.heatmap(
    cm,
    annot=True,
    fmt='d',
    cmap='Blues',
    cbar=False,
    linewidths=0.5
)
plt.title('Confusion matrix heatmap (synthetic churn)')
plt.show()






# Create a cohort-style dataset: product categories vs weekly metrics
categories = ['Electronics', 'Fashion', 'Home', 'Beauty', 'Sports', 'Books']
weeks = pd.date_range('2024-01-01', periods=24, freq='W')

rows = []
for cat in categories:
    cat_factor = {
        'Electronics': (1.3, 1.15),
        'Fashion': (1.0, 0.95),
        'Home': (0.9, 1.05),
        'Beauty': (0.85, 1.10),
        'Sports': (1.05, 1.00),
        'Books': (0.7, 0.90),
    }[cat]
    spend_factor, conv_factor = cat_factor

    for w in weeks:
        base_spend = np.random.lognormal(8.5, 0.25) * spend_factor
        base_sessions = (base_spend / 18 + np.random.normal(0, 900)).clip(1500, None)
        base_conv = np.clip(np.random.normal(0.018, 0.004) * conv_factor, 0.005, 0.05)
        base_aov = np.clip(np.random.normal(1200, 220) * (1.15 if cat == 'Electronics' else 0.95), 300, 3500)
        base_orders = base_sessions * base_conv
        base_revenue = base_orders * base_aov

        rows.append({
            'category': cat,
            'week': w,
            'ad_spend': base_spend,
            'sessions': base_sessions,
            'conversion_rate': base_conv,
            'aov': base_aov,
            'revenue': base_revenue,
        })

cohort = pd.DataFrame(rows)

# Aggregate by category (mean over weeks)
cohort_mean = cohort.groupby('category')[['ad_spend','sessions','conversion_rate','aov','revenue']].mean()
cohort_mean



# Clustermap on category-level KPIs
# standard_scale=1 standardizes each column so clustering isn't dominated by high-magnitude features like revenue
sns.clustermap(
    cohort_mean,
    standard_scale=1,
    method='average',
    metric='euclidean',
    cmap='mako',
    linewidths=0.5,
    figsize=(8, 6)
)
plt.suptitle('Clustermap: product category KPIs (standardized by column)', y=1.02)
plt.show()






# Practice template (fill TODOs)

# TODO A1-A3

# TODO B4

# TODO B5 (challenge): precision, recall

# TODO C6-C7

from typing import Any

def plot_corr_triangle(df: pd.DataFrame, **kwargs: Any):
    # TODO: implement masked correlation heatmap
    pass




