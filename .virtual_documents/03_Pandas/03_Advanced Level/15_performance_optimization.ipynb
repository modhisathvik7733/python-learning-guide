


import pandas as pd
import numpy as np
import time
import sys
from functools import wraps

# Display settings
pd.set_option('display.max_rows', 20)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.precision', 2)

# Timer decorator for measuring performance
def timer(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        end = time.time()
        print(f"{func.__name__}: {end - start:.4f}s")
        return result
    return wrapper

print("✅ Libraries imported")
print(f"Pandas version: {pd.__version__}")
print(f"NumPy version: {np.__version__}")





print("=== MEMORY OPTIMIZATION ===\n")

# Example 1: Data type impact
print("Example 1: Integer data type comparison\n")
n = 1_000_000
values = np.random.randint(0, 100, n)

df_int64 = pd.DataFrame({'value': values})
df_int8 = pd.DataFrame({'value': values.astype('int8')})

mem_int64 = df_int64.memory_usage(deep=True).sum() / 1024**2
mem_int8 = df_int8.memory_usage(deep=True).sum() / 1024**2

print(f"1M integers (0-100):")
print(f"  int64:  {mem_int64:.2f} MB")
print(f"  int8:   {mem_int8:.2f} MB")
print(f"  Savings: {(1 - mem_int8/mem_int64)*100:.1f}%")
print()

# Example 2: Float precision
print("="*70)
print("Example 2: Float precision impact\n")
float_values = np.random.randn(n)

df_float64 = pd.DataFrame({'value': float_values})
df_float32 = pd.DataFrame({'value': float_values.astype('float32')})

mem_float64 = df_float64.memory_usage(deep=True).sum() / 1024**2
mem_float32 = df_float32.memory_usage(deep=True).sum() / 1024**2

print(f"1M floats:")
print(f"  float64: {mem_float64:.2f} MB")
print(f"  float32: {mem_float32:.2f} MB")
print(f"  Savings: {(1 - mem_float32/mem_float64)*100:.1f}%")
print()

# Example 3: Categorical for strings
print("="*70)
print("Example 3: String vs Categorical\n")
categories = ['A', 'B', 'C', 'D', 'E']
strings = np.random.choice(categories, n)

df_object = pd.DataFrame({'category': strings})
df_category = pd.DataFrame({'category': pd.Categorical(strings)})

mem_object = df_object.memory_usage(deep=True).sum() / 1024**2
mem_category = df_category.memory_usage(deep=True).sum() / 1024**2

print(f"1M strings (5 unique values):")
print(f"  object:    {mem_object:.2f} MB")
print(f"  category:  {mem_category:.2f} MB")
print(f"  Savings: {(1 - mem_category/mem_object)*100:.1f}%")
print()

# Example 4: Automatic optimization function
print("="*70)
print("Example 4: Automatic dtype optimization\n")

def optimize_dtypes(df):
    """Optimize DataFrame data types"""
    for col in df.select_dtypes(include=['int']).columns:
        col_min = df[col].min()
        col_max = df[col].max()
        
        if col_min >= 0:
            if col_max < 255:
                df[col] = df[col].astype('uint8')
            elif col_max < 65535:
                df[col] = df[col].astype('uint16')
        else:
            if col_min > -128 and col_max < 127:
                df[col] = df[col].astype('int8')
            elif col_min > -32768 and col_max < 32767:
                df[col] = df[col].astype('int16')
    
    for col in df.select_dtypes(include=['float']).columns:
        df[col] = df[col].astype('float32')
    
    for col in df.select_dtypes(include=['object']).columns:
        num_unique = df[col].nunique()
        if num_unique / len(df) < 0.5:
            df[col] = df[col].astype('category')
    
    return df

# Create test DataFrame
df_test = pd.DataFrame({
    'age': np.random.randint(0, 100, 10000),
    'score': np.random.randn(10000),
    'category': np.random.choice(['A', 'B', 'C'], 10000)
})

print("Before optimization:")
print(df_test.dtypes)
mem_before = df_test.memory_usage(deep=True).sum() / 1024
print(f"Memory: {mem_before:.2f} KB")

df_optimized = optimize_dtypes(df_test.copy())
print("\nAfter optimization:")
print(df_optimized.dtypes)
mem_after = df_optimized.memory_usage(deep=True).sum() / 1024
print(f"Memory: {mem_after:.2f} KB")
print(f"\nSavings: {(1 - mem_after/mem_before)*100:.1f}%")





print("=== VECTORIZATION PERFORMANCE ===\n")

# Create test data
n = 100_000
df = pd.DataFrame({
    'a': np.random.randint(0, 100, n),
    'b': np.random.randint(0, 100, n),
    'value': np.random.randn(n),
    'name': np.random.choice(['alice', 'bob', 'charlie'], n)
})

# Example 1: Simple arithmetic
print("Example 1: Arithmetic operations\n")

# Method 1: Loop (slowest)
start = time.time()
result = []
for i in range(len(df)):
    result.append(df.loc[i, 'a'] + df.loc[i, 'b'])
df['result_loop'] = result
time_loop = time.time() - start
print(f"Loop:       {time_loop:.4f}s")

# Method 2: iterrows
start = time.time()
result = []
for idx, row in df.iterrows():
    result.append(row['a'] + row['b'])
df['result_iterrows'] = result
time_iterrows = time.time() - start
print(f"iterrows:   {time_iterrows:.4f}s")

# Method 3: apply
start = time.time()
df['result_apply'] = df.apply(lambda row: row['a'] + row['b'], axis=1)
time_apply = time.time() - start
print(f"apply:      {time_apply:.4f}s")

# Method 4: Vectorized
start = time.time()
df['result_vectorized'] = df['a'] + df['b']
time_vec = time.time() - start
print(f"Vectorized: {time_vec:.4f}s")

print(f"\nSpeedup: {time_loop/time_vec:.0f}x faster!")
print()

# Example 2: Conditional logic
print("="*70)
print("Example 2: Conditional operations\n")

# Method 1: apply
start = time.time()
df['category_apply'] = df['value'].apply(
    lambda x: 'Positive' if x > 0 else 'Negative'
)
time_apply = time.time() - start
print(f"apply:      {time_apply:.4f}s")

# Method 2: np.where (vectorized)
start = time.time()
df['category_where'] = np.where(df['value'] > 0, 'Positive', 'Negative')
time_where = time.time() - start
print(f"np.where:   {time_where:.4f}s")

print(f"\nSpeedup: {time_apply/time_where:.0f}x faster!")
print()

# Example 3: Multiple conditions
print("="*70)
print("Example 3: Multiple conditions\n")

# Method 1: apply with if-elif-else
def categorize_apply(x):
    if x > 1:
        return 'High'
    elif x > 0:
        return 'Medium'
    elif x > -1:
        return 'Low'
    else:
        return 'Very Low'

start = time.time()
df['level_apply'] = df['value'].apply(categorize_apply)
time_apply = time.time() - start
print(f"apply:      {time_apply:.4f}s")

# Method 2: np.select (vectorized)
start = time.time()
conditions = [
    df['value'] > 1,
    df['value'] > 0,
    df['value'] > -1
]
choices = ['High', 'Medium', 'Low']
df['level_select'] = np.select(conditions, choices, default='Very Low')
time_select = time.time() - start
print(f"np.select:  {time_select:.4f}s")

print(f"\nSpeedup: {time_apply/time_select:.0f}x faster!")
print()

# Example 4: String operations
print("="*70)
print("Example 4: String operations\n")

# Method 1: apply
start = time.time()
df['name_upper_apply'] = df['name'].apply(lambda x: x.upper())
time_apply = time.time() - start
print(f"apply:      {time_apply:.4f}s")

# Method 2: str accessor (vectorized)
start = time.time()
df['name_upper_str'] = df['name'].str.upper()
time_str = time.time() - start
print(f"str.upper:  {time_str:.4f}s")

print(f"\nSpeedup: {time_apply/time_str:.0f}x faster!")
print()

# Example 5: Mathematical operations
print("="*70)
print("Example 5: Math operations\n")

# Create positive values for sqrt
df['positive'] = np.abs(df['value']) + 1

# Method 1: apply
start = time.time()
df['sqrt_apply'] = df['positive'].apply(lambda x: x**0.5)
time_apply = time.time() - start
print(f"apply:      {time_apply:.4f}s")

# Method 2: NumPy (vectorized)
start = time.time()
df['sqrt_numpy'] = np.sqrt(df['positive'])
time_numpy = time.time() - start
print(f"np.sqrt:    {time_numpy:.4f}s")

print(f"\nSpeedup: {time_apply/time_numpy:.0f}x faster!")





print("=== EFFICIENT OPERATIONS ===\n")

# Create test data
n = 100_000
df = pd.DataFrame({
    'age': np.random.randint(20, 70, n),
    'city': np.random.choice(['NYC', 'LA', 'Chicago'], n),
    'score': np.random.randint(50, 100, n),
    'a': np.random.randn(n),
    'b': np.random.randn(n),
    'c': np.random.randn(n),
    'd': np.random.randn(n),
    'e': np.random.randn(n) + 1  # Avoid division by zero
})

# Example 1: query() vs boolean indexing
print("Example 1: query() vs boolean indexing\n")

# Boolean indexing
start = time.time()
result1 = df[(df['age'] > 25) & (df['city'] == 'NYC') & (df['score'] > 80)]
time_bool = time.time() - start
print(f"Boolean:  {time_bool:.4f}s")

# query()
start = time.time()
result2 = df.query('age > 25 and city == "NYC" and score > 80')
time_query = time.time() - start
print(f"query():  {time_query:.4f}s")

print(f"\nResults match: {len(result1) == len(result2)}")
if time_bool > time_query:
    print(f"query() is {time_bool/time_query:.1f}x faster")
else:
    print(f"Boolean is {time_query/time_bool:.1f}x faster")
print()

# Example 2: eval() for complex expressions
print("="*70)
print("Example 2: eval() for expressions\n")

# Standard operation
start = time.time()
df['result_standard'] = df['a'] + df['b'] * df['c'] - df['d'] / df['e']
time_standard = time.time() - start
print(f"Standard: {time_standard:.4f}s")

# eval()
start = time.time()
df['result_eval'] = df.eval('a + b * c - d / e')
time_eval = time.time() - start
print(f"eval():   {time_eval:.4f}s")

print(f"\neval() is {time_standard/time_eval:.1f}x faster")
print()

# Example 3: Efficient groupby
print("="*70)
print("Example 3: GroupBy performance\n")

# Single aggregation
start = time.time()
result1 = df.groupby('city')['score'].sum()
time1 = time.time() - start
print(f"Single agg:    {time1:.4f}s")

# Multiple separate aggregations
start = time.time()
sum_result = df.groupby('city')['score'].sum()
mean_result = df.groupby('city')['score'].mean()
count_result = df.groupby('city')['score'].count()
time2 = time.time() - start
print(f"Separate aggs: {time2:.4f}s")

# Combined aggregation (efficient)
start = time.time()
result3 = df.groupby('city')['score'].agg(['sum', 'mean', 'count'])
time3 = time.time() - start
print(f"Combined agg:  {time3:.4f}s")

print(f"\nCombined is {time2/time3:.1f}x faster than separate")
print()

# Example 4: Index optimization
print("="*70)
print("Example 4: Index optimization\n")

# Create DataFrame with random order
df_unsorted = df.copy()
df_unsorted.index = np.random.permutation(df.index)

# Selection without sorted index
start = time.time()
for i in range(100):
    _ = df_unsorted.loc[df_unsorted['city'] == 'NYC']
time_unsorted = time.time() - start
print(f"Unsorted index: {time_unsorted:.4f}s (100 selections)")

# Set and sort index
df_sorted = df.set_index('city').sort_index()

# Selection with sorted index
start = time.time()
for i in range(100):
    _ = df_sorted.loc['NYC']
time_sorted = time.time() - start
print(f"Sorted index:   {time_sorted:.4f}s (100 selections)")

print(f"\nSorted index is {time_unsorted/time_sorted:.1f}x faster")
print()

# Example 5: Method chaining
print("="*70)
print("Example 5: Efficient method chaining\n")

# Chained operations
start = time.time()
result = (df
    .query('age > 30')
    .groupby('city')['score']
    .mean()
    .sort_values(ascending=False)
)
time_chain = time.time() - start

print("Chained operation result:")
print(result)
print(f"\nTime: {time_chain:.4f}s")





print("=== LARGE DATA HANDLING ===\n")

# Create sample large CSV for demonstration
n = 1_000_000
sample_data = pd.DataFrame({
    'id': range(n),
    'category': np.random.choice(['A', 'B', 'C'], n),
    'value': np.random.randn(n),
    'amount': np.random.randint(0, 1000, n),
    'date': pd.date_range('2020-01-01', periods=n, freq='T')
})
sample_data.to_csv('large_sample.csv', index=False)
print("✅ Created large_sample.csv (1M rows)\n")

# Example 1: Default vs optimized reading
print("Example 1: Optimized CSV reading\n")

# Default reading
start = time.time()
df_default = pd.read_csv('large_sample.csv')
time_default = time.time() - start
mem_default = df_default.memory_usage(deep=True).sum() / 1024**2
print(f"Default read:")
print(f"  Time: {time_default:.2f}s")
print(f"  Memory: {mem_default:.1f} MB")

# Optimized reading
start = time.time()
df_optimized = pd.read_csv('large_sample.csv',
                           dtype={'id': 'int32',
                                  'category': 'category',
                                  'value': 'float32',
                                  'amount': 'int16'},
                           parse_dates=['date'])
time_optimized = time.time() - start
mem_optimized = df_optimized.memory_usage(deep=True).sum() / 1024**2
print(f"\nOptimized read:")
print(f"  Time: {time_optimized:.2f}s")
print(f"  Memory: {mem_optimized:.1f} MB")

print(f"\nMemory savings: {(1 - mem_optimized/mem_default)*100:.1f}%")
print()

# Example 2: Select only needed columns
print("="*70)
print("Example 2: Column selection (usecols)\n")

# Read all columns
start = time.time()
df_all = pd.read_csv('large_sample.csv')
time_all = time.time() - start
mem_all = df_all.memory_usage(deep=True).sum() / 1024**2
print(f"All columns (5):")
print(f"  Time: {time_all:.2f}s")
print(f"  Memory: {mem_all:.1f} MB")

# Read only 2 columns
start = time.time()
df_subset = pd.read_csv('large_sample.csv', usecols=['id', 'value'])
time_subset = time.time() - start
mem_subset = df_subset.memory_usage(deep=True).sum() / 1024**2
print(f"\nOnly 2 columns:")
print(f"  Time: {time_subset:.2f}s")
print(f"  Memory: {mem_subset:.1f} MB")

print(f"\nMemory savings: {(1 - mem_subset/mem_all)*100:.1f}%")
print(f"Speed improvement: {time_all/time_subset:.1f}x")
print()

# Example 3: Chunked processing
print("="*70)
print("Example 3: Chunked processing\n")

# Process in chunks and filter
chunk_size = 100_000
results = []

start = time.time()
for i, chunk in enumerate(pd.read_csv('large_sample.csv', chunksize=chunk_size)):
    # Filter chunk
    filtered = chunk[chunk['amount'] > 500]
    results.append(filtered)
    if i == 0:
        print(f"Processing chunk {i+1}: {len(filtered)} rows kept")

df_filtered = pd.concat(results, ignore_index=True)
time_chunk = time.time() - start

print(f"\nProcessed {len(sample_data)} rows in {time_chunk:.2f}s")
print(f"Result: {len(df_filtered)} rows (filtered)")
print()

# Example 4: Aggregation without loading all data
print("="*70)
print("Example 4: Chunk aggregation\n")

# Calculate sum and count without loading full DataFrame
total_sum = 0
total_count = 0

start = time.time()
for chunk in pd.read_csv('large_sample.csv', chunksize=100_000):
    total_sum += chunk['amount'].sum()
    total_count += len(chunk)

mean_value = total_sum / total_count
time_agg = time.time() - start

print(f"Total sum: {total_sum:,.0f}")
print(f"Mean: {mean_value:.2f}")
print(f"Time: {time_agg:.2f}s")
print()

# Example 5: Sampling
print("="*70)
print("Example 5: Sampling strategies\n")

# Read first N rows
start = time.time()
df_head = pd.read_csv('large_sample.csv', nrows=10_000)
time_head = time.time() - start
print(f"First 10K rows: {time_head:.4f}s")

# Read random sample (~1%)
start = time.time()
df_sample = pd.read_csv('large_sample.csv',
                        skiprows=lambda i: i > 0 and np.random.random() > 0.01)
time_sample = time.time() - start
print(f"Random 1% sample: {time_sample:.4f}s ({len(df_sample)} rows)")
print()

# Example 6: File format comparison
print("="*70)
print("Example 6: File format performance\n")

# Prepare smaller dataset for format comparison
df_format = sample_data.head(100_000)

# CSV
start = time.time()
df_format.to_csv('test.csv', index=False)
time_csv_write = time.time() - start
start = time.time()
_ = pd.read_csv('test.csv')
time_csv_read = time.time() - start
import os
size_csv = os.path.getsize('test.csv') / 1024**2

# Parquet
start = time.time()
df_format.to_parquet('test.parquet')
time_parquet_write = time.time() - start
start = time.time()
_ = pd.read_parquet('test.parquet')
time_parquet_read = time.time() - start
size_parquet = os.path.getsize('test.parquet') / 1024**2

# Pickle
start = time.time()
df_format.to_pickle('test.pkl')
time_pickle_write = time.time() - start
start = time.time()
_ = pd.read_pickle('test.pkl')
time_pickle_read = time.time() - start
size_pickle = os.path.getsize('test.pkl') / 1024**2

print(f"{'Format':<10} {'Write':<10} {'Read':<10} {'Size (MB)':<10}")
print("-" * 40)
print(f"{'CSV':<10} {time_csv_write:<10.3f} {time_csv_read:<10.3f} {size_csv:<10.1f}")
print(f"{'Parquet':<10} {time_parquet_write:<10.3f} {time_parquet_read:<10.3f} {size_parquet:<10.1f}")
print(f"{'Pickle':<10} {time_pickle_write:<10.3f} {time_pickle_read:<10.3f} {size_pickle:<10.1f}")

# Cleanup
import os
for f in ['large_sample.csv', 'test.csv', 'test.parquet', 'test.pkl']:
    if os.path.exists(f):
        os.remove(f)
print("\n✅ Cleanup complete")





print("=== ADVANCED TECHNIQUES ===\n")

# Example 1: Numba acceleration (if available)
print("Example 1: Numba acceleration\n")

try:
    from numba import jit
    
    n = 1_000_000
    arr = np.random.randn(n)
    
    # Regular Python function
    def calculate_python(arr):
        result = np.zeros_like(arr)
        for i in range(len(arr)):
            result[i] = arr[i] ** 2 + np.sqrt(np.abs(arr[i]))
        return result
    
    # JIT-compiled version
    @jit(nopython=True)
    def calculate_numba(arr):
        result = np.zeros_like(arr)
        for i in range(len(arr)):
            result[i] = arr[i] ** 2 + np.sqrt(np.abs(arr[i]))
        return result
    
    # Warm up JIT
    _ = calculate_numba(arr[:100])
    
    # Benchmark
    start = time.time()
    result_python = calculate_python(arr)
    time_python = time.time() - start
    print(f"Python loop:  {time_python:.4f}s")
    
    start = time.time()
    result_numba = calculate_numba(arr)
    time_numba = time.time() - start
    print(f"Numba JIT:    {time_numba:.4f}s")
    
    print(f"\nNumba is {time_python/time_numba:.0f}x faster!")
    
except ImportError:
    print("⚠️ Numba not installed (pip install numba)")
    print("Skipping Numba example")

print()

# Example 2: Sparse arrays
print("="*70)
print("Example 2: Sparse arrays for sparse data\n")

n = 1_000_000
# Create mostly zero array (95% zeros)
sparse_data = np.random.choice([0, 1, 2, 3], n, p=[0.95, 0.03, 0.01, 0.01])

# Dense array
df_dense = pd.DataFrame({'values': sparse_data})
mem_dense = df_dense.memory_usage(deep=True).sum() / 1024**2
print(f"Dense array:  {mem_dense:.2f} MB")

# Sparse array
df_sparse = pd.DataFrame({
    'values': pd.arrays.SparseArray(sparse_data)
})
mem_sparse = df_sparse.memory_usage(deep=True).sum() / 1024**2
print(f"Sparse array: {mem_sparse:.2f} MB")
print(f"\nMemory savings: {(1 - mem_sparse/mem_dense)*100:.1f}%")
print()

# Example 3: String dtype
print("="*70)
print("Example 3: String dtype performance\n")

n = 100_000
strings = ['text_' + str(i % 100) for i in range(n)]

# Object dtype
df_object = pd.DataFrame({'text': strings})
mem_object = df_object.memory_usage(deep=True).sum() / 1024**2
print(f"Object dtype:  {mem_object:.2f} MB")

# String dtype
df_string = pd.DataFrame({'text': pd.array(strings, dtype='string')})
mem_string = df_string.memory_usage(deep=True).sum() / 1024**2
print(f"String dtype:  {mem_string:.2f} MB")

# Performance test
start = time.time()
_ = df_object['text'].str.upper()
time_object = time.time() - start
print(f"\nObject upper:  {time_object:.4f}s")

start = time.time()
_ = df_string['text'].str.upper()
time_string = time.time() - start
print(f"String upper:  {time_string:.4f}s")
print()

# Example 4: Window operations
print("="*70)
print("Example 4: Efficient window operations\n")

n = 1_000_000
df_window = pd.DataFrame({
    'value': np.random.randn(n)
})

# Rolling mean (efficient C implementation)
start = time.time()
df_window['rolling_mean'] = df_window['value'].rolling(window=100).mean()
time_rolling = time.time() - start
print(f"Rolling mean (window=100): {time_rolling:.4f}s")

# Expanding mean
start = time.time()
df_window['expanding_mean'] = df_window['value'].expanding().mean()
time_expanding = time.time() - start
print(f"Expanding mean:            {time_expanding:.4f}s")
print()

# Example 5: Efficient regex operations
print("="*70)
print("Example 5: Vectorized string operations\n")

n = 10_000
emails = [f"user{i}@example.com" for i in range(n)]
df_email = pd.DataFrame({'email': emails})

# Extract username (vectorized)
start = time.time()
df_email['username'] = df_email['email'].str.extract(r'(.+)@')
time_extract = time.time() - start
print(f"Extract username:  {time_extract:.4f}s")

# Check pattern (vectorized)
start = time.time()
df_email['is_valid'] = df_email['email'].str.contains(r'^[\w.]+@[\w.]+\.[a-z]+$', regex=True)
time_pattern = time.time() - start
print(f"Pattern matching:  {time_pattern:.4f}s")

# Replace (vectorized)
start = time.time()
df_email['domain'] = df_email['email'].str.replace(r'.+@', '', regex=True)
time_replace = time.time() - start
print(f"Replace pattern:   {time_replace:.4f}s")
print()

# Example 6: Copy-on-Write mode
print("="*70)
print("Example 6: Copy-on-Write optimization\n")

# Check if COW is available (Pandas 2.0+)
try:
    original_cow = pd.options.mode.copy_on_write
    print(f"Copy-on-Write mode: {original_cow}")
    print("\nCOW Benefits:")
    print("  - Avoids unnecessary data copies")
    print("  - Reduces memory usage")
    print("  - More predictable behavior")
    print("\nTo enable: pd.options.mode.copy_on_write = True")
except AttributeError:
    print("⚠️ Copy-on-Write not available (requires Pandas 2.0+)")
    print("Your version:", pd.__version__)












