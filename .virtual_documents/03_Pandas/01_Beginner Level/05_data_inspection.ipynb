


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Create a comprehensive sample dataset with various data types and issues
np.random.seed(42)

# Generate sample data
n_rows = 1000
data = {
    'customer_id': [f'CUST{str(i).zfill(4)}' for i in range(1, n_rows + 1)],
    'age': np.random.randint(18, 70, n_rows),
    'salary': np.random.normal(50000, 15000, n_rows).round(2),
    'purchase_amount': np.random.exponential(100, n_rows).round(2),
    'city': np.random.choice(['New York', 'London', 'Tokyo', 'Paris', 'Berlin', 
                              'Sydney', 'Singapore', 'Toronto'], n_rows, p=[0.3, 0.2, 0.15, 0.1, 0.08, 0.07, 0.06, 0.04]),
    'membership_tier': np.random.choice(['Bronze', 'Silver', 'Gold', 'Platinum', None], 
                                        n_rows, p=[0.4, 0.3, 0.2, 0.05, 0.05]),
    'last_purchase_date': pd.date_range('2022-01-01', periods=n_rows, freq='D').to_list(),
    'is_active': np.random.choice([True, False], n_rows, p=[0.7, 0.3]),
    'satisfaction_score': np.random.choice([1, 2, 3, 4, 5, np.nan], n_rows, p=[0.05, 0.1, 0.15, 0.3, 0.35, 0.05]),
    'product_category': np.random.choice(['Electronics', 'Clothing', 'Home', 'Books', 'Sports', 
                                          'Beauty', 'Food', np.nan], n_rows)
}

# Convert to DataFrame
df = pd.DataFrame(data)

# Introduce some missing values randomly
for col in ['age', 'salary', 'purchase_amount']:
    mask = np.random.random(n_rows) < 0.02  # 2% missing
    df.loc[mask, col] = np.nan

# Introduce some duplicates
df_duplicates = df.iloc[:5].copy()
df_duplicates.index = range(n_rows, n_rows + 5)
df = pd.concat([df, df_duplicates])

# Introduce some outliers in salary
outlier_indices = np.random.choice(df.index, 10, replace=False)
df.loc[outlier_indices, 'salary'] = df.loc[outlier_indices, 'salary'] * 5

print("Sample dataset created successfully!")
print(f"Shape: {df.shape}")
print(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")





print("=== BASIC STRUCTURAL INSPECTION ===")
print()

# Shape and size
print("1. DataFrame Dimensions:")
print(f"   Shape (rows, columns): {df.shape}")
print(f"   Total elements: {df.size}")
print(f"   Number of dimensions: {df.ndim}")
print()

# Columns and Index
print("2. Column Information:")
print(f"   Columns: {df.columns.tolist()}")
print(f"   Number of columns: {len(df.columns)}")
print(f"   Column names as list: {df.columns.tolist()[:5]}...")  # Show first 5
print()

print("3. Index Information:")
print(f"   Index type: {type(df.index)}")
print(f"   Index values (first 5): {df.index[:5].tolist()}")
print(f"   Index start: {df.index[0]}, Index end: {df.index[-1]}")
print()

# Data types
print("4. Data Types:")
print(df.dtypes)
print()

# Axes
print("5. Axes Information:")
print(f"   Row axis: {df.axes[0][:5]}...")  # Show first 5 indices
print(f"   Column axis: {df.axes[1].tolist()}")





print("=== VIEWING DATA SAMPLES ===")
print()

# First few rows
print("1. First 5 rows (df.head()):")
print(df.head())
print()

# Last few rows
print("2. Last 3 rows (df.tail(3)):")
print(df.tail(3))
print()

# Random samples
print("3. Random 5 rows (df.sample(5)):")
print(df.sample(5, random_state=42))
print()

# Specific range
print("4. Rows 10 to 15 (df.iloc[10:16]):")
print(df.iloc[10:16])
print()

# View specific columns
print("5. First 5 rows of specific columns:")
print(df[['customer_id', 'age', 'city', 'salary']].head())
print()

# Transpose for wide data
print("6. Transpose of first 3 rows (df.head(3).T):")
print(df.head(3).T)





print("=== DATA TYPE AND MEMORY INSPECTION ===")
print()

# Comprehensive info
print("1. DataFrame Info (df.info()):")
df.info()
print()

# Memory usage
print("2. Memory Usage:")
print("   Per column (bytes):")
print(df.memory_usage())
print(f"\n   Total: {df.memory_usage().sum() / 1024:.2f} KB")
print(f"   Deep memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")
print()

# Select columns by data type
print("3. Select columns by data type:")
print("   Numeric columns:")
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
print(f"   {numeric_cols}")
print("\n   Object (string) columns:")
object_cols = df.select_dtypes(include=['object']).columns.tolist()
print(f"   {object_cols}")
print("\n   Datetime columns:")
datetime_cols = df.select_dtypes(include=['datetime']).columns.tolist()
print(f"   {datetime_cols}")
print("\n   Boolean columns:")
bool_cols = df.select_dtypes(include=['bool']).columns.tolist()
print(f"   {bool_cols}")
print()

# Check specific column types
print("4. Check specific column properties:")
print(f"   'age' dtype: {df['age'].dtype}")
print(f"   'city' dtype: {df['city'].dtype}")
print(f"   'last_purchase_date' dtype: {df['last_purchase_date'].dtype}")
print()

# Memory optimization example
print("5. Memory Optimization Example:")
print(f"   Current 'age' dtype: {df['age'].dtype}, max value: {df['age'].max()}")
print("   Could use int16 (range: -32768 to 32767) for memory saving")





print("=== STATISTICAL SUMMARY ===")
print()

# Basic describe (numerical columns only)
print("1. Basic Statistical Summary (df.describe()):")
print(df.describe())
print()

# Include all columns
print("2. Complete Statistical Summary (df.describe(include='all')):")
print(df.describe(include='all').round(2))
print()

# Specific statistics for numerical columns
print("3. Specific Statistics for Numerical Columns:")
numeric_df = df.select_dtypes(include=[np.number])
print(f"{'Column':<20} {'Mean':<10} {'Median':<10} {'Std':<10} {'Min':<10} {'Max':<10}")
print("-" * 70)
for col in numeric_df.columns:
    print(f"{col:<20} {df[col].mean():<10.2f} {df[col].median():<10.2f} "
          f"{df[col].std():<10.2f} {df[col].min():<10.2f} {df[col].max():<10.2f}")
print()

# Quantiles
print("4. Quantiles (Percentiles):")
print("   Age quantiles:")
print(df['age'].quantile([0, 0.25, 0.5, 0.75, 1]))
print("\n   Salary quantiles (custom percentiles):")
print(df['salary'].quantile([0.1, 0.9, 0.95, 0.99]))
print()

# Skewness and Kurtosis
print("5. Distribution Shape:")
print(f"{'Column':<20} {'Skewness':<15} {'Kurtosis':<15}")
print("-" * 50)
for col in numeric_df.columns:
    print(f"{col:<20} {df[col].skew():<15.2f} {df[col].kurtosis():<15.2f}")
print("\n   Interpretation:")
print("   Skewness: 0=symmetric, >0=right-skewed, <0=left-skewed")
print("   Kurtosis: 3=normal, >3=heavy tails, <3=light tails")





print("=== MISSING VALUE ANALYSIS ===")
print()

# Check for missing values
print("1. Missing Values Per Column:")
missing_counts = df.isna().sum()
missing_percent = (missing_counts / len(df)) * 100
missing_df = pd.DataFrame({
    'Missing_Count': missing_counts,
    'Missing_Percent': missing_percent.round(2)
})
print(missing_df[missing_df['Missing_Count'] > 0])
print()

print("2. Total Missing Values:")
print(f"   Total missing cells: {df.isna().sum().sum()}")
print(f"   Percentage of total cells: {(df.isna().sum().sum() / df.size * 100):.2f}%")
print()

# Visualize missing values
print("3. Visualizing Missing Values:")
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
sns.heatmap(df.isna(), cbar=False, cmap='viridis', yticklabels=False)
plt.title('Missing Values Heatmap (Yellow = Missing)')
plt.show()
print()

# Rows with missing values
print("4. Rows with Missing Values:")
rows_with_missing = df[df.isna().any(axis=1)]
print(f"   Number of rows with missing values: {len(rows_with_missing)}")
print(f"   Percentage of rows: {(len(rows_with_missing) / len(df) * 100):.2f}%")
print("\n   Sample of rows with missing values:")
print(rows_with_missing.head())
print()

# Pattern of missingness
print("5. Missing Value Patterns:")
print("   Columns that are missing together:")
missing_pattern = df.isna()
# Find correlations between missingness
missing_corr = missing_pattern.corr()
print(missing_corr)





print("=== UNIQUE VALUE ANALYSIS ===")
print()

# Count unique values per column
print("1. Number of Unique Values Per Column:")
print(f"{'Column':<25} {'Unique Values':<15} {'% of Total':<10}")
print("-" * 50)
for col in df.columns:
    nunique = df[col].nunique()
    pct_unique = (nunique / len(df)) * 100
    print(f"{col:<25} {nunique:<15} {pct_unique:<10.2f}")
print()

# View unique values
print("2. Unique Values for Categorical Columns:")
categorical_cols = ['city', 'membership_tier', 'product_category', 'is_active']
for col in categorical_cols:
    print(f"\n   {col}:")
    unique_vals = df[col].unique()
    print(f"   {len(unique_vals)} unique values: {unique_vals[:10]}{'...' if len(unique_vals) > 10 else ''}")
print()

# Value counts with percentages
print("3. Value Counts with Percentages:")
for col in ['city', 'membership_tier']:
    print(f"\n   {col} distribution:")
    value_counts = df[col].value_counts(dropna=False)
    value_pct = df[col].value_counts(dropna=False, normalize=True) * 100
    
    # Create a summary DataFrame
    summary = pd.DataFrame({
        'Count': value_counts,
        'Percent': value_pct.round(2)
    })
    print(summary.head(10))  # Show top 10
    if len(summary) > 10:
        print(f"   ... and {len(summary) - 10} more categories")
print()

# Cardinality check (high vs low cardinality)
print("4. Cardinality Analysis:")
print("   High cardinality (many unique values):")
for col in df.columns:
    nunique = df[col].nunique()
    if nunique > 50 and df[col].dtype == 'object':
        print(f"   {col}: {nunique} unique values")
print("\n   Low cardinality (few unique values):")
for col in df.columns:
    nunique = df[col].nunique()
    if nunique <= 5:
        print(f"   {col}: {nunique} unique values ({df[col].unique()})")





print("=== DUPLICATE DETECTION ===")
print()

# Check for duplicate rows
print("1. Duplicate Rows (entire row match):")
n_duplicates = df.duplicated().sum()
print(f"   Number of duplicate rows: {n_duplicates}")
print(f"   Percentage: {(n_duplicates / len(df) * 100):.2f}%")
print()

if n_duplicates > 0:
    print("   Sample duplicate rows:")
    duplicate_rows = df[df.duplicated(keep=False)]  # Show all duplicates
    print(duplicate_rows.head())
print()

# Check for duplicates in specific columns
print("2. Duplicates in Key Columns:")
key_columns = ['customer_id']  # Should be unique
for col in key_columns:
    n_dups = df[col].duplicated().sum()
    print(f"   {col}: {n_dups} duplicates")
    if n_dups > 0:
        dup_values = df[col][df[col].duplicated(keep=False)].unique()
        print(f"     Duplicate values: {dup_values[:5]}{'...' if len(dup_values) > 5 else ''}")
print()

# Find duplicate pairs
print("3. Find Duplicate Pairs:")
# Create a temporary duplicate flag
df['is_duplicate'] = df.duplicated(subset=['customer_id'], keep=False)
if df['is_duplicate'].any():
    duplicates = df[df['is_duplicate']].sort_values('customer_id')
    print(duplicates[['customer_id', 'age', 'city', 'salary']].head(10))
else:
    print("   No duplicates found in customer_id")
print()

# Remove duplicates (for demonstration)
print("4. Removing Duplicates:")
df_no_duplicates = df.drop_duplicates()
print(f"   Original shape: {df.shape}")
print(f"   After dropping duplicates: {df_no_duplicates.shape}")
print(f"   Rows removed: {df.shape[0] - df_no_duplicates.shape[0]}")
print()

# Keep first/last occurrence
print("5. Duplicate Removal Options:")
print("   keep='first' (default): Keep first occurrence, drop rest")
print("   keep='last': Keep last occurrence, drop rest")
print("   keep=False: Drop all duplicates")





print("=== DATA DISTRIBUTION VISUALIZATION ===")
print()

# Set up the plotting style
plt.style.use('seaborn-v0_8')
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
fig.suptitle('Data Distribution Visualizations', fontsize=16)

# 1. Histogram of age
df['age'].hist(ax=axes[0, 0], bins=30, edgecolor='black', alpha=0.7)
axes[0, 0].set_title('Age Distribution')
axes[0, 0].set_xlabel('Age')
axes[0, 0].set_ylabel('Frequency')
axes[0, 0].axvline(df['age'].mean(), color='red', linestyle='dashed', linewidth=1, label=f'Mean: {df["age"].mean():.1f}')
axes[0, 0].legend()

# 2. Box plot of salary
df['salary'].plot.box(ax=axes[0, 1])
axes[0, 1].set_title('Salary Distribution (Box Plot)')
axes[0, 1].set_ylabel('Salary')

# 3. Bar plot of city counts
df['city'].value_counts().plot.bar(ax=axes[0, 2], color='skyblue', edgecolor='black')
axes[0, 2].set_title('City Distribution')
axes[0, 2].set_xlabel('City')
axes[0, 2].set_ylabel('Count')
axes[0, 2].tick_params(axis='x', rotation=45)

# 4. KDE plot of purchase amount
df['purchase_amount'].plot.kde(ax=axes[1, 0])
axes[1, 0].set_title('Purchase Amount Density')
axes[1, 0].set_xlabel('Purchase Amount')
axes[1, 0].set_ylabel('Density')

# 5. Scatter plot: Age vs Salary
axes[1, 1].scatter(df['age'], df['salary'], alpha=0.5, s=20)
axes[1, 1].set_title('Age vs Salary')
axes[1, 1].set_xlabel('Age')
axes[1, 1].set_ylabel('Salary')

# 6. Pie chart of membership tier (without missing)
membership_counts = df['membership_tier'].value_counts(dropna=True)
axes[1, 2].pie(membership_counts.values, labels=membership_counts.index, autopct='%1.1f%%')
axes[1, 2].set_title('Membership Tier Distribution')

plt.tight_layout()
plt.show()
print()

# Correlation heatmap for numerical columns
print("Correlation Matrix (Numerical Columns):")
numeric_cols = df.select_dtypes(include=[np.number]).columns
corr_matrix = df[numeric_cols].corr()

plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, 
            square=True, linewidths=1, cbar_kws={"shrink": 0.8})
plt.title('Correlation Heatmap')
plt.show()








print("=== OUTLIER DETECTION ===")
print()

# Function to detect outliers using IQR method
def detect_outliers_iqr(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = series[(series < lower_bound) | (series > upper_bound)]
    return outliers, lower_bound, upper_bound

# Function to detect outliers using Z-score method (FIXED)
def detect_outliers_zscore(series, threshold=3):
    from scipy import stats
    clean_series = series.dropna()  # Work with clean data
    z_scores = np.abs(stats.zscore(clean_series))
    outliers = clean_series[z_scores > threshold]  # Use clean_series, not original
    return outliers

# Analyze numerical columns for outliers
print("1. Outlier Detection using IQR Method:")
print(f"{'Column':<20} {'Total':<10} {'Outliers':<10} {'% Outliers':<12} {'Min':<10} {'Max':<10}")
print("-" * 72)

numeric_cols = df.select_dtypes(include=[np.number]).columns
outlier_summary = []

for col in numeric_cols:
    if col in df.columns:
        outliers, lower, upper = detect_outliers_iqr(df[col].dropna())
        n_outliers = len(outliers)
        n_total = len(df[col].dropna())
        pct_outliers = (n_outliers / n_total * 100) if n_total > 0 else 0
        
        outlier_summary.append({
            'Column': col,
            'Total': n_total,
            'Outliers': n_outliers,
            '% Outliers': pct_outliers,
            'Lower Bound': lower,
            'Upper Bound': upper,
            'Actual Min': df[col].min(),
            'Actual Max': df[col].max()
        })
        
        print(f"{col:<20} {n_total:<10} {n_outliers:<10} {pct_outliers:<11.2f}% "
              f"{df[col].min():<10.2f} {df[col].max():<10.2f}")

outlier_df = pd.DataFrame(outlier_summary)
print()

# Show extreme outliers
print("2. Extreme Outliers (Z-score > 3):")
for col in numeric_cols:
    if col in df.columns:
        outliers = detect_outliers_zscore(df[col])
        if len(outliers) > 0:
            print(f"\n   {col}: {len(outliers)} outliers")
            print(f"   Example outliers: {outliers.iloc[:3].tolist()}{'...' if len(outliers) > 3 else ''}")
print()

# Visualize outliers (FIXED - check if columns exist)
print("3. Visualizing Outliers:")
plot_cols = [col for col in ['age', 'salary', 'purchase_amount'] if col in df.columns][:3]

if len(plot_cols) > 0:
    fig, axes = plt.subplots(1, len(plot_cols), figsize=(5*len(plot_cols), 5))
    if len(plot_cols) == 1:
        axes = [axes]  # Make it iterable
    
    for idx, col in enumerate(plot_cols):
        # Box plot
        bp = axes[idx].boxplot(df[col].dropna(), patch_artist=True)
        bp['boxes'][0].set_facecolor('lightblue')
        axes[idx].set_title(f'{col} - Box Plot')
        axes[idx].set_ylabel(col)
        
        # Add text with outlier count
        outliers, _, _ = detect_outliers_iqr(df[col].dropna())
        axes[idx].text(0.95, 0.95, f'Outliers: {len(outliers)}', 
                       transform=axes[idx].transAxes, ha='right', va='top',
                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    plt.show()
else:
    print("   No numeric columns found for plotting")
print()

# Handle outliers (demonstration) - FIXED
print("4. Handling Outliers (Options):")
print("   a) Remove outliers:")
# Get all outlier indices across numeric columns
all_outlier_indices = set()
for col in numeric_cols:
    if col in df.columns:
        outliers, _, _ = detect_outliers_iqr(df[col].dropna())
        all_outlier_indices.update(outliers.index)

df_no_outliers = df.drop(index=all_outlier_indices)
print(f"      Original: {df.shape}, Without outliers: {df_no_outliers.shape}")
print()

print("   b) Cap outliers (winsorize):")
df_capped = df.copy()
if 'salary' in df.columns:
    _, lower, upper = detect_outliers_iqr(df['salary'].dropna())
    df_capped['salary'] = df_capped['salary'].clip(lower=lower, upper=upper)
    print(f"      Salary capped between: [{lower:.2f}, {upper:.2f}]")
print()

print("   c) Transform outliers (log transform):")
df_transformed = df.copy()
if 'salary' in df.columns:
    df_transformed['log_salary'] = np.log1p(df_transformed['salary'])
    print("      Created log_salary column")






print("=== ADVANCED INSPECTION TECHNIQUES ===")
print()

# Cross-tabulation
print("1. Cross-tabulation:")
print("   City vs Membership Tier:")
cross_tab = pd.crosstab(df['city'], df['membership_tier'], margins=True, margins_name="Total")
print(cross_tab)
print()

# With percentages
print("   City vs Membership Tier (Row %):")
cross_tab_pct = pd.crosstab(df['city'], df['membership_tier'], 
                           normalize='index') * 100
print(cross_tab_pct.round(2))
print()

# Group-wise statistics
print("2. Group-wise Analysis:")
print("   Average salary by city:")
group_stats = df.groupby('city')['salary'].agg(['mean', 'median', 'std', 'count'])
print(group_stats.round(2))
print()

print("   Age and salary statistics by membership tier:")
group_stats2 = df.groupby('membership_tier').agg({
    'age': ['mean', 'median', 'min', 'max'],
    'salary': ['mean', 'median', 'std'],
    'customer_id': 'count'
}).round(2)
print(group_stats2)
print()

# Time-based analysis
print("3. Time-based Analysis:")
if 'last_purchase_date' in df.columns:
    df['purchase_month'] = df['last_purchase_date'].dt.to_period('M')
    monthly_stats = df.groupby('purchase_month').agg({
        'purchase_amount': 'sum',
        'customer_id': 'count',
        'salary': 'mean'
    }).round(2)
    monthly_stats.columns = ['Total_Purchase', 'Transaction_Count', 'Avg_Salary']
    print(monthly_stats.head())
    print()
    
    # Time trend visualization
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    monthly_stats['Total_Purchase'].plot(kind='line', marker='o')
    plt.title('Monthly Purchase Trend')
    plt.xlabel('Month')
    plt.ylabel('Total Purchase Amount')
    plt.xticks(rotation=45)
    
    plt.subplot(1, 2, 2)
    monthly_stats['Transaction_Count'].plot(kind='bar', color='skyblue')
    plt.title('Monthly Transaction Count')
    plt.xlabel('Month')
    plt.ylabel('Number of Transactions')
    plt.xticks(rotation=45)
    
    plt.tight_layout()
    plt.show()
print()

# Pattern detection
print("4. Pattern Detection:")
print("   Check for purchase patterns by day of week:")
if 'last_purchase_date' in df.columns:
    df['purchase_day'] = df['last_purchase_date'].dt.day_name()
    day_pattern = df.groupby('purchase_day')['purchase_amount'].agg(['mean', 'count', 'sum'])
    day_pattern = day_pattern.reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])
    print(day_pattern.round(2))






print("=== AUTOMATED DATA PROFILING ===")
print()

# Check if pandas-profiling is available
try:
    from ydata_profiling import ProfileReport
    HAS_PROFILING = True
except ImportError:
    try:
        from pandas_profiling import ProfileReport
        HAS_PROFILING = True
    except ImportError:
        HAS_PROFILING = False
        print("pandas-profiling/ ydata-profiling not installed.")
        print("Install with: pip install ydata-profiling")
        print("Or for older version: pip install pandas-profiling")

if HAS_PROFILING:
    print("Generating profile report... (This may take a moment)")
    
    # Create a smaller subset for faster demonstration
    df_sample = df.sample(500, random_state=42)
    
    # Generate the report
    profile = ProfileReport(df_sample, title="Data Inspection Report", explorative=True)
    
    # Display in notebook
    profile.to_notebook_iframe()
    
    # To save as HTML file:
    # profile.to_file("data_inspection_report.html")
    
    print("\nReport generated successfully!")
    print("Features included in the report:")
    print("1. Overview with warnings")
    print("2. Variables (detailed analysis per column)")
    print("3. Interactions (scatter plots)")
    print("4. Correlations (multiple methods)")
    print("5. Missing values analysis")
    print("6. Sample rows")
else:
    print("\nManual alternative: Create summary statistics")
    print(df.describe(include='all').T)





print("=== DATA QUALITY ASSESSMENT ===")
print()

# Create a data quality assessment function
def assess_data_quality(df):
    """Comprehensive data quality assessment"""
    
    quality_report = {
        'dimension': [],
        'metric': [],
        'value': [],
        'score': [],
        'issues': []
    }
    
    # 1. Completeness
    total_cells = df.size
    missing_cells = df.isna().sum().sum()
    completeness = ((total_cells - missing_cells) / total_cells) * 100
    
    quality_report['dimension'].append('Completeness')
    quality_report['metric'].append('Missing Values')
    quality_report['value'].append(f"{missing_cells}/{total_cells}")
    quality_report['score'].append(completeness)
    quality_report['issues'].append(f"{missing_cells} missing values ({missing_cells/df.shape[0]:.2f} per row)")
    
    # 2. Uniqueness
    duplicate_rows = df.duplicated().sum()
    uniqueness = ((len(df) - duplicate_rows) / len(df)) * 100
    
    quality_report['dimension'].append('Uniqueness')
    quality_report['metric'].append('Duplicate Rows')
    quality_report['value'].append(f"{duplicate_rows}/{len(df)}")
    quality_report['score'].append(uniqueness)
    quality_report['issues'].append(f"{duplicate_rows} duplicate rows found")
    
    # 3. Validity (example: age should be between 18 and 100)
    invalid_age = df[(df['age'] < 18) | (df['age'] > 100)].shape[0]
    validity = ((len(df) - invalid_age) / len(df)) * 100 if 'age' in df.columns else 100
    
    quality_report['dimension'].append('Validity')
    quality_report['metric'].append('Invalid Age Values')
    quality_report['value'].append(f"{invalid_age}/{len(df)}")
    quality_report['score'].append(validity)
    quality_report['issues'].append(f"{invalid_age} age values outside 18-100 range")
    
    # 4. Consistency (example: salary should be positive)
    invalid_salary = df[df['salary'] < 0].shape[0] if 'salary' in df.columns else 0
    consistency = ((len(df) - invalid_salary) / len(df)) * 100 if 'salary' in df.columns else 100
    
    quality_report['dimension'].append('Consistency')
    quality_report['metric'].append('Negative Salary')
    quality_report['value'].append(f"{invalid_salary}/{len(df)}")
    quality_report['score'].append(consistency)
    quality_report['issues'].append(f"{invalid_salary} negative salary values")
    
    # Create DataFrame
    quality_df = pd.DataFrame(quality_report)
    
    return quality_df

# Run assessment
print("Data Quality Assessment Report:")
print("-" * 80)
quality_df = assess_data_quality(df)
print(quality_df.to_string(index=False))
print()

# Calculate overall score
overall_score = quality_df['score'].mean()
print(f"Overall Data Quality Score: {overall_score:.1f}/100")
print()

# Color code based on score
def score_color(score):
    if score >= 90:
        return "ðŸŸ¢ Excellent"
    elif score >= 80:
        return "ðŸŸ¡ Good"
    elif score >= 70:
        return "ðŸŸ  Acceptable"
    else:
        return "ðŸ”´ Needs Improvement"

print("Score Interpretation:")
for idx, row in quality_df.iterrows():
    print(f"  {row['dimension']}: {score_color(row['score'])} ({row['score']:.1f})")












