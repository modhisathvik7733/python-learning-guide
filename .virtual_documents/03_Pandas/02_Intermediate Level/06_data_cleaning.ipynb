


import pandas as pd
import numpy as np
import re
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Create a realistic messy dataset for cleaning exercises
np.random.seed(42)

# Customer data with various issues
data = {
    'customer_id': ['C001', 'C002', 'C003', 'C004', 'C005', 'C006', 'C007', 'C008', 'C009', 'C010'] +
                   ['C001', 'C011', None, 'C013', 'C014', 'C015', 'C016', 'C017', 'C018', 'C019'],
    
    'name': ['john doe', 'Jane Smith', 'ALICE JOHNSON', 'bob', 'mary-anne', 
             'David Lee', 'Sarah Connor', 'Mike O\'Brien', 'Lisa', 'Tom Jones'] +
            ['John Doe', 'Emma Watson', 'Chris Brown', 'Anna Karenina', 'Leo Tolstoy',
             'F. Scott Fitzgerald', 'Ernest Hemingway', 'Virginia Woolf', 'James Joyce', 'Mark Twain'],
    
    'email': ['john@email.com', 'jane@gmail.com', 'alice@company.org', 'bob@yahoo.com', 
              'mary@hotmail.com', 'david@work.com', 'sarah@test.com', 'mike@domain.com',
              'lisa@example.com', 'tom@server.com'] +
             ['john@email.com', 'emma@email.com', 'chris@email.com', None, 'leo@email.com',
              'f.scott@email.com', 'ernest@email.com', 'virginia@email.com', 'james@email.com', 'mark@email.com'],
    
    'phone': ['123-456-7890', '9876543210', '(555) 123-4567', '444 555 6666', 
              '777-888-9999', '1234567890', '0987654321', '555-1234', 
              'invalid', '9998887777'] +
             ['123-456-7890', '111-222-3333', '444-555-6666', '777-888-9999', 
              '123-456-7890', '987-654-3210', '555-555-5555', '666-666-6666', 
              '777-777-7777', '888-888-8888'],
    
    'age': [25, 30, '35', 40, 45, 28, 32, 29, 150, 33] +
           [25, 22, 27, 31, 45, 38, 41, 36, 29, 44],
    
    'salary': [50000, 60000, 75000, 80000, 55000, 62000, 71000, 48000, 90000, 58000] +
              [50000, 43000, 67000, 82000, 92000, 38000, 77000, 65000, 59000, 72000],
    
    'join_date': ['2022-01-15', '2021-03-22', '2020-11-30', '2023-02-14', '2019-08-05',
                  '2022-07-19', '2021-12-01', '2020-05-17', '2023-01-01', '2022-09-10'] +
                 ['2022-01-15', '2023-03-10', '2021-07-22', '2018-04-18', '2017-06-30',
                  '2019-11-11', '2020-08-09', '2022-12-25', '2021-02-28', '2023-05-15'],
    
    'status': ['active', 'inactive', 'ACTIVE', 'active', 'pending', 
               'active', 'suspended', 'active', 'INACTIVE', 'active'] +
              ['active', 'active', 'inactive', 'active', 'pending', 
               'suspended', 'active', 'inactive', 'active', 'active'],
    
    'purchase_amount': [150.50, 200.00, 75.25, '300.00', 450.75, 
                        125.00, 275.50, 350.25, 500.00, 175.80] +
                       [150.50, 225.00, 180.75, 320.50, 410.25,
                        135.00, 290.75, 195.50, 275.00, 330.25]
}

df = pd.DataFrame(data)
print("üì¶ MESSY DATASET CREATED")
print(f"Shape: {df.shape}")
print("\nüîç First look at the data:")
print(df.head(12))
print("\n‚ö†Ô∏è Issues to fix:")
print("1. Duplicate rows (customer_id C001)")
print("2. Missing values (customer_id, email)")
print("3. Inconsistent text formatting (name, status)")
print("4. Invalid phone numbers")
print("5. Wrong data types (age as string)")
print("6. Outliers (age 150)")
print("7. Inconsistent date formats")








# Fill with constant
df.fillna(0)                                    # Fill all with 0
df.fillna({'col1': 0, 'col2': 'Unknown'})      # Different values per column

# Fill with statistical measures - ADD numeric_only=True
df.fillna(df.mean(numeric_only=True))          # Fill with column mean ‚úÖ
df.fillna(df.median(numeric_only=True))        # Fill with column median ‚úÖ
df.fillna(df.mode().iloc[0])                   # Fill with most frequent value

# Forward/backward fill (time series)
df.fillna(method='ffill')                      # Forward fill (carry last value forward)
df.fillna(method='bfill')                      # Backward fill (use next value)

# Fill with interpolation
df.interpolate()                               # Linear interpolation
















## Cell 4: Missing Values - Practical Application

print("=== HANDLING MISSING VALUES ===")
print("Original DataFrame:")
print(df.info())
print()

# 1. Identify missing values
print("üîç 1. Identify Missing Values:")
missing_summary = pd.DataFrame({
    'Missing_Count': df.isna().sum(),
    'Missing_Percent': (df.isna().sum() / len(df) * 100).round(2)
})
print(missing_summary[missing_summary['Missing_Count'] > 0])
print()

# 2. Visualize missing values
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 4))
sns.heatmap(df.isna(), cbar=False, cmap='Reds', yticklabels=False)
plt.title('Missing Values Heatmap (Red = Missing)')
plt.show()
print()

# 3. Different strategies for different columns
print("üìù 3. Apply Different Strategies Per Column:")

# Create a copy for cleaning
df_clean = df.copy()

# Strategy A: Remove rows with missing customer_id (critical field)
print("   Strategy A: Remove rows where customer_id is missing (critical field)")
before = len(df_clean)
df_clean = df_clean.dropna(subset=['customer_id'])
print(f"   Removed {before - len(df_clean)} rows")
print()

# Strategy B: Fill email with placeholder
print("   Strategy B: Fill missing emails with 'no-email@placeholder.com'")
df_clean['email'] = df_clean['email'].fillna('no-email@placeholder.com')
print(f"   Filled {df['email'].isna().sum()} missing emails")
print()

# Strategy C: Keep as-is for other columns (if <2% missing)
print("   Strategy C: Analyze other columns for missing patterns:")
for col in df_clean.columns:
    missing_pct = (df_clean[col].isna().sum() / len(df_clean)) * 100
    if missing_pct > 0:
        print(f"   - {col}: {missing_pct:.1f}% missing")
print()

# 4. Verify missing values after cleaning
print("‚úÖ 4. Missing Values After Cleaning:")
print(df_clean.isna().sum())
print(f"\nTotal missing values remaining: {df_clean.isna().sum().sum()}")





# Remove exact duplicates
df.drop_duplicates()                # Keep first occurrence
df.drop_duplicates(keep='last')     # Keep last occurrence
df.drop_duplicates(keep=False)      # Remove all duplicates

# Remove based on key columns
df.drop_duplicates(subset=['email', 'phone'])  # Business logic












## Cell 6: Duplicates - Practical Application

print("=== REMOVING DUPLICATES ===")

# 1. Identify duplicates
print("üîç 1. Identify All Types of Duplicates:")
print(f"Total rows: {len(df_clean)}")

# Exact duplicates (all columns match)
exact_dups = df_clean.duplicated().sum()
print(f"Exact duplicates (all columns): {exact_dups}")

# Business logic duplicates (customer_id should be unique)
id_dups = df_clean.duplicated(subset=['customer_id']).sum()
print(f"Duplicate customer IDs: {id_dups}")

# Email duplicates
email_dups = df_clean.duplicated(subset=['email']).sum()
print(f"Duplicate emails: {email_dups}")

# Phone duplicates
phone_dups = df_clean.duplicated(subset=['phone']).sum()
print(f"Duplicate phones: {phone_dups}")
print()

# 2. View duplicate records
print("üìã 2. View Duplicate Records:")
if id_dups > 0:
    duplicate_ids = df_clean[df_clean.duplicated(subset=['customer_id'], keep=False)]
    print("Duplicate customer_id records:")
    print(duplicate_ids.sort_values('customer_id')[['customer_id', 'name', 'email', 'join_date']])
print()

# 3. Decision making for duplicates
print("ü§î 3. Decision Making Strategy:")

# Create rules for which duplicate to keep
def choose_best_duplicate(group):
    """
    Business logic for choosing which duplicate to keep
    Priority: 1. Has email, 2. Most recent join_date, 3. Complete name
    """
    # Score each row
    scores = []
    for idx, row in group.iterrows():
        score = 0
        if '@' in str(row['email']) and 'placeholder' not in str(row['email']):
            score += 3
        if pd.notna(row['name']) and len(str(row['name']).split()) >= 2:
            score += 2
        if pd.notna(row['phone']) and len(str(row['phone'])) >= 10:
            score += 1
        scores.append(score)
    
    # Return row with highest score
    return group.iloc[scores.index(max(scores))]

# 4. Apply duplicate removal
print("üîÑ 4. Removing Duplicates with Business Logic:")

# First, keep only one record per customer_id
# Use our custom function for groups with same customer_id
df_clean = df_clean.sort_values('join_date', ascending=False)  # Most recent first

# For simplicity, we'll keep first occurrence after sorting
df_clean = df_clean.drop_duplicates(subset=['customer_id'], keep='first')

print(f"After removing customer_id duplicates: {len(df_clean)} rows")
print()

# 5. Check for other potential duplicates
print("üîé 5. Check for Fuzzy Duplicates (Similar Names):")
# Find names that might be the same person with typos
names = df_clean['name'].str.lower().str.strip().unique()
print(f"Unique names (case-insensitive): {len(names)} out of {len(df_clean)} rows")

# Example: Check for 'john doe' vs 'John Doe'
name_counts = df_clean['name'].str.lower().str.strip().value_counts()
potential_dups = name_counts[name_counts > 1]
if len(potential_dups) > 0:
    print(f"\nPotential fuzzy duplicates (same name, different case):")
    for name, count in potential_dups.items():
        print(f"  '{name}': {count} occurrences")











# Before optimization
df['status'].dtype  # object (string) - uses ~100KB

# After optimization
df['status'] = df['status'].astype('category')
df['status'].dtype  # category - uses ~10KB (90% savings!)





# Common Patterns for Data Cleaning

# IMPORTANT: Check your DataFrame first!
print("Available columns:", df.columns.tolist())
print("\nDataFrame preview:")
print(df.head())
print()

# ===== PATTERN 1: Remove currency symbols and commas =====
if 'price' in df.columns:
    df['price'] = df['price'].str.replace('$', '', regex=False).str.replace(',', '', regex=False).astype(float)
    print("‚úì Cleaned 'price' column")
else:
    print("‚ö† Column 'price' not found")

# ===== PATTERN 2: Convert percentage strings =====
if 'discount' in df.columns:
    df['discount'] = df['discount'].str.replace('%', '', regex=False).astype(float) / 100
    print("‚úì Cleaned 'discount' column")
else:
    print("‚ö† Column 'discount' not found")

# ===== PATTERN 3: Handle mixed formats =====
def convert_mixed_numbers(val):
    """Convert string numbers with commas or plain numbers to float"""
    if isinstance(val, str):
        return float(val.replace(',', ''))
    return float(val)

if 'value' in df.columns:
    df['value'] = df['value'].apply(convert_mixed_numbers)
    print("‚úì Cleaned 'value' column")
else:
    print("‚ö† Column 'value' not found")

print("\nCleaning complete!")


















# Clean person names
df['name'] = (df['name'].str.strip()          # Remove spaces
                      .str.title()            # Title case
                      .str.replace(r'\s+', ' ', regex=True)  # Multiple spaces to one
                      .str.replace(r'[^\w\s-]', '', regex=True))  # Remove special chars





# Standardize phone numbers
def clean_phone(phone):
    if pd.isna(phone):
        return phone
    # Remove all non-digits
    digits = re.sub(r'\D', '', str(phone))
    # Format as XXX-XXX-XXXX if 10 digits
    if len(digits) == 10:
        return f"{digits[:3]}-{digits[3:6]}-{digits[6:]}"
    return phone

df['phone'] = df['phone'].apply(clean_phone)





# Basic email validation
def is_valid_email(email):
    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
    return bool(re.match(pattern, str(email)))

df['email_valid'] = df['email'].apply(is_valid_email)





# 4. Address Standardization

# OPTION 1: Check if column exists first
if 'address' in df.columns:
    # Standardize address abbreviations
    abbreviations = {
        'St.': 'Street',
        'St': 'Street',
        'Ave.': 'Avenue',
        'Ave': 'Avenue',
        'Rd.': 'Road',
        'Rd': 'Road'
    }
    
    for abbr, full in abbreviations.items():
        df['address'] = df['address'].str.replace(abbr, full, regex=False)
    
    print("‚úì Address standardization complete")
else:
    print("‚ö† Column 'address' not found in DataFrame")
    print(f"Available columns: {df.columns.tolist()}")










## Cell 10: String Cleaning - Practical Application

print("=== STRING CLEANING AND STANDARDIZATION ===")

# 1. Clean names column
print("üë§ 1. Cleaning 'name' Column:")
print("   Before cleaning - Sample values:")
print(df_clean['name'].head(10).tolist())

# Create a cleaned version
df_clean['name_clean'] = (
    df_clean['name']
    .str.strip()                    # Remove whitespace
    .str.title()                    # Convert to title case
    .str.replace(r'\s+', ' ', regex=True)  # Multiple spaces to single
    .str.replace(r'[^\w\s\'-]', '', regex=True)  # Remove special chars except hyphen and apostrophe
)

print("\n   After cleaning - Sample values:")
print(df_clean['name_clean'].head(10).tolist())

# Check for empty names after cleaning
empty_names = df_clean[df_clean['name_clean'].str.strip() == ''].shape[0]
print(f"   Empty names after cleaning: {empty_names}")
print()

# 2. Standardize phone numbers
print("üì± 2. Standardizing 'phone' Column:")

def standardize_phone(phone):
    """Standardize phone number to format: XXX-XXX-XXXX"""
    if pd.isna(phone):
        return None
    
    # Convert to string and remove all non-digit characters
    phone_str = str(phone)
    digits = re.sub(r'\D', '', phone_str)
    
    # Check if we have valid digits
    if len(digits) == 10:
        return f"{digits[:3]}-{digits[3:6]}-{digits[6:]}"
    elif len(digits) == 7:  # Local number without area code
        return f"XXX-{digits[:3]}-{digits[3:]}"
    else:
        return None  # Invalid phone number

df_clean['phone_clean'] = df_clean['phone'].apply(standardize_phone)

print("   Before cleaning - Sample values:")
print(df_clean['phone'].head(10).tolist())
print("\n   After cleaning - Sample values:")
print(df_clean['phone_clean'].head(10).tolist())

# Count valid phone numbers
valid_phones = df_clean['phone_clean'].notna().sum()
print(f"   Valid phone numbers: {valid_phones}/{len(df_clean)}")
print()

# 3. Email validation and cleaning
print("üìß 3. Validating and Cleaning 'email' Column:")

def validate_and_clean_email(email):
    """Validate email format and clean if needed"""
    if pd.isna(email) or email == 'no-email@placeholder.com':
        return email
    
    email = str(email).strip().lower()
    
    # Basic email validation regex
    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
    
    if re.match(pattern, email):
        return email
    else:
        # Try to fix common issues
        email = email.replace(' ', '')  # Remove spaces
        email = email.replace('@@', '@')  # Fix double @
        
        if re.match(pattern, email):
            return email
        else:
            return 'invalid-email@placeholder.com'

df_clean['email_clean'] = df_clean['email'].apply(validate_and_clean_email)

print("   Before cleaning - Sample values:")
print(df_clean['email'].head(10).tolist())
print("\n   After cleaning - Sample values:")
print(df_clean['email_clean'].head(10).tolist())

invalid_emails = (df_clean['email_clean'] == 'invalid-email@placeholder.com').sum()
print(f"   Invalid emails marked: {invalid_emails}")
print()

# 4. Status standardization
print("üè∑Ô∏è 4. Standardizing 'status' Column:")
print(f"   Current unique values: {df_clean['status'].unique()}")

# Map to standardized statuses
status_mapping = {
    'active': 'active',
    'inactive': 'inactive',
    'pending': 'pending',
    'suspended': 'suspended',
    'act': 'active',  # Common abbreviation
    'inact': 'inactive'
}

df_clean['status_standardized'] = (
    df_clean['status']
    .str.lower()
    .str.strip()
    .str[:7]  # Take first 7 chars to catch 'inactive' vs 'inact'
    .map(status_mapping)
)

# Fill any unmapped values with 'unknown'
df_clean['status_standardized'] = df_clean['status_standardized'].fillna('unknown')

print(f"   Standardized unique values: {df_clean['status_standardized'].unique()}")
print()

# 5. Summary of string cleaning
print("‚úÖ 5. String Cleaning Summary:")
print(f"   Original columns with issues: name, phone, email, status")
print(f"   Created cleaned columns: name_clean, phone_clean, email_clean, status_standardized")
print(f"   Invalid data flagged: {invalid_emails} invalid emails")





import pandas as pd
import matplotlib.pyplot as plt

# First, check what columns you have
print("Available columns:", df.columns.tolist())
print("\nDataFrame preview:")
print(df.head())
print()

# ===== Box plots =====
# Check if columns exist before plotting
box_cols = ['col1', 'col2']
existing_box_cols = [col for col in box_cols if col in df.columns]

if existing_box_cols:
    df.boxplot(column=existing_box_cols)
    plt.title('Box Plot')
    plt.show()
else:
    print(f"‚ö† Columns {box_cols} not found for box plot")

# ===== Scatter plots =====
if 'x' in df.columns and 'y' in df.columns:
    plt.scatter(df['x'], df['y'])
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title('Scatter Plot')
    plt.show()
else:
    print("‚ö† Columns 'x' and 'y' not found for scatter plot")

# ===== Histograms =====
if 'col' in df.columns:
    df['col'].hist(bins=50)
    plt.title('Histogram')
    plt.xlabel('col')
    plt.ylabel('Frequency')
    plt.show()
else:
    print("‚ö† Column 'col' not found for histogram")


















# Option B: Remove Outliers

# First, check what columns exist
print("Available columns:", df.columns.tolist())
print("\nNumeric columns:", df.select_dtypes(include=[np.number]).columns.tolist())
print()

# Replace 'col1' with your actual column name
# For this example, let's use the first numeric column
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

if len(numeric_cols) > 0:
    col_name = numeric_cols[0]  # Use first numeric column
    print(f"Removing outliers from: {col_name}")
    print()
    
    # METHOD 1: Remove using IQR bounds
    Q1 = df[col_name].quantile(0.25)
    Q3 = df[col_name].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    df_no_outliers = df[(df[col_name] >= lower_bound) & (df[col_name] <= upper_bound)]
    print(f"IQR Method: {len(df)} ‚Üí {len(df_no_outliers)} rows")
    print()
    
    # METHOD 2: Remove using percentile
    lower_percentile = df[col_name].quantile(0.01)
    upper_percentile = df[col_name].quantile(0.99)
    
    df_no_outliers = df[(df[col_name] >= lower_percentile) & (df[col_name] <= upper_percentile)]
    print(f"Percentile Method: {len(df)} ‚Üí {len(df_no_outliers)} rows")
else:
    print("‚ö† No numeric columns found!")






import pandas as pd
import numpy as np

# Sample data
np.random.seed(42)
df = pd.DataFrame({
    'age': np.concatenate([np.random.normal(30, 5, 95), [100, 120, 150, 200, -10]])
})

print(f"Original data: {len(df)} rows")
print(f"Range: [{df['age'].min():.2f}, {df['age'].max():.2f}]")
print()

# OPTION A: Remove outliers (loses data)
Q1 = df['age'].quantile(0.25)
Q3 = df['age'].quantile(0.75)
IQR = Q3 - Q1
lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR

df_removed = df[(df['age'] >= lower) & (df['age'] <= upper)]
print(f"After REMOVAL: {len(df_removed)} rows (lost {len(df) - len(df_removed)} rows)")
print(f"Range: [{df_removed['age'].min():.2f}, {df_removed['age'].max():.2f}]")
print()

# OPTION B: Cap outliers (keeps all data)
df['age_capped'] = df['age'].clip(lower=lower, upper=upper)
print(f"After CAPPING: {len(df)} rows (no data loss)")
print(f"Range: [{df['age_capped'].min():.2f}, {df['age_capped'].max():.2f}]")
print()

print("Comparison:")
print(df[['age', 'age_capped']].describe())






import pandas as pd
import numpy as np
from scipy import stats

# Check your columns first
print("Columns:", df.columns.tolist())

# Replace 'col' with your actual column name
col_name = 'age'  # ‚Üê Change this to your actual column name

# Check if column exists
if col_name in df.columns:
    
    # Log transformation (for right-skewed data)
    df[f'{col_name}_log'] = np.log1p(df[col_name])
    
    # Square root transformation
    df[f'{col_name}_sqrt'] = np.sqrt(df[col_name].clip(lower=0))  # Ensure non-negative
    
    # Box-Cox transformation (requires positive values)
    if (df[col_name] > 0).all():
        df[f'{col_name}_boxcox'], _ = stats.boxcox(df[col_name])
    else:
        # Add constant to make all values positive
        df[f'{col_name}_boxcox'], _ = stats.boxcox(df[col_name] + 1 - df[col_name].min())
    
    print(f"‚úì Transformations applied to '{col_name}'")
else:
    print(f"‚ö† Column '{col_name}' not found!")










## Cell 12: Outliers - Practical Application

print("=== HANDLING OUTLIERS ===")

# 1. Identify outliers in numerical columns
print("üîç 1. Identify Outliers in Numerical Columns:")
numerical_cols = ['age', 'salary', 'purchase_amount']

for col in numerical_cols:
    print(f"\n   Analyzing '{col}':")
    
    # Calculate statistics
    Q1 = df_clean[col].quantile(0.25)
    Q3 = df_clean[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    # Identify outliers
    outliers = df_clean[(df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)]
    
    print(f"     Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}")
    print(f"     Bounds: [{lower_bound:.2f}, {upper_bound:.2f}]")
    print(f"     Outliers: {len(outliers)} ({len(outliers)/len(df_clean)*100:.1f}%)")
    
    if len(outliers) > 0:
        print(f"     Example outliers: {outliers[col].iloc[:3].tolist()}")
print()

# 2. Visualize outliers
print("üìä 2. Visualize Outliers with Box Plots:")

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

for idx, col in enumerate(numerical_cols):
    axes[idx].boxplot(df_clean[col].dropna())
    axes[idx].set_title(f'{col} Distribution')
    axes[idx].set_ylabel(col)
    
    # Calculate and show outlier count
    Q1 = df_clean[col].quantile(0.25)
    Q3 = df_clean[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers_count = ((df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)).sum()
    
    axes[idx].text(0.95, 0.95, f'Outliers: {outliers_count}', 
                  transform=axes[idx].transAxes, ha='right', va='top',
                  bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))

plt.tight_layout()
plt.show()
print()

# 3. Handle outliers differently per column
print("üîÑ 3. Apply Different Strategies Per Column:")

# Strategy A: Remove impossible age values (age > 100)
print("   Strategy A: Remove impossible age values (age > 100):")
invalid_age = df_clean[df_clean['age'] > 100]
print(f"     Found {len(invalid_age)} records with age > 100")
df_clean = df_clean[df_clean['age'] <= 100]
print(f"     Removed. Remaining records: {len(df_clean)}")
print()

# Strategy B: Cap salary outliers (winsorize at 1st and 99th percentiles)
print("   Strategy B: Cap salary outliers (winsorize at 1st and 99th percentiles):")
lower_cap = df_clean['salary'].quantile(0.01)
upper_cap = df_clean['salary'].quantile(0.99)

df_clean['salary_capped'] = df_clean['salary'].clip(lower=lower_cap, upper=upper_cap)

print(f"     Capped salary between: [${lower_cap:,.2f}, ${upper_cap:,.2f}]")
print(f"     Original range: [${df_clean['salary'].min():,.2f}, ${df_clean['salary'].max():,.2f}]")
print(f"     Capped range: [${df_clean['salary_capped'].min():,.2f}, ${df_clean['salary_capped'].max():,.2f}]")
print()

# Strategy C: Transform purchase_amount with log (right-skewed)
print("   Strategy C: Transform purchase_amount with log (right-skewed):")
# Add small constant to avoid log(0)
df_clean['purchase_log'] = np.log1p(df_clean['purchase_amount'])

print(f"     Original purchase_amount - Skewness: {df_clean['purchase_amount'].skew():.2f}")
print(f"     Log-transformed - Skewness: {df_clean['purchase_log'].skew():.2f}")
print("     (Closer to 0 means more symmetric distribution)")
print()

# 4. Create outlier flags for analysis
print("üö© 4. Create Outlier Flags for Analysis:")

for col in ['salary', 'purchase_amount']:
    Q1 = df_clean[col].quantile(0.25)
    Q3 = df_clean[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    flag_col = f'{col}_outlier'
    df_clean[flag_col] = (df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)
    
    print(f"     {flag_col}: {df_clean[flag_col].sum()} outliers identified")

print("\n‚úÖ Outlier handling complete! We now have:")
print("   - Cleaned age (removed impossible values)")
print("   - Capped salary (winsorized)")
print("   - Log-transformed purchase amount")
print("   - Outlier flags for analysis")












































def generate_validation_report(df):
    report = {
        'summary': {
            'total_rows': len(df),
            'total_columns': len(df.columns),
            'total_cells': df.size
        },
        'completeness': {},
        'validity': {},
        'uniqueness': {},
        'issues': []
    }
    
    # Add checks here...
    
    return report







## Cell 14: Validation - Practical Application

print("=== DATA VALIDATION AND QUALITY CHECKS ===")

# 1. Create validation functions
print("üîç 1. Define Validation Rules:")

def validate_completeness(df):
    """Check for missing values in critical fields"""
    critical_fields = ['customer_id', 'name_clean', 'email_clean']
    results = {}
    
    for field in critical_fields:
        missing = df[field].isna().sum()
        pct_missing = (missing / len(df)) * 100
        results[field] = {
            'missing': missing,
            'pct_missing': pct_missing,
            'status': 'PASS' if pct_missing == 0 else 'FAIL'
        }
    
    return results

def validate_uniqueness(df):
    """Check uniqueness of key fields"""
    results = {}
    
    # customer_id should be unique
    duplicates = df['customer_id'].duplicated().sum()
    results['customer_id'] = {
        'duplicates': duplicates,
        'status': 'PASS' if duplicates == 0 else 'FAIL'
    }
    
    # email should be unique (excluding placeholder)
    valid_emails = df[~df['email_clean'].str.contains('placeholder', na=False)]
    email_dups = valid_emails['email_clean'].duplicated().sum()
    results['email'] = {
        'duplicates': email_dups,
        'status': 'PASS' if email_dups == 0 else 'FAIL'
    }
    
    return results

def validate_business_rules(df):
    """Validate against business logic"""
    issues = []
    
    # Rule 1: Age should be between 18 and 100
    invalid_age = df[(df['age'] < 18) | (df['age'] > 100)]
    if len(invalid_age) > 0:
        issues.append(f"Invalid age range: {len(invalid_age)} records")
    
    # Rule 2: Salary should be positive
    invalid_salary = df[df['salary'] <= 0]
    if len(invalid_salary) > 0:
        issues.append(f"Non-positive salary: {len(invalid_salary)} records")
    
    # Rule 3: Join date shouldn't be in future
    if 'join_date' in df.columns:
        future_join = df[df['join_date'] > pd.Timestamp.now()]
        if len(future_join) > 0:
            issues.append(f"Future join dates: {len(future_join)} records")
    
    # Rule 4: Active status requires valid email
    active_no_email = df[
        (df['status_standardized'] == 'active') & 
        (df['email_clean'].str.contains('placeholder', na=False))
    ]
    if len(active_no_email) > 0:
        issues.append(f"Active customers with placeholder email: {len(active_no_email)}")
    
    return issues

def validate_data_types(df):
    """Verify correct data types"""
    expected_types = {
        'customer_id': 'object',
        'age': 'float64',  # Allow NaN
        'salary': 'float64',
        'purchase_amount': 'float64',
        'join_date': 'datetime64[ns]'
    }
    
    issues = []
    for col, expected_type in expected_types.items():
        if col in df.columns:
            actual_type = str(df[col].dtype)
            if actual_type != expected_type:
                issues.append(f"{col}: Expected {expected_type}, got {actual_type}")
    
    return issues

# 2. Run validation
print("üöÄ 2. Run Validation Checks:")

validation_results = {
    'completeness': validate_completeness(df_clean),
    'uniqueness': validate_uniqueness(df_clean),
    'business_rules': validate_business_rules(df_clean),
    'data_types': validate_data_types(df_clean)
}

# 3. Display validation report
print("üìã 3. Validation Report:")

print("\nA. Completeness Check:")
for field, result in validation_results['completeness'].items():
    status_icon = "‚úÖ" if result['status'] == 'PASS' else "‚ùå"
    print(f"   {status_icon} {field}: {result['missing']} missing ({result['pct_missing']:.1f}%)")

print("\nB. Uniqueness Check:")
for field, result in validation_results['uniqueness'].items():
    status_icon = "‚úÖ" if result['status'] == 'PASS' else "‚ùå"
    print(f"   {status_icon} {field}: {result['duplicates']} duplicates")

print("\nC. Business Rule Violations:")
if validation_results['business_rules']:
    for issue in validation_results['business_rules']:
        print(f"   ‚ö†Ô∏è  {issue}")
else:
    print("   ‚úÖ No business rule violations")

print("\nD. Data Type Issues:")
if validation_results['data_types']:
    for issue in validation_results['data_types']:
        print(f"   ‚ö†Ô∏è  {issue}")
else:
    print("   ‚úÖ All data types correct")

# 4. Calculate overall data quality score
print("\nüìä 4. Data Quality Score Calculation:")

def calculate_quality_score(results):
    total_checks = 0
    passed_checks = 0
    
    # Completeness checks
    for field, result in results['completeness'].items():
        total_checks += 1
        if result['status'] == 'PASS':
            passed_checks += 1
    
    # Uniqueness checks
    for field, result in results['uniqueness'].items():
        total_checks += 1
        if result['status'] == 'PASS':
            passed_checks += 1
    
    # Business rules
    total_checks += 1
    if len(results['business_rules']) == 0:
        passed_checks += 1
    
    # Data types
    total_checks += 1
    if len(results['data_types']) == 0:
        passed_checks += 1
    
    return (passed_checks / total_checks) * 100

quality_score = calculate_quality_score(validation_results)
print(f"   Data Quality Score: {quality_score:.1f}%")

if quality_score >= 90:
    print("   üéâ Excellent data quality!")
elif quality_score >= 75:
    print("   üëç Good data quality")
elif quality_score >= 60:
    print("   ‚ö†Ô∏è  Acceptable data quality (needs improvement)")
else:
    print("   ‚ùå Poor data quality (requires significant cleaning)")






















































