{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["# Audio and Advanced Signal Processing\n- Audio I/O, spectral features, advanced techniques\n- Real examples: Audio processing, Voice activity detection"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["import numpy as np\nfrom scipy import signal\nimport matplotlib.pyplot as plt\nprint('Audio processing module loaded')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Audio Signal Basics\n- **Sample rate**: 44.1 kHz (CD quality), 48 kHz (professional)\n- **Bit depth**: 16-bit (CD), 24-bit (professional)\n- **Channels**: Mono (1), Stereo (2), Surround (5.1, 7.1)\n\n**Nyquist**: Human hearing ~20 Hz to 20 kHz\nSample at 40+ kHz to capture all frequencies"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Generate audio tone\nfs = 44100  # CD quality\nduration = 2\nt = np.linspace(0, duration, int(fs*duration))\n\n# A440 note (concert A)\nfreq = 440\naudio_tone = np.sin(2*np.pi*freq*t)\n\n# Apply envelope (attack-decay)\nenvelope = np.exp(-3*t)\naudio_tone *= envelope\n\nprint(f'Audio signal:')\nprint(f'  Sample rate: {fs} Hz')\nprint(f'  Duration: {duration} s')\nprint(f'  Samples: {len(audio_tone)}')\nprint(f'  Frequency: {freq} Hz (A4 note)')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Spectral Features\nExtract features for audio analysis/classification\n\n**Common features**:\n- **Spectral centroid**: Brightness (center of mass)\n- **Spectral rolloff**: Frequency below which X% of energy\n- **Zero-crossing rate**: Percussiveness, noisiness\n- **MFCCs**: Mel-frequency cepstral coefficients (speech)"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Spectral centroid\ndef spectral_centroid(sig, fs):\n    freqs = np.fft.rfftfreq(len(sig), 1/fs)\n    fft_vals = np.abs(np.fft.rfft(sig))\n    return np.sum(freqs * fft_vals) / np.sum(fft_vals)\n\n# Zero-crossing rate\ndef zero_crossing_rate(sig):\n    return np.sum(np.abs(np.diff(np.sign(sig)))) / (2 * len(sig))\n\n# Calculate for our tone\ncentroid = spectral_centroid(audio_tone, fs)\nzcr = zero_crossing_rate(audio_tone)\n\nprint(f'Audio features:')\nprint(f'  Spectral centroid: {centroid:.1f} Hz')\nprint(f'  Zero-crossing rate: {zcr:.4f}')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Noise Reduction: Spectral Subtraction\nEstimate noise spectrum and subtract from signal\n\n**Steps**:\n1. Estimate noise (from silent portion)\n2. FFT of noisy signal\n3. Subtract noise spectrum\n4. Inverse FFT\n\nUsed in: Voice enhancement, audio cleanup"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Clean speech simulation\nfs = 16000  # Speech sample rate\nt = np.linspace(0, 1, fs)\nspeech = np.sin(2*np.pi*300*t) + 0.5*np.sin(2*np.pi*800*t)  # Simple speech\n\n# Add noise\nnoise = np.random.randn(len(speech)) * 0.3\nnoisy_speech = speech + noise\n\n# Estimate noise spectrum (from first 0.1s)\nnoise_sample = noisy_speech[:int(0.1*fs)]\nnoise_spectrum = np.abs(np.fft.rfft(noise_sample))**2\n\n# Process with spectral subtraction (simplified)\nnoisy_spectrum = np.fft.rfft(noisy_speech)\nnoisy_mag = np.abs(noisy_spectrum)\nnoisy_phase = np.angle(noisy_spectrum)\n\n# Subtract noise\nclean_mag = np.maximum(noisy_mag - np.sqrt(noise_spectrum), 0.1*noisy_mag)\nclean_spectrum = clean_mag * np.exp(1j * noisy_phase)\n\n# Inverse FFT\nclean_speech = np.fft.irfft(clean_spectrum)\n\n# Calculate SNR improvement\nsnr_before = 10*np.log10(np.var(speech) / np.var(noise))\nsnr_after = 10*np.log10(np.var(speech) / np.var(clean_speech - speech))\n\nprint(f'Noise reduction:')\nprint(f'  SNR before: {snr_before:.2f} dB')\nprint(f'  SNR after: {snr_after:.2f} dB')\nprint(f'  Improvement: {snr_after - snr_before:.2f} dB')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Real Example: Voice Activity Detection (VAD)\nDetect speech vs silence in audio\nUsed in: Speech recognition, teleconferencing, compression\n\n**Method**: Energy + zero-crossing rate"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Simulate audio with speech and silence\nfs = 16000\nt_total = np.linspace(0, 5, 5*fs)\n\n# Create segments: silence-speech-silence-speech\naudio_vad = np.zeros(len(t_total))\n\n# Speech segments (higher energy, more zero crossings)\nspeech_seg1 = slice(int(0.5*fs), int(1.5*fs))\nspeech_seg2 = slice(int(3.0*fs), int(4.0*fs))\n\naudio_vad[speech_seg1] = np.random.randn(len(range(*speech_seg1.indices(len(audio_vad))))) * 0.5\naudio_vad[speech_seg2] = np.random.randn(len(range(*speech_seg2.indices(len(audio_vad))))) * 0.5\n\n# Add background noise everywhere\naudio_vad += np.random.randn(len(audio_vad)) * 0.05\n\n# VAD: Compute energy in frames\nframe_length = int(0.025 * fs)  # 25ms frames\nstep = int(0.010 * fs)  # 10ms step\n\nenergy = []\nfor i in range(0, len(audio_vad) - frame_length, step):\n    frame = audio_vad[i:i+frame_length]\n    energy.append(np.sum(frame**2))\n\nenergy = np.array(energy)\n\n# Threshold for voice detection\nthreshold = np.percentile(energy, 75)\nvoice_detected = energy > threshold\n\nprint(f'Voice Activity Detection:')\nprint(f'  Total frames: {len(energy)}')\nprint(f'  Voice frames: {voice_detected.sum()}')\nprint(f'  Silence frames: {(~voice_detected).sum()}')\nprint(f'  Voice activity: {voice_detected.sum()/len(voice_detected)*100:.1f}%')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Advanced: Spectrogram Manipulation\nProcess signals in time-frequency domain\n\n**Applications**:\n- Time stretching (change speed without pitch)\n- Pitch shifting (change pitch without speed)\n- Source separation\n- Audio effects"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Generate chirp\nfs = 8000\nt = np.linspace(0, 2, 2*fs)\nchirp_audio = signal.chirp(t, f0=200, f1=2000, t1=2)\n\n# STFT\nf, t_spec, Zxx = signal.stft(chirp_audio, fs=fs, nperseg=256)\n\nprint(f'STFT analysis:')\nprint(f'  Frequency bins: {len(f)}')\nprint(f'  Time frames: {len(t_spec)}')\nprint(f'  Spectrogram shape: {Zxx.shape}')\n\n# Manipulate: Pitch shift up by multiplying frequencies\n# (Simplified - real pitch shifting is more complex)\nZxx_shifted = np.vstack([Zxx[::2, :], np.zeros((Zxx.shape[0]//2, Zxx.shape[1]))])\n\n# Inverse STFT\n_, audio_shifted = signal.istft(Zxx_shifted, fs=fs)\n\nprint(f'\nPitch shift applied (simplified version)')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Summary\n\n### Audio Processing Pipeline:\n1. **Load audio**: Read file, resample if needed\n2. **Preprocess**: Normalize, remove DC, pre-emphasis\n3. **Feature extraction**: STFT, MFCCs, spectral features\n4. **Processing**: Filtering, enhancement, effects\n5. **Analysis**: Classification, detection, recognition\n\n### Key Techniques:\n\n**Time domain**:\n- Amplitude envelope\n- Zero-crossing rate\n- Energy/RMS\n\n**Frequency domain**:\n- FFT spectrum\n- Spectral centroid\n- Spectral rolloff\n- Spectral flux\n\n**Time-frequency**:\n- STFT/Spectrogram\n- Mel-spectrogram\n- Wavelet transform\n\n### Applications:\n\n**Speech**:\n- Voice activity detection\n- Speech recognition\n- Speaker identification\n- Noise reduction\n\n**Music**:\n- Beat tracking\n- Tempo estimation\n- Chord recognition\n- Genre classification\n\n**General**:\n- Audio compression\n- Sound effects\n- Acoustic analysis\n- Environmental sound classification\n\n### Best Practices:\n\n✓ **Sample rate**: 44.1/48 kHz for audio, 16 kHz for speech\n✓ **Framing**: 20-40ms windows, 50% overlap\n✓ **Windowing**: Hann/Hamming for STFT\n✓ **Normalize**: Scale audio to [-1, 1]\n✓ **Pre-emphasis**: Boost high frequencies for speech\n\n### Advanced Topics:\n- Deep learning (neural audio processing)\n- Source separation (isolate instruments)\n- Audio synthesis (generate sounds)\n- Spatial audio (3D sound)\n- Real-time processing (low latency)"]
    }
  ],
  "metadata": {
    "kernelspec": {"display_name": "Python 3", "name": "python3"},
    "language_info": {"version": "3.8.0"}
  },
  "nbformat": 4,
  "nbformat_minor": 5
}