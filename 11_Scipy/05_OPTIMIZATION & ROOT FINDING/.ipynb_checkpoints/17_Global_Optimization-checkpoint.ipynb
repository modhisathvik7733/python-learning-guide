{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Global Optimization\n",
        "- **Purpose**: Find global minimum among many local minima\n",
        "- **scipy.optimize**: differential_evolution, basinhopping, shgo, dual_annealing\n",
        "- **Challenge**: Avoid getting stuck in local minima\n",
        "\n",
        "Key concepts:\n",
        "- **Local minimum**: f(x*) \u2264 f(x) nearby (what gradient methods find)\n",
        "- **Global minimum**: f(x*) \u2264 f(x) everywhere (what we want)\n",
        "- **Multimodal functions**: Multiple peaks and valleys\n",
        "- **Exploration vs exploitation**: Balance searching widely vs refining\n",
        "\n",
        "Real applications:\n",
        "- **Machine Learning**: Hyperparameter optimization, architecture search\n",
        "- **Molecular dynamics**: Find lowest energy configuration\n",
        "- **Engineering design**: Optimal component parameters\n",
        "- **Trading strategies**: Parameter optimization\n",
        "- **Drug discovery**: Molecular docking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import optimize\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "np.set_printoptions(precision=4, suppress=True)\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "\n",
        "print(\"Global optimization module loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Local Optimization Fails\n",
        "\n",
        "**Example**: Multiple minima function\n",
        "\n",
        "\\[ f(x) = \\sin(x) + \\sin\\left(\\frac{10x}{3}\\right) \\]\n",
        "\n",
        "**Problem**: Gradient descent finds nearest local minimum\n",
        "\n",
        "**Solution**: Global optimization algorithms explore entire domain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multimodal function\n",
        "def f_multimodal(x):\n",
        "    return np.sin(x) + np.sin(10*x/3)\n",
        "\n",
        "# Try local optimization from different starting points\n",
        "x0_values = [1.0, 3.0, 5.0, 7.0]\n",
        "\n",
        "print(\"Local Optimization on Multimodal Function\")\n",
        "print(\"f(x) = sin(x) + sin(10x/3)\\n\")\n",
        "\n",
        "print(\"Starting from different initial points:\\n\")\n",
        "local_results = []\n",
        "for x0 in x0_values:\n",
        "    result = optimize.minimize_scalar(f_multimodal, bracket=(x0-0.5, x0+0.5),\n",
        "                                     method='brent')\n",
        "    local_results.append(result)\n",
        "    print(f\"x0 = {x0:.1f} \u2192 minimum at x = {result.x:.4f}, f = {result.fun:.4f}\")\n",
        "\n",
        "# Global optimization\n",
        "bounds = [(0, 8)]\n",
        "result_global = optimize.differential_evolution(lambda x: f_multimodal(x[0]), bounds)\n",
        "\n",
        "print(f\"\\nGlobal optimization:\")\n",
        "print(f\"  Global minimum at x = {result_global.x[0]:.4f}, f = {result_global.fun:.4f}\")\n",
        "\n",
        "# Visualize\n",
        "x = np.linspace(0, 8, 500)\n",
        "y = f_multimodal(x)\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.plot(x, y, 'b-', linewidth=2, label='f(x) = sin(x) + sin(10x/3)')\n",
        "\n",
        "# Local minima\n",
        "for i, (x0, result) in enumerate(zip(x0_values, local_results)):\n",
        "    plt.plot(result.x, result.fun, 'o', markersize=10, \n",
        "             label=f'Local min from x0={x0:.1f}')\n",
        "\n",
        "# Global minimum\n",
        "plt.plot(result_global.x[0], result_global.fun, 'r*', markersize=25,\n",
        "         label=f'Global min: x={result_global.x[0]:.3f}')\n",
        "\n",
        "plt.xlabel('x', fontsize=13)\n",
        "plt.ylabel('f(x)', fontsize=13)\n",
        "plt.title('Local vs Global Optimization', fontsize=15)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nLocal methods find different minima depending on starting point!\")\n",
        "print(\"Global optimization finds the true global minimum.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Differential Evolution\n",
        "\n",
        "**Algorithm**: Evolutionary algorithm inspired by natural selection\n",
        "\n",
        "**How it works**:\n",
        "1. Create population of candidate solutions\n",
        "2. Mutate and combine candidates (crossover)\n",
        "3. Keep best solutions (selection)\n",
        "4. Repeat until convergence\n",
        "\n",
        "**Advantages**:\n",
        "- No gradient needed\n",
        "- Good for multimodal functions\n",
        "- Handles bounds naturally\n",
        "- Robust and reliable\n",
        "\n",
        "**Syntax**:\n",
        "```python\n",
        "bounds = [(x1_min, x1_max), (x2_min, x2_max), ...]\n",
        "result = optimize.differential_evolution(func, bounds)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rastrigin function: classic test for global optimization\n",
        "# Many local minima, one global minimum at origin\n",
        "def rastrigin(x):\n",
        "    n = len(x)\n",
        "    A = 10\n",
        "    return A*n + np.sum(x**2 - A*np.cos(2*np.pi*x))\n",
        "\n",
        "# 2D version for visualization\n",
        "print(\"Rastrigin Function (highly multimodal)\")\n",
        "print(\"  Many local minima\")\n",
        "print(\"  Global minimum at (0, 0), f = 0\\n\")\n",
        "\n",
        "# Bounds\n",
        "bounds = [(-5, 5), (-5, 5)]\n",
        "\n",
        "# Differential Evolution\n",
        "result = optimize.differential_evolution(rastrigin, bounds, seed=42)\n",
        "\n",
        "print(\"Differential Evolution result:\")\n",
        "print(f\"  Global minimum: x = {result.x}\")\n",
        "print(f\"  f(x) = {result.fun:.6f}\")\n",
        "print(f\"  Function evaluations: {result.nfev}\")\n",
        "print(f\"  Success: {result.success}\")\n",
        "print(f\"\\nError from true minimum: {np.linalg.norm(result.x):.2e}\")\n",
        "\n",
        "# Visualize Rastrigin\n",
        "x = np.linspace(-5, 5, 200)\n",
        "y = np.linspace(-5, 5, 200)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = np.zeros_like(X)\n",
        "for i in range(X.shape[0]):\n",
        "    for j in range(X.shape[1]):\n",
        "        Z[i, j] = rastrigin([X[i, j], Y[i, j]])\n",
        "\n",
        "fig = plt.figure(figsize=(16, 7))\n",
        "\n",
        "# 3D plot\n",
        "ax1 = fig.add_subplot(121, projection='3d')\n",
        "surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
        "ax1.plot([result.x[0]], [result.x[1]], [result.fun], 'r*', markersize=20)\n",
        "ax1.set_xlabel('x', fontsize=12)\n",
        "ax1.set_ylabel('y', fontsize=12)\n",
        "ax1.set_zlabel('f(x,y)', fontsize=12)\n",
        "ax1.set_title('Rastrigin Function 3D', fontsize=14)\n",
        "\n",
        "# Contour plot\n",
        "ax2 = fig.add_subplot(122)\n",
        "contour = ax2.contour(X, Y, Z, levels=30, cmap='viridis')\n",
        "ax2.plot(0, 0, 'go', markersize=15, label='True global min (0,0)')\n",
        "ax2.plot(result.x[0], result.x[1], 'r*', markersize=20,\n",
        "         label=f'Found: ({result.x[0]:.3f}, {result.x[1]:.3f})')\n",
        "ax2.set_xlabel('x', fontsize=12)\n",
        "ax2.set_ylabel('y', fontsize=12)\n",
        "ax2.set_title('Rastrigin Contours (many local minima)', fontsize=14)\n",
        "ax2.legend(fontsize=11)\n",
        "plt.colorbar(contour, ax=ax2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nDifferential Evolution successfully finds global minimum!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basin Hopping\n",
        "\n",
        "**Algorithm**: Combines local optimization with random perturbations\n",
        "\n",
        "**How it works**:\n",
        "1. Run local optimization from current point\n",
        "2. Accept or reject new minimum (Metropolis criterion)\n",
        "3. Random jump to new region\n",
        "4. Repeat\n",
        "\n",
        "**Advantages**:\n",
        "- Leverages fast local optimizers\n",
        "- Escapes local minima via jumps\n",
        "- Good for smooth functions\n",
        "\n",
        "**Use when**: Function is continuous and differentiable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Eggholder function: challenging landscape\n",
        "def eggholder(x):\n",
        "    return (-(x[1] + 47) * np.sin(np.sqrt(abs(x[0]/2 + (x[1] + 47)))) -\n",
        "            x[0] * np.sin(np.sqrt(abs(x[0] - (x[1] + 47)))))\n",
        "\n",
        "# Global minimum around (512, 404.2319), f \u2248 -959.6407\n",
        "\n",
        "print(\"Eggholder Function (very challenging)\")\n",
        "print(\"  Complex landscape with many local minima\")\n",
        "print(\"  Global minimum around (512, 404), f \u2248 -959.64\\n\")\n",
        "\n",
        "# Basin hopping\n",
        "minimizer_kwargs = {\"method\": \"L-BFGS-B\", \n",
        "                   \"bounds\": [(-512, 512), (-512, 512)]}\n",
        "result = optimize.basinhopping(eggholder, [0, 0], \n",
        "                              minimizer_kwargs=minimizer_kwargs,\n",
        "                              niter=100, seed=42)\n",
        "\n",
        "print(\"Basin Hopping result:\")\n",
        "print(f\"  Global minimum: x = {result.x}\")\n",
        "print(f\"  f(x) = {result.fun:.4f}\")\n",
        "print(f\"  Function evaluations: {result.nfev}\")\n",
        "print(f\"\\nKnown global minimum: f \u2248 -959.64\")\n",
        "print(f\"Error: {abs(result.fun - (-959.64)):.4f}\")\n",
        "\n",
        "# Visualize (zoomed to interesting region)\n",
        "x = np.linspace(400, 600, 200)\n",
        "y = np.linspace(300, 500, 200)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = np.zeros_like(X)\n",
        "for i in range(X.shape[0]):\n",
        "    for j in range(X.shape[1]):\n",
        "        Z[i, j] = eggholder([X[i, j], Y[i, j]])\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "contour = plt.contour(X, Y, Z, levels=30, cmap='viridis')\n",
        "plt.colorbar(contour, label='f(x,y)')\n",
        "plt.plot(512, 404.2319, 'go', markersize=15, label='Known global min')\n",
        "plt.plot(result.x[0], result.x[1], 'r*', markersize=20,\n",
        "         label=f'Found: ({result.x[0]:.1f}, {result.x[1]:.1f})')\n",
        "plt.xlabel('x', fontsize=13)\n",
        "plt.ylabel('y', fontsize=13)\n",
        "plt.title('Eggholder Function: Complex Landscape', fontsize=15)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nBasin hopping successfully navigates complex landscape!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real Example: Hyperparameter Tuning\n",
        "\n",
        "**Problem**: Find optimal hyperparameters for ML model\n",
        "\n",
        "**Hyperparameters** (for gradient boosting):\n",
        "- Learning rate: 0.01 - 0.3\n",
        "- Max depth: 3 - 10\n",
        "- Min samples split: 2 - 20\n",
        "\n",
        "**Objective**: Minimize validation error (non-convex!)\n",
        "\n",
        "**Challenge**: Expensive function evaluations (train model each time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate ML model validation error (synthetic)\n",
        "# In reality, this would train and evaluate actual model\n",
        "def model_error(params):\n",
        "    \"\"\"Simulated validation error for hyperparameters\"\"\"\n",
        "    learning_rate, max_depth, min_samples = params\n",
        "    \n",
        "    # Synthetic error function (multimodal)\n",
        "    # Real function would be from cross-validation\n",
        "    error = (0.3 + 0.2*np.sin(10*learning_rate) + \n",
        "             0.1*np.sin(max_depth) + \n",
        "             0.05*np.sin(min_samples/5))\n",
        "    \n",
        "    # Penalty for extreme values\n",
        "    if learning_rate < 0.05 or learning_rate > 0.25:\n",
        "        error += 0.1\n",
        "    if max_depth < 4 or max_depth > 8:\n",
        "        error += 0.05\n",
        "    \n",
        "    # Add noise (stochastic validation)\n",
        "    error += np.random.randn() * 0.01\n",
        "    \n",
        "    return error\n",
        "\n",
        "# Bounds for hyperparameters\n",
        "bounds = [\n",
        "    (0.01, 0.3),   # learning_rate\n",
        "    (3, 10),       # max_depth\n",
        "    (2, 20)        # min_samples_split\n",
        "]\n",
        "\n",
        "print(\"Hyperparameter Optimization\")\n",
        "print(\"\\nHyperparameters:\")\n",
        "print(\"  Learning rate: [0.01, 0.3]\")\n",
        "print(\"  Max depth: [3, 10]\")\n",
        "print(\"  Min samples split: [2, 20]\")\n",
        "print(\"\\nObjective: Minimize validation error\\n\")\n",
        "\n",
        "# Use Differential Evolution (good for expensive functions)\n",
        "np.random.seed(42)\n",
        "result = optimize.differential_evolution(\n",
        "    model_error, \n",
        "    bounds, \n",
        "    maxiter=50,  # Limit iterations (expensive function)\n",
        "    seed=42,\n",
        "    workers=1\n",
        ")\n",
        "\n",
        "lr_opt, depth_opt, samples_opt = result.x\n",
        "\n",
        "print(\"Optimal hyperparameters:\")\n",
        "print(f\"  Learning rate: {lr_opt:.4f}\")\n",
        "print(f\"  Max depth: {int(round(depth_opt))}\")\n",
        "print(f\"  Min samples split: {int(round(samples_opt))}\")\n",
        "print(f\"\\nOptimized validation error: {result.fun:.4f}\")\n",
        "print(f\"Function evaluations: {result.nfev}\")\n",
        "\n",
        "# Compare with random search (baseline)\n",
        "n_random = 50\n",
        "random_errors = []\n",
        "np.random.seed(42)\n",
        "for _ in range(n_random):\n",
        "    random_params = [\n",
        "        np.random.uniform(bounds[0][0], bounds[0][1]),\n",
        "        np.random.uniform(bounds[1][0], bounds[1][1]),\n",
        "        np.random.uniform(bounds[2][0], bounds[2][1])\n",
        "    ]\n",
        "    random_errors.append(model_error(random_params))\n",
        "\n",
        "best_random = np.min(random_errors)\n",
        "\n",
        "print(f\"\\nComparison:\")\n",
        "print(f\"  Differential Evolution: {result.fun:.4f}\")\n",
        "print(f\"  Random Search (50 trials): {best_random:.4f}\")\n",
        "print(f\"  Improvement: {(best_random - result.fun)/best_random * 100:.1f}%\")\n",
        "\n",
        "# Visualize convergence\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.hist(random_errors, bins=20, alpha=0.6, label='Random search', color='blue')\n",
        "plt.axvline(result.fun, color='red', linestyle='--', linewidth=3,\n",
        "           label=f'Differential Evolution: {result.fun:.4f}')\n",
        "plt.axvline(best_random, color='green', linestyle='--', linewidth=2,\n",
        "           label=f'Best random: {best_random:.4f}')\n",
        "plt.xlabel('Validation Error', fontsize=13)\n",
        "plt.ylabel('Frequency', fontsize=13)\n",
        "plt.title('Hyperparameter Optimization: DE vs Random Search', fontsize=14)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nDifferential Evolution outperforms random search!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dual Annealing\n",
        "\n",
        "**Algorithm**: Simulated annealing with dual temperature schedule\n",
        "\n",
        "**Inspiration**: Physical annealing (cooling metal to minimize energy)\n",
        "\n",
        "**How it works**:\n",
        "1. Start at high \"temperature\" (accept worse solutions)\n",
        "2. Gradually \"cool\" (become more selective)\n",
        "3. Escape local minima early, refine solution later\n",
        "\n",
        "**Advantages**:\n",
        "- Probabilistic completeness (guaranteed to find global minimum given infinite time)\n",
        "- Good for expensive functions\n",
        "- Handles bounds naturally\n",
        "\n",
        "**Use when**: Other methods struggle, have computational budget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ackley function: Another classic test\n",
        "def ackley(x):\n",
        "    n = len(x)\n",
        "    sum1 = np.sum(x**2)\n",
        "    sum2 = np.sum(np.cos(2*np.pi*x))\n",
        "    return (-20*np.exp(-0.2*np.sqrt(sum1/n)) - \n",
        "            np.exp(sum2/n) + 20 + np.e)\n",
        "\n",
        "print(\"Ackley Function\")\n",
        "print(\"  Global minimum at origin (0, 0), f = 0\\n\")\n",
        "\n",
        "bounds = [(-5, 5), (-5, 5)]\n",
        "\n",
        "# Dual Annealing\n",
        "result_da = optimize.dual_annealing(ackley, bounds, seed=42)\n",
        "\n",
        "# Compare with Differential Evolution\n",
        "result_de = optimize.differential_evolution(ackley, bounds, seed=42)\n",
        "\n",
        "print(\"Dual Annealing:\")\n",
        "print(f\"  Minimum: x = {result_da.x}\")\n",
        "print(f\"  f(x) = {result_da.fun:.6f}\")\n",
        "print(f\"  Function evaluations: {result_da.nfev}\")\n",
        "\n",
        "print(\"\\nDifferential Evolution:\")\n",
        "print(f\"  Minimum: x = {result_de.x}\")\n",
        "print(f\"  f(x) = {result_de.fun:.6f}\")\n",
        "print(f\"  Function evaluations: {result_de.nfev}\")\n",
        "\n",
        "# Visualize Ackley\n",
        "x = np.linspace(-5, 5, 200)\n",
        "y = np.linspace(-5, 5, 200)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = np.zeros_like(X)\n",
        "for i in range(X.shape[0]):\n",
        "    for j in range(X.shape[1]):\n",
        "        Z[i, j] = ackley([X[i, j], Y[i, j]])\n",
        "\n",
        "fig = plt.figure(figsize=(16, 7))\n",
        "\n",
        "# 3D\n",
        "ax1 = fig.add_subplot(121, projection='3d')\n",
        "surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
        "ax1.plot([0], [0], [0], 'g*', markersize=20, label='True min')\n",
        "ax1.set_xlabel('x', fontsize=12)\n",
        "ax1.set_ylabel('y', fontsize=12)\n",
        "ax1.set_zlabel('f(x,y)', fontsize=12)\n",
        "ax1.set_title('Ackley Function 3D', fontsize=14)\n",
        "\n",
        "# Contour\n",
        "ax2 = fig.add_subplot(122)\n",
        "contour = ax2.contour(X, Y, Z, levels=30, cmap='viridis')\n",
        "ax2.plot(0, 0, 'go', markersize=15, label='True global min')\n",
        "ax2.plot(result_da.x[0], result_da.x[1], 'r*', markersize=20,\n",
        "         label=f'Dual Annealing: {result_da.fun:.4f}')\n",
        "ax2.plot(result_de.x[0], result_de.x[1], 'b^', markersize=15,\n",
        "         label=f'Diff. Evolution: {result_de.fun:.4f}')\n",
        "ax2.set_xlabel('x', fontsize=12)\n",
        "ax2.set_ylabel('y', fontsize=12)\n",
        "ax2.set_title('Ackley Contours', fontsize=14)\n",
        "ax2.legend(fontsize=10)\n",
        "plt.colorbar(contour, ax=ax2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nBoth methods successfully find global minimum!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Method Comparison\n",
        "\n",
        "**scipy.optimize global methods**:\n",
        "\n",
        "| Method | Type | Gradient | Constraints | Best For |\n",
        "|--------|------|----------|-------------|----------|\n",
        "| **differential_evolution** | Evolutionary | No | Bounds | General, robust |\n",
        "| **basinhopping** | Stochastic | Uses local | Bounds | Smooth functions |\n",
        "| **dual_annealing** | Simulated annealing | No | Bounds | Expensive functions |\n",
        "| **shgo** | Simplicial | Yes (opt) | Nonlinear | Lipschitz-cont. |\n",
        "| **brute** | Grid search | No | Bounds | Low dimensions |\n",
        "\n",
        "### Recommendations:\n",
        "\n",
        "**Default choice**: `differential_evolution`\n",
        "- Most reliable\n",
        "- No gradient needed\n",
        "- Good performance\n",
        "\n",
        "**For smooth functions**: `basinhopping`\n",
        "- Leverages fast local optimizers\n",
        "- Better convergence rate\n",
        "\n",
        "**For expensive functions**: `dual_annealing`\n",
        "- Fewer function evaluations\n",
        "- Theoretical guarantees\n",
        "\n",
        "**High dimensions**: `L-BFGS-B` + multiple restarts\n",
        "- Global methods scale poorly > 10 dimensions\n",
        "- Use local method with many starting points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all methods on Rastrigin 2D\n",
        "print(\"Global Method Comparison (Rastrigin 2D)\\n\")\n",
        "print(f\"{'Method':<25} {'f(x)':<12} {'nfev':<8} {'Time (s)':<10}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "bounds = [(-5, 5), (-5, 5)]\n",
        "methods = [\n",
        "    ('differential_evolution', \n",
        "     lambda: optimize.differential_evolution(rastrigin, bounds, seed=42)),\n",
        "    ('dual_annealing', \n",
        "     lambda: optimize.dual_annealing(rastrigin, bounds, seed=42)),\n",
        "    ('basinhopping',\n",
        "     lambda: optimize.basinhopping(rastrigin, [0, 0], \n",
        "                                  minimizer_kwargs={'method': 'L-BFGS-B',\n",
        "                                                   'bounds': bounds},\n",
        "                                  niter=50, seed=42)),\n",
        "]\n",
        "\n",
        "import time\n",
        "results = {}\n",
        "\n",
        "for name, method in methods:\n",
        "    t0 = time.time()\n",
        "    result = method()\n",
        "    elapsed = time.time() - t0\n",
        "    results[name] = result\n",
        "    print(f\"{name:<25} {result.fun:<12.6f} {result.nfev:<8} {elapsed:<10.4f}\")\n",
        "\n",
        "print(\"\\nAll methods successfully find global minimum (f \u2248 0)!\")\n",
        "print(\"\\nKey differences:\")\n",
        "print(\"  - differential_evolution: Most function evaluations, very robust\")\n",
        "print(\"  - dual_annealing: Balanced performance\")\n",
        "print(\"  - basinhopping: Fast if good local optimizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### When to Use Global Optimization:\n",
        "\n",
        "\u2713 **Multimodal functions** (many local minima)  \n",
        "\u2713 **Unknown landscape** (don't know where minimum is)  \n",
        "\u2713 **Non-convex problems**  \n",
        "\u2713 **Hyperparameter tuning**  \n",
        "\u2713 **Design optimization**  \n",
        "\n",
        "### Best Practices:\n",
        "\n",
        "1. **Set bounds tightly** \u2192 faster convergence\n",
        "2. **Use local optimization** as polish step\n",
        "3. **Multiple runs** with different seeds\n",
        "4. **Parallel evaluation** when possible\n",
        "5. **Budget function evaluations** for expensive functions\n",
        "\n",
        "### Typical Workflow:\n",
        "\n",
        "```python\n",
        "# 1. Global search\n",
        "result_global = optimize.differential_evolution(func, bounds)\n",
        "\n",
        "# 2. Local refinement (optional)\n",
        "result_local = optimize.minimize(func, result_global.x, method='L-BFGS-B')\n",
        "\n",
        "# 3. Verify\n",
        "if result_local.fun < result_global.fun:\n",
        "    optimal = result_local\n",
        "else:\n",
        "    optimal = result_global\n",
        "```\n",
        "\n",
        "### Limitations:\n",
        "\n",
        "\u26a0\ufe0f **Computational cost**: Many function evaluations  \n",
        "\u26a0\ufe0f **No guarantees**: May not find global minimum (except theoretically for some methods)  \n",
        "\u26a0\ufe0f **Scaling**: Exponential with dimensions (curse of dimensionality)  \n",
        "\u26a0\ufe0f **Hyperparameters**: Methods have their own parameters to tune  \n",
        "\n",
        "### Applications:\n",
        "\n",
        "- **Machine Learning**: Neural architecture search, AutoML\n",
        "- **Chemistry**: Molecular conformation, drug design\n",
        "- **Engineering**: Design optimization, control tuning\n",
        "- **Finance**: Portfolio optimization, strategy parameters\n",
        "- **Physics**: Ground state energy, phase transitions\n",
        "\n",
        "### Alternatives for High Dimensions:\n",
        "\n",
        "- **Bayesian Optimization** (for expensive black-box)\n",
        "- **Gradient-based** with multiple restarts\n",
        "- **Evolutionary strategies** (CMA-ES)\n",
        "- **Reinforcement learning** (for sequential decisions)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}