{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c45f51",
   "metadata": {},
   "source": [
    "# Univariate Optimization (1D)\n",
    "- **Purpose**: Find minimum or maximum of single-variable functions\n",
    "- **scipy.optimize**: minimize_scalar, brent, golden, bracket\n",
    "- **Methods**: Brent, Golden Section, Bounded\n",
    "\n",
    "Key concepts:\n",
    "- **Univariate**: Function of one variable f(x)\n",
    "- **Local minimum**: f(x*) ≤ f(x) for nearby x\n",
    "- **Global minimum**: f(x*) ≤ f(x) for all x\n",
    "- **Bracketing**: Find interval containing minimum\n",
    "\n",
    "Real applications:\n",
    "- Hyperparameter tuning (learning rate, regularization)\n",
    "- Line search in optimization algorithms\n",
    "- Cost minimization (pricing, inventory)\n",
    "- Physics: projectile range, optimal angle\n",
    "- Economics: profit maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8708211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Univariate optimization module loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"Univariate optimization module loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32f5fa2",
   "metadata": {},
   "source": [
    "## Basic Example: Parabola\n",
    "\n",
    "**Simple function**: \\( f(x) = (x-3)^2 + 1 \\)\n",
    "\n",
    "**Analytical minimum**: x = 3, f(3) = 1\n",
    "\n",
    "**scipy.optimize.minimize_scalar()**: Main function for 1D optimization\n",
    "\n",
    "**Syntax**:\n",
    "```python\n",
    "result = optimize.minimize_scalar(f)\n",
    "result.x      # Optimal x\n",
    "result.fun    # Minimum value f(x)\n",
    "result.nfev   # Number of function evaluations\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8a40549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimize f(x) = (x-3)² + 1\n",
      "\n",
      "Optimization result:\n",
      "  Minimum at x = 3.000000\n",
      "  Minimum value f(x) = 1.000000\n",
      "  Function evaluations: 9\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/scipy/_lib/_util.py:1091\u001b[0m, in \u001b[0;36m_RichResult.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1091\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'method'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Minimum value f(x) = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mfun\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Function evaluations: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mnfev\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Method: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnalytical solution: x = 3, f(3) = 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mabs\u001b[39m(result\u001b[38;5;241m.\u001b[39mx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/scipy/_lib/_util.py:1093\u001b[0m, in \u001b[0;36m_RichResult.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1091\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(name) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: method"
     ]
    }
   ],
   "source": [
    "# Define function\n",
    "def f(x):\n",
    "    return (x - 3)**2 + 1\n",
    "\n",
    "# Minimize\n",
    "result = optimize.minimize_scalar(f)\n",
    "\n",
    "print(\"Minimize f(x) = (x-3)² + 1\")\n",
    "print(\"\\nOptimization result:\")\n",
    "print(f\"  Minimum at x = {result.x:.6f}\")\n",
    "print(f\"  Minimum value f(x) = {result.fun:.6f}\")\n",
    "print(f\"  Function evaluations: {result.nfev}\")\n",
    "print(f\"  Method: {result.method}\")\n",
    "print(f\"\\nAnalytical solution: x = 3, f(3) = 1\")\n",
    "print(f\"Error: {abs(result.x - 3):.2e}\")\n",
    "\n",
    "# Visualize\n",
    "x = np.linspace(0, 6, 300)\n",
    "y = f(x)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(x, y, 'b-', linewidth=2, label='f(x) = (x-3)² + 1')\n",
    "plt.plot(result.x, result.fun, 'ro', markersize=15, \n",
    "         label=f'Minimum: ({result.x:.4f}, {result.fun:.4f})')\n",
    "plt.xlabel('x', fontsize=13)\n",
    "plt.ylabel('f(x)', fontsize=13)\n",
    "plt.title('Univariate Optimization: Simple Parabola', fontsize=15)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOptimization successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c670c913",
   "metadata": {},
   "source": [
    "## Bounded Optimization\n",
    "\n",
    "**Problem**: Find minimum within bounds [a, b]\n",
    "\n",
    "**Use case**: Physical constraints, search regions\n",
    "\n",
    "**Syntax**:\n",
    "```python\n",
    "result = optimize.minimize_scalar(f, bounds=(a, b), method='bounded')\n",
    "```\n",
    "\n",
    "**Methods**:\n",
    "- **'bounded'**: Brent method with bounds\n",
    "- Guaranteed to converge within bounds\n",
    "- Handles boundary minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a924ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function with minimum outside natural range\n",
    "def f(x):\n",
    "    return x**2 - 10*x + 25  # Minimum at x=5\n",
    "\n",
    "# Case 1: Minimum inside bounds\n",
    "result1 = optimize.minimize_scalar(f, bounds=(0, 10), method='bounded')\n",
    "\n",
    "# Case 2: Minimum outside bounds (at boundary)\n",
    "result2 = optimize.minimize_scalar(f, bounds=(6, 10), method='bounded')\n",
    "\n",
    "print(\"Bounded Optimization: f(x) = x² - 10x + 25\")\n",
    "print(\"Analytical minimum: x = 5, f(5) = 0\\n\")\n",
    "\n",
    "print(\"Case 1: Bounds [0, 10] (contains minimum)\")\n",
    "print(f\"  Optimal x = {result1.x:.6f}\")\n",
    "print(f\"  Minimum f(x) = {result1.fun:.6f}\")\n",
    "\n",
    "print(\"\\nCase 2: Bounds [6, 10] (minimum outside)\")\n",
    "print(f\"  Optimal x = {result2.x:.6f} (boundary!)\")\n",
    "print(f\"  Minimum f(x) = {result2.fun:.6f}\")\n",
    "\n",
    "# Visualize\n",
    "x = np.linspace(0, 10, 300)\n",
    "y = f(x)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(x, y, 'b-', linewidth=2, label='f(x) = x² - 10x + 25')\n",
    "plt.axvline(5, color='gray', linestyle='--', alpha=0.5, label='True minimum (x=5)')\n",
    "plt.plot(result1.x, result1.fun, 'go', markersize=15, \n",
    "         label=f'Case 1: x={result1.x:.2f} (interior)')\n",
    "plt.plot(result2.x, result2.fun, 'ro', markersize=15, \n",
    "         label=f'Case 2: x={result2.x:.2f} (boundary)')\n",
    "plt.axvspan(6, 10, alpha=0.2, color='red', label='Case 2 bounds')\n",
    "plt.xlabel('x', fontsize=13)\n",
    "plt.ylabel('f(x)', fontsize=13)\n",
    "plt.title('Bounded Optimization: Interior vs Boundary Minimum', fontsize=15)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6df1b63",
   "metadata": {},
   "source": [
    "## Real Example: Optimal Pricing\n",
    "\n",
    "**Business problem**: Maximize profit by finding optimal price\n",
    "\n",
    "**Model**:\n",
    "- **Demand**: Q(p) = 1000 - 20p (decreases with price)\n",
    "- **Cost**: C = 10 per unit\n",
    "- **Revenue**: R(p) = p × Q(p)\n",
    "- **Profit**: Π(p) = R(p) - C × Q(p)\n",
    "\n",
    "**Goal**: Find price p* that maximizes profit\n",
    "\n",
    "**Note**: Maximize f(x) = Minimize -f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6ea4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profit function\n",
    "def profit(price):\n",
    "    demand = 1000 - 20*price\n",
    "    if demand <= 0:\n",
    "        return -1e6  # Invalid price\n",
    "    cost_per_unit = 10\n",
    "    revenue = price * demand\n",
    "    cost = cost_per_unit * demand\n",
    "    return revenue - cost\n",
    "\n",
    "# Maximize profit = minimize negative profit\n",
    "def neg_profit(price):\n",
    "    return -profit(price)\n",
    "\n",
    "# Find optimal price (must be positive, max reasonable price 50)\n",
    "result = optimize.minimize_scalar(neg_profit, bounds=(10, 50), method='bounded')\n",
    "\n",
    "optimal_price = result.x\n",
    "max_profit = -result.fun\n",
    "optimal_demand = 1000 - 20*optimal_price\n",
    "revenue = optimal_price * optimal_demand\n",
    "cost = 10 * optimal_demand\n",
    "\n",
    "print(\"Optimal Pricing Problem\")\n",
    "print(\"  Demand: Q(p) = 1000 - 20p\")\n",
    "print(\"  Cost: $10 per unit\")\n",
    "print(\"  Profit: Π(p) = p·Q(p) - 10·Q(p)\\n\")\n",
    "\n",
    "print(\"Optimal solution:\")\n",
    "print(f\"  Price: ${optimal_price:.2f}\")\n",
    "print(f\"  Demand: {optimal_demand:.0f} units\")\n",
    "print(f\"  Revenue: ${revenue:.2f}\")\n",
    "print(f\"  Cost: ${cost:.2f}\")\n",
    "print(f\"  Profit: ${max_profit:.2f}\")\n",
    "\n",
    "# Analytical solution: dΠ/dp = 0\n",
    "# Π(p) = p(1000-20p) - 10(1000-20p) = 1000p - 20p² - 10000 + 200p\n",
    "# Π(p) = -20p² + 1200p - 10000\n",
    "# dΠ/dp = -40p + 1200 = 0 → p = 30\n",
    "analytical_price = 30\n",
    "print(f\"\\nAnalytical solution: p* = ${analytical_price}\")\n",
    "print(f\"Error: ${abs(optimal_price - analytical_price):.2e}\")\n",
    "\n",
    "# Visualize\n",
    "prices = np.linspace(10, 50, 300)\n",
    "profits = [profit(p) for p in prices]\n",
    "demands = [1000 - 20*p for p in prices]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Profit curve\n",
    "axes[0].plot(prices, profits, 'b-', linewidth=2, label='Profit Π(p)')\n",
    "axes[0].plot(optimal_price, max_profit, 'ro', markersize=15, \n",
    "             label=f'Optimal: p=${optimal_price:.2f}, Π=${max_profit:.2f}')\n",
    "axes[0].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xlabel('Price ($)', fontsize=13)\n",
    "axes[0].set_ylabel('Profit ($)', fontsize=13)\n",
    "axes[0].set_title('Profit vs Price', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Demand curve\n",
    "axes[1].plot(prices, demands, 'g-', linewidth=2, label='Demand Q(p)')\n",
    "axes[1].plot(optimal_price, optimal_demand, 'ro', markersize=15,\n",
    "             label=f'Optimal: Q={optimal_demand:.0f} at p=${optimal_price:.2f}')\n",
    "axes[1].set_xlabel('Price ($)', fontsize=13)\n",
    "axes[1].set_ylabel('Demand (units)', fontsize=13)\n",
    "axes[1].set_title('Demand vs Price', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOptimal price balances revenue and demand!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33ca821",
   "metadata": {},
   "source": [
    "## Real Example: Machine Learning - Optimal Learning Rate\n",
    "\n",
    "**Problem**: Find optimal learning rate for gradient descent\n",
    "\n",
    "**Model**: Simple quadratic loss\n",
    "\\[ L(\\theta) = (\\theta - 5)^2 \\]\n",
    "\n",
    "**Gradient descent**: \\( \\theta_{t+1} = \\theta_t - \\alpha \\nabla L(\\theta_t) \\)\n",
    "\n",
    "**Goal**: Find learning rate α that minimizes iterations to converge\n",
    "\n",
    "**Consideration**:\n",
    "- Too small α → slow convergence\n",
    "- Too large α → oscillation or divergence\n",
    "- Optimal α → fastest convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1725eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate gradient descent with given learning rate\n",
    "def gradient_descent_iterations(learning_rate, max_iters=1000, tol=1e-6):\n",
    "    \"\"\"Count iterations to converge for given learning rate\"\"\"\n",
    "    theta = 0.0  # Initial parameter\n",
    "    target = 5.0  # True minimum\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        # Gradient of (theta - 5)^2 is 2(theta - 5)\n",
    "        grad = 2 * (theta - target)\n",
    "        theta = theta - learning_rate * grad\n",
    "        \n",
    "        # Check convergence\n",
    "        if abs(theta - target) < tol:\n",
    "            return i + 1\n",
    "    \n",
    "    return max_iters  # Did not converge\n",
    "\n",
    "# Objective: minimize iterations (or maximize negative iterations)\n",
    "def objective(lr):\n",
    "    iters = gradient_descent_iterations(lr)\n",
    "    return iters\n",
    "\n",
    "# Find optimal learning rate\n",
    "result = optimize.minimize_scalar(objective, bounds=(0.01, 1.0), method='bounded')\n",
    "\n",
    "optimal_lr = result.x\n",
    "min_iters = int(result.fun)\n",
    "\n",
    "print(\"Optimal Learning Rate for Gradient Descent\")\n",
    "print(\"  Loss: L(θ) = (θ - 5)²\")\n",
    "print(\"  Update: θ ← θ - α·∇L(θ)\")\n",
    "print(\"  Goal: Minimize iterations to convergence\\n\")\n",
    "\n",
    "print(\"Optimization result:\")\n",
    "print(f\"  Optimal learning rate: α = {optimal_lr:.4f}\")\n",
    "print(f\"  Iterations to converge: {min_iters}\")\n",
    "\n",
    "# Compare with other learning rates\n",
    "test_lrs = [0.1, 0.5, optimal_lr, 0.9]\n",
    "print(\"\\nComparison:\")\n",
    "for lr in test_lrs:\n",
    "    iters = gradient_descent_iterations(lr)\n",
    "    print(f\"  α = {lr:.4f} → {iters} iterations\")\n",
    "\n",
    "# Visualize\n",
    "learning_rates = np.linspace(0.01, 1.0, 100)\n",
    "iterations = [gradient_descent_iterations(lr) for lr in learning_rates]\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(learning_rates, iterations, 'b-', linewidth=2, \n",
    "         label='Iterations to converge')\n",
    "plt.plot(optimal_lr, min_iters, 'ro', markersize=15,\n",
    "         label=f'Optimal: α={optimal_lr:.4f}, iters={min_iters}')\n",
    "plt.xlabel('Learning Rate (α)', fontsize=13)\n",
    "plt.ylabel('Iterations to Converge', fontsize=13)\n",
    "plt.title('Finding Optimal Learning Rate', fontsize=15)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOptimal α = {optimal_lr:.4f} converges fastest!\")\n",
    "print(\"Too small → slow, too large → oscillation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417252ac",
   "metadata": {},
   "source": [
    "## Methods Comparison\n",
    "\n",
    "**scipy.optimize.minimize_scalar()** supports multiple methods:\n",
    "\n",
    "| Method | Description | Use When |\n",
    "|--------|-------------|----------|\n",
    "| **'brent'** | Brent's method (default) | No bounds, smooth function |\n",
    "| **'bounded'** | Bounded Brent | Constraints on x |\n",
    "| **'golden'** | Golden section search | Derivative-free, simple |\n",
    "\n",
    "**Brent's method**:\n",
    "- Combines golden section + parabolic interpolation\n",
    "- Fast convergence (superlinear)\n",
    "- Default choice for most problems\n",
    "\n",
    "**Golden section**:\n",
    "- Uses golden ratio φ = (1+√5)/2 ≈ 1.618\n",
    "- Guaranteed convergence\n",
    "- Slower but more robust\n",
    "\n",
    "**Bounded**:\n",
    "- Respects bounds strictly\n",
    "- Handles boundary minima\n",
    "- Essential for constrained problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be84fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare methods on challenging function\n",
    "def f(x):\n",
    "    return np.sin(x) + np.sin(10*x/3) + np.log(x) - 0.84*x + 3\n",
    "\n",
    "print(\"Method Comparison\")\n",
    "print(\"Function: f(x) = sin(x) + sin(10x/3) + log(x) - 0.84x + 3\")\n",
    "print(\"Search interval: [2.7, 7.5]\\n\")\n",
    "\n",
    "methods = ['brent', 'golden', 'bounded']\n",
    "results = {}\n",
    "\n",
    "for method in methods:\n",
    "    if method == 'bounded':\n",
    "        result = optimize.minimize_scalar(f, bounds=(2.7, 7.5), method=method)\n",
    "    else:\n",
    "        result = optimize.minimize_scalar(f, bracket=(2.7, 7.5), method=method)\n",
    "    \n",
    "    results[method] = result\n",
    "    print(f\"{method.upper()}:\")\n",
    "    print(f\"  Minimum at x = {result.x:.6f}\")\n",
    "    print(f\"  Function value = {result.fun:.6f}\")\n",
    "    print(f\"  Function evaluations: {result.nfev}\")\n",
    "    print()\n",
    "\n",
    "# Visualize\n",
    "x = np.linspace(2.7, 7.5, 500)\n",
    "y = [f(xi) for xi in x]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(x, y, 'b-', linewidth=2, label='f(x)', alpha=0.7)\n",
    "\n",
    "colors = {'brent': 'red', 'golden': 'green', 'bounded': 'orange'}\n",
    "for method, result in results.items():\n",
    "    plt.plot(result.x, result.fun, 'o', color=colors[method], markersize=12,\n",
    "             label=f'{method}: x={result.x:.4f}')\n",
    "\n",
    "plt.xlabel('x', fontsize=13)\n",
    "plt.ylabel('f(x)', fontsize=13)\n",
    "plt.title('Method Comparison: All converge to same minimum', fontsize=15)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"All methods find the same minimum!\")\n",
    "print(\"Brent is fastest (fewest function evaluations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b14f1d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Functions:\n",
    "\n",
    "```python\n",
    "# Main function\n",
    "result = optimize.minimize_scalar(f)\n",
    "\n",
    "# With bounds\n",
    "result = optimize.minimize_scalar(f, bounds=(a, b), method='bounded')\n",
    "\n",
    "# With bracket (initial guess)\n",
    "result = optimize.minimize_scalar(f, bracket=(a, b), method='brent')\n",
    "\n",
    "# Access results\n",
    "result.x       # Optimal point\n",
    "result.fun     # Minimum value\n",
    "result.nfev    # Function evaluations\n",
    "result.success # Convergence flag\n",
    "```\n",
    "\n",
    "### When to Use:\n",
    "\n",
    "✓ **Single variable** optimization  \n",
    "✓ **Line search** in multi-dimensional algorithms  \n",
    "✓ **Hyperparameter tuning** (learning rate, regularization)  \n",
    "✓ **Physics/engineering** problems  \n",
    "✓ **Economics** (pricing, inventory)  \n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Use bounds** when possible → faster, more reliable\n",
    "2. **Brent method** (default) is best for most problems\n",
    "3. **Golden section** for noisy/discontinuous functions\n",
    "4. **Bracket** helps if you know approximate location\n",
    "5. **Maximize f**: minimize -f\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "⚠️ Finds **local** minimum (not global)  \n",
    "⚠️ Requires **unimodal** function in search region  \n",
    "⚠️ For multiple minima → use global optimization  \n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Multivariate**: optimize.minimize() for f(x₁, x₂, ...)\n",
    "- **Constrained**: Subject to g(x) ≤ 0\n",
    "- **Global**: Find global minimum among many local minima"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
