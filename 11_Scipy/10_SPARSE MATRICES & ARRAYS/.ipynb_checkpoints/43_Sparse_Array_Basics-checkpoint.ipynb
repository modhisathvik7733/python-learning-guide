{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["# Sparse Array Basics\n- Sparse matrix formats, Storage efficiency, When to use sparse\n- Real examples: Text data, Network graphs, Large-scale data"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["import numpy as np\nfrom scipy import sparse\nimport matplotlib.pyplot as plt\nprint('Sparse arrays module loaded')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## What are Sparse Arrays?\n\n**Definition**: Arrays where most elements are zero\n\n**Why use sparse?**\n- **Memory**: Store only non-zero elements\n- **Speed**: Skip zero computations\n- **Scale**: Handle massive matrices\n\n**Rule of thumb**: Use sparse when >90% zeros\n\n**Common in**:\n- Text processing (TF-IDF, word counts)\n- Graphs/networks (adjacency matrices)\n- Scientific computing (FEM, CFD)\n- Machine learning (features, embeddings)"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Dense vs sparse comparison\nn = 10000\ndensity = 0.01  # 1% non-zero\n\n# Dense array\nnp.random.seed(42)\ndata = np.random.rand(n, n)\ndata[data > density] = 0  # Make sparse\n\nprint(f'Array: {n}×{n}')\nprint(f'Density: {density*100}%\\n')\n\n# Memory comparison\ndense_size = data.nbytes / (1024**2)  # MB\nsparse_csr = sparse.csr_array(data)\nsparse_size = (sparse_csr.data.nbytes + sparse_csr.indices.nbytes + \n               sparse_csr.indptr.nbytes) / (1024**2)\n\nprint(f'Dense array:')\nprint(f'  Memory: {dense_size:.2f} MB')\nprint(f'  Non-zeros: {np.count_nonzero(data):,}\\n')\n\nprint(f'Sparse array (CSR):')\nprint(f'  Memory: {sparse_size:.2f} MB')\nprint(f'  Compression: {dense_size/sparse_size:.1f}×')\nprint(f'  Memory saved: {(1-sparse_size/dense_size)*100:.1f}%')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Sparse Matrix Formats\n\n**scipy.sparse** supports multiple formats:\n\n1. **CSR (Compressed Sparse Row)**: Fast row slicing, arithmetic\n2. **CSC (Compressed Sparse Column)**: Fast column slicing\n3. **COO (Coordinate)**: Fast construction, easy to build\n4. **DOK (Dictionary of Keys)**: Incremental construction\n5. **LIL (List of Lists)**: Flexible construction\n6. **DIA (Diagonal)**: Diagonal matrices\n7. **BSR (Block Sparse Row)**: Dense sub-blocks\n\n**Most used**: CSR (general), CSC (column ops), COO (construction)"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Create small sparse matrix\ndata_dense = np.array([\n    [1, 0, 0, 2],\n    [0, 3, 0, 0],\n    [0, 0, 0, 0],\n    [4, 0, 5, 6]\n], dtype=float)\n\nprint('Dense matrix:')\nprint(data_dense)\nprint(f'\\nNon-zeros: {np.count_nonzero(data_dense)}/{data_dense.size}')\nprint(f'Sparsity: {(1 - np.count_nonzero(data_dense)/data_dense.size)*100:.1f}%\\n')\n\n# Convert to different formats\nformats = ['csr', 'csc', 'coo', 'lil', 'dok']\n\nfor fmt in formats:\n    if fmt == 'csr':\n        sp = sparse.csr_array(data_dense)\n    elif fmt == 'csc':\n        sp = sparse.csc_array(data_dense)\n    elif fmt == 'coo':\n        sp = sparse.coo_array(data_dense)\n    elif fmt == 'lil':\n        sp = sparse.lil_array(data_dense)\n    elif fmt == 'dok':\n        sp = sparse.dok_array(data_dense)\n    \n    print(f'{fmt.upper()}: {type(sp).__name__}')\n    print(f'  Shape: {sp.shape}')\n    print(f'  Non-zeros: {sp.nnz}')\n    print(f'  Data type: {sp.dtype}')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## CSR Format Deep Dive\n\n**CSR (Compressed Sparse Row)**: Most common format\n\n**Storage**: Three arrays\n- **data**: Non-zero values\n- **indices**: Column indices\n- **indptr**: Row pointers\n\n**Row i spans**: indices[indptr[i]:indptr[i+1]]\n\n**Advantages**:\n- Fast row access\n- Fast matrix-vector multiplication\n- Efficient arithmetic operations"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# CSR internals\ncsr = sparse.csr_array(data_dense)\n\nprint('CSR internal structure:\\n')\nprint(f'data (values):    {csr.data}')\nprint(f'indices (cols):   {csr.indices}')\nprint(f'indptr (rows):    {csr.indptr}\\n')\n\nprint('Decoding:')\nfor i in range(csr.shape[0]):\n    start, end = csr.indptr[i], csr.indptr[i+1]\n    if start < end:\n        values = csr.data[start:end]\n        cols = csr.indices[start:end]\n        print(f'  Row {i}: {list(zip(cols, values))}')\n    else:\n        print(f'  Row {i}: empty')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Real Example: Text Document Matrix\n**Problem**: Represent documents as word count vectors\n**Why sparse**: Most words don't appear in most documents\n\n**TF-IDF, bag-of-words**: Naturally sparse"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Simulate document-term matrix\nn_docs = 1000\nn_words = 10000\navg_words_per_doc = 100  # Sparsity ~99%\n\nprint('Text Document Matrix')\nprint(f'  Documents: {n_docs:,}')\nprint(f'  Vocabulary: {n_words:,}')\nprint(f'  Avg words/doc: {avg_words_per_doc}\\n')\n\n# Build sparse matrix (DOK for construction)\nnp.random.seed(42)\ndoc_term = sparse.dok_array((n_docs, n_words), dtype=np.int32)\n\nfor doc_id in range(n_docs):\n    # Random words in this document\n    n_words_doc = np.random.poisson(avg_words_per_doc)\n    word_ids = np.random.choice(n_words, size=min(n_words_doc, n_words), replace=False)\n    counts = np.random.randint(1, 10, size=len(word_ids))\n    \n    for word_id, count in zip(word_ids, counts):\n        doc_term[doc_id, word_id] = count\n\n# Convert to CSR for operations\ndoc_term_csr = doc_term.tocsr()\n\nprint(f'Matrix stats:')\nprint(f'  Shape: {doc_term_csr.shape}')\nprint(f'  Non-zeros: {doc_term_csr.nnz:,}')\nprint(f'  Density: {doc_term_csr.nnz / (n_docs * n_words) * 100:.3f}%\\n')\n\n# Memory comparison\ndense_mem = n_docs * n_words * 4 / (1024**2)  # int32 = 4 bytes\nsparse_mem = (doc_term_csr.data.nbytes + doc_term_csr.indices.nbytes + \n              doc_term_csr.indptr.nbytes) / (1024**2)\n\nprint(f'Memory usage:')\nprint(f'  Dense would be: {dense_mem:.1f} MB')\nprint(f'  Sparse (CSR): {sparse_mem:.1f} MB')\nprint(f'  Savings: {(1 - sparse_mem/dense_mem)*100:.1f}%')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Real Example: Social Network Graph\n**Problem**: Store friendships in social network\n**Adjacency matrix**: A[i,j]=1 if i and j are friends\n\n**Why sparse**: Most people not friends with most others"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Social network\nn_users = 10000\navg_friends = 50  # Average friends per person\n\nprint('Social Network Graph')\nprint(f'  Users: {n_users:,}')\nprint(f'  Avg friends/user: {avg_friends}\\n')\n\n# Build adjacency matrix (symmetric)\nnp.random.seed(42)\n# Use LIL for construction\nadj = sparse.lil_array((n_users, n_users), dtype=np.int8)\n\nfor user in range(n_users):\n    n_friends = np.random.poisson(avg_friends)\n    friends = np.random.choice(n_users, size=min(n_friends, n_users-1), replace=False)\n    friends = friends[friends != user]  # Remove self\n    \n    for friend in friends:\n        adj[user, friend] = 1\n        adj[friend, user] = 1  # Symmetric\n\n# Convert to CSR\nadj_csr = adj.tocsr()\n\nprint(f'Adjacency matrix:')\nprint(f'  Shape: {adj_csr.shape}')\nprint(f'  Edges: {adj_csr.nnz // 2:,}')  # Divide by 2 for undirected\nprint(f'  Density: {adj_csr.nnz / (n_users**2) * 100:.3f}%\\n')\n\n# Find user with most friends\nfriend_counts = np.array(adj_csr.sum(axis=1)).flatten()\nmax_friends_user = friend_counts.argmax()\n\nprint(f'User statistics:')\nprint(f'  Most connected: User {max_friends_user} ({friend_counts[max_friends_user]} friends)')\nprint(f'  Avg friends: {friend_counts.mean():.1f}')\nprint(f'  Median friends: {np.median(friend_counts):.0f}')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Summary\n\n### When to Use Sparse:\n✓ **>90% zeros**: Large memory/speed gains  \n✓ **Large scale**: Can't fit dense in memory  \n✓ **Natural sparsity**: Text, graphs, sensor data  \n\n### Format Selection:\n\n| Format | Construction | Row ops | Col ops | Arithmetic | Best for |\n|--------|-------------|---------|---------|------------|----------|\n| **CSR** | Slow | Fast | Slow | Fast | General, matrix-vector |\n| **CSC** | Slow | Slow | Fast | Fast | Column slicing |\n| **COO** | Fast | Slow | Slow | Medium | Construction, I/O |\n| **LIL** | Fast | Fast | Slow | Slow | Incremental build |\n| **DOK** | Fast | Medium | Medium | Slow | Random access |\n\n### Best Practices:\n✓ **Build in COO/LIL/DOK**: Fast incremental construction  \n✓ **Convert to CSR/CSC**: For computation  \n✓ **Check density**: `nnz / (rows * cols)`  \n✓ **Use appropriate format**: Match access pattern  \n\n### Common Applications:\n- **NLP**: TF-IDF, word embeddings, co-occurrence\n- **Graphs**: Social networks, web graphs, molecules\n- **Images**: Feature descriptors, SIFT, HOG\n- **Finance**: Correlation matrices, time series\n- **Physics**: FEM meshes, Laplacian matrices\n- **ML**: Feature matrices, kernel matrices"]
    }
  ],
  "metadata": {
    "kernelspec": {"display_name": "Python 3", "name": "python3"},
    "language_info": {"version": "3.8.0"}
  },
  "nbformat": 4,
  "nbformat_minor": 5
}