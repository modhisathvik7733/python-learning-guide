{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e80bacd",
   "metadata": {},
   "source": [
    "# Sparse Advanced Operations\n",
    "- Graph algorithms, Sparse eigensolvers, Advanced decompositions\n",
    "- Real examples: PageRank, Network analysis, Large-scale PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98010163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse advanced operations module loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse import linalg as splinalg\n",
    "from scipy.sparse import csgraph\n",
    "print('Sparse advanced operations module loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92165448",
   "metadata": {},
   "source": [
    "## Sparse Eigenvalue Problems\n",
    "\n",
    "**Challenge**: Find eigenvalues of huge matrices\n",
    "**Solution**: Iterative methods (Arnoldi, Lanczos)\n",
    "**Functions**: `eigs`, `eigsh` (symmetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff374b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse Eigenvalue Solvers\n",
      "\n",
      "Matrix size: 1000×1000\n",
      "Non-zeros: 2,998\n",
      "Density: 0.2998%\n",
      "\n",
      "Computed 6 largest eigenvalues:\n",
      "  λ_1 = 3.999990\n",
      "  λ_2 = 3.999961\n",
      "  λ_3 = 3.999911\n",
      "  λ_4 = 3.999842\n",
      "  λ_5 = 3.999754\n",
      "  λ_6 = 3.999645\n",
      "\n",
      "Eigenvector shape: (1000, 6)\n",
      "Method: Lanczos iteration (much faster than dense methods)\n"
     ]
    }
   ],
   "source": [
    "print('Sparse Eigenvalue Solvers\\n')\n",
    "\n",
    "# Create large sparse matrix\n",
    "n = 1000\n",
    "diag = np.ones(n) * 2\n",
    "off_diag = -np.ones(n-1)\n",
    "A = sparse.diags([off_diag, diag, off_diag], [-1, 0, 1], format='csr')\n",
    "\n",
    "print(f'Matrix size: {n}×{n}')\n",
    "print(f'Non-zeros: {A.nnz:,}')\n",
    "print(f'Density: {A.nnz/(n*n)*100:.4f}%\\n')\n",
    "\n",
    "# Find 6 largest eigenvalues\n",
    "k = 6\n",
    "eigenvalues, eigenvectors = splinalg.eigsh(A, k=k, which='LA')\n",
    "\n",
    "print(f'Computed {k} largest eigenvalues:')\n",
    "for i, lam in enumerate(eigenvalues[::-1]):\n",
    "    print(f'  λ_{i+1} = {lam:.6f}')\n",
    "\n",
    "print(f'\\nEigenvector shape: {eigenvectors.shape}')\n",
    "print('Method: Lanczos iteration (much faster than dense methods)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c666a8",
   "metadata": {},
   "source": [
    "## Graph Algorithms\n",
    "\n",
    "**scipy.sparse.csgraph**: Compressed sparse graph algorithms\n",
    "**Algorithms**: Shortest paths, MST, connected components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffb22b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph Algorithms\n",
      "\n",
      "Graph: 5 nodes, 7 edges\n",
      "\n",
      "Shortest paths (Dijkstra):\n",
      "  From node 0 to 0: 0\n",
      "  From node 0 to 1: 3\n",
      "  From node 0 to 2: 2\n",
      "  From node 0 to 3: 8\n",
      "  From node 0 to 4: 10\n"
     ]
    }
   ],
   "source": [
    "print('\\nGraph Algorithms\\n')\n",
    "\n",
    "# Create graph (adjacency matrix)\n",
    "edges = np.array([[0,1,4], [0,2,2], [1,2,1], [1,3,5], \n",
    "                  [2,3,8], [2,4,10], [3,4,2]])\n",
    "n_nodes = 5\n",
    "\n",
    "# Build adjacency matrix\n",
    "G = sparse.lil_matrix((n_nodes, n_nodes))\n",
    "for i, j, w in edges:\n",
    "    G[i,j] = w\n",
    "    G[j,i] = w  # Undirected\n",
    "G = G.tocsr()\n",
    "\n",
    "print(f'Graph: {n_nodes} nodes, {len(edges)} edges\\n')\n",
    "\n",
    "# Shortest paths (Dijkstra)\n",
    "dist_matrix = csgraph.shortest_path(G, method='D')\n",
    "\n",
    "print('Shortest paths (Dijkstra):')\n",
    "for i in range(n_nodes):\n",
    "    print(f'  From node 0 to {i}: {dist_matrix[0,i]:.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd13cd1f",
   "metadata": {},
   "source": [
    "## Real Example: PageRank Algorithm\n",
    "\n",
    "**Problem**: Rank web pages by importance\n",
    "**Method**: Dominant eigenvector of transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e006eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PageRank Algorithm\n",
      "\n",
      "Web graph: 10 pages\n",
      "Links: 15\n",
      "\n",
      "PageRank scores:\n",
      "  1. Page 9: 0.1356\n",
      "  2. Page 8: 0.1336\n",
      "  3. Page 0: 0.1324\n",
      "  4. Page 4: 0.1288\n",
      "  5. Page 5: 0.1249\n",
      "\n",
      "Higher score = more important page\n"
     ]
    }
   ],
   "source": [
    "print('\\nPageRank Algorithm\\n')\n",
    "\n",
    "# Create web graph (link structure)\n",
    "n_pages = 10\n",
    "np.random.seed(42)\n",
    "\n",
    "# Random links (sparse)\n",
    "links = sparse.random(n_pages, n_pages, density=0.2, format='lil')\n",
    "links.setdiag(0)  # No self-links\n",
    "\n",
    "print(f'Web graph: {n_pages} pages')\n",
    "print(f'Links: {links.nnz}\\n')\n",
    "\n",
    "# Normalize columns (transition probabilities)\n",
    "col_sums = np.array(links.sum(axis=0)).flatten()\n",
    "col_sums[col_sums == 0] = 1  # Handle dangling nodes\n",
    "P = links.tocsc()\n",
    "for i in range(n_pages):\n",
    "    if col_sums[i] > 0:\n",
    "        P.data[P.indptr[i]:P.indptr[i+1]] /= col_sums[i]\n",
    "\n",
    "# PageRank with damping\n",
    "damping = 0.85\n",
    "M = damping * P + (1-damping)/n_pages * sparse.csr_matrix(np.ones((n_pages, n_pages)))\n",
    "\n",
    "# Find dominant eigenvector\n",
    "eigenvalues, eigenvectors = splinalg.eigs(M.T, k=1, which='LM')\n",
    "pagerank = np.abs(eigenvectors.flatten().real)\n",
    "pagerank = pagerank / pagerank.sum()\n",
    "\n",
    "print('PageRank scores:')\n",
    "ranked = sorted(enumerate(pagerank), key=lambda x: x[1], reverse=True)\n",
    "for i, (page, score) in enumerate(ranked[:5]):\n",
    "    print(f'  {i+1}. Page {page}: {score:.4f}')\n",
    "\n",
    "print('\\nHigher score = more important page')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8715cb4d",
   "metadata": {},
   "source": [
    "## Connected Components\n",
    "\n",
    "**Problem**: Find separate subgraphs\n",
    "**Use**: Network analysis, clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cccf0ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connected Components\n",
      "\n",
      "Number of components: 3\n",
      "\n",
      "Component 1: nodes [np.int64(0), np.int64(1), np.int64(2)]\n",
      "Component 2: nodes [np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n",
      "Component 3: nodes [np.int64(7), np.int64(8), np.int64(9)]\n",
      "\n",
      "Applications: Community detection, network partitioning\n"
     ]
    }
   ],
   "source": [
    "print('\\nConnected Components\\n')\n",
    "\n",
    "# Create graph with 3 components\n",
    "G = sparse.lil_matrix((10, 10))\n",
    "# Component 1\n",
    "G[0,1] = G[1,0] = 1\n",
    "G[1,2] = G[2,1] = 1\n",
    "# Component 2\n",
    "G[3,4] = G[4,3] = 1\n",
    "G[4,5] = G[5,4] = 1\n",
    "G[5,6] = G[6,5] = 1\n",
    "# Component 3\n",
    "G[7,8] = G[8,7] = 1\n",
    "G[8,9] = G[9,8] = 1\n",
    "\n",
    "G = G.tocsr()\n",
    "\n",
    "n_components, labels = csgraph.connected_components(G, directed=False)\n",
    "\n",
    "print(f'Number of components: {n_components}\\n')\n",
    "for comp in range(n_components):\n",
    "    nodes = np.where(labels == comp)[0]\n",
    "    print(f'Component {comp+1}: nodes {list(nodes)}')\n",
    "\n",
    "print('\\nApplications: Community detection, network partitioning')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eba94d",
   "metadata": {},
   "source": [
    "## Sparse SVD\n",
    "\n",
    "**Purpose**: Dimensionality reduction for large sparse matrices\n",
    "**Use**: Text mining, recommender systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf7969d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparse SVD\n",
      "\n",
      "Term-document matrix: 1000×500\n",
      "Non-zeros: 5,000\n",
      "Density: 1.00%\n",
      "\n",
      "Computed 10 singular values:\n",
      "  σ_1 = 4.3299\n",
      "  σ_2 = 3.4724\n",
      "  σ_3 = 3.3650\n",
      "  σ_4 = 3.3244\n",
      "  σ_5 = 3.3153\n",
      "  σ_6 = 3.2840\n",
      "  σ_7 = 3.2600\n",
      "  σ_8 = 3.2226\n",
      "  σ_9 = 3.1964\n",
      "  σ_10 = 3.1857\n",
      "\n",
      "Reduced representation: 500×10\n",
      "Applications: Latent Semantic Analysis, topic modeling\n"
     ]
    }
   ],
   "source": [
    "print('\\nSparse SVD\\n')\n",
    "\n",
    "# Term-document matrix (sparse)\n",
    "n_terms = 1000\n",
    "n_docs = 500\n",
    "tf_idf = sparse.random(n_terms, n_docs, density=0.01, format='csr')\n",
    "\n",
    "print(f'Term-document matrix: {n_terms}×{n_docs}')\n",
    "print(f'Non-zeros: {tf_idf.nnz:,}')\n",
    "print(f'Density: {tf_idf.nnz/(n_terms*n_docs)*100:.2f}%\\n')\n",
    "\n",
    "# Compute top k singular values\n",
    "k = 10\n",
    "u, s, vt = splinalg.svds(tf_idf, k=k)\n",
    "\n",
    "print(f'Computed {k} singular values:')\n",
    "for i, sv in enumerate(s[::-1]):\n",
    "    print(f'  σ_{i+1} = {sv:.4f}')\n",
    "\n",
    "print(f'\\nReduced representation: {n_docs}×{k}')\n",
    "print('Applications: Latent Semantic Analysis, topic modeling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e0839a",
   "metadata": {},
   "source": [
    "## Real Example: Social Network Analysis\n",
    "\n",
    "**Problem**: Find influential users in social network\n",
    "**Method**: Centrality measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c80cfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Social Network Analysis\n",
      "\n",
      "Social network: 100 users\n",
      "Connections: 489\n",
      "\n",
      "Top 5 most connected users:\n",
      "  1. User 3: 9 connections\n",
      "      (followers: 4, following: 4)\n",
      "  2. User 0: 9 connections\n",
      "      (followers: 5, following: 3)\n",
      "  3. User 30: 9 connections\n",
      "      (followers: 4, following: 4)\n",
      "  4. User 7: 8 connections\n",
      "      (followers: 5, following: 2)\n",
      "  5. User 94: 8 connections\n",
      "      (followers: 3, following: 4)\n",
      "\n",
      "Metrics computed efficiently on sparse graph!\n"
     ]
    }
   ],
   "source": [
    "print('\\nSocial Network Analysis\\n')\n",
    "\n",
    "np.random.seed(42)\n",
    "n_users = 100\n",
    "\n",
    "# Random social network\n",
    "follows = sparse.random(n_users, n_users, density=0.05, format='lil')\n",
    "follows.setdiag(0)\n",
    "follows = follows.tocsr()\n",
    "\n",
    "print(f'Social network: {n_users} users')\n",
    "print(f'Connections: {follows.nnz}\\n')\n",
    "\n",
    "# Degree centrality (number of connections)\n",
    "in_degree = np.array(follows.sum(axis=0)).flatten()\n",
    "out_degree = np.array(follows.sum(axis=1)).flatten()\n",
    "total_degree = in_degree + out_degree\n",
    "\n",
    "top_users = np.argsort(total_degree)[::-1][:5]\n",
    "\n",
    "print('Top 5 most connected users:')\n",
    "for i, user in enumerate(top_users):\n",
    "    print(f'  {i+1}. User {user}: {int(total_degree[user])} connections')\n",
    "    print(f'      (followers: {int(in_degree[user])}, following: {int(out_degree[user])})')\n",
    "\n",
    "print('\\nMetrics computed efficiently on sparse graph!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a350eb",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Sparse Eigensolvers:\n",
    "```python\n",
    "# Symmetric\n",
    "eigenvalues, eigenvectors = splinalg.eigsh(A, k=10, which='LA')\n",
    "\n",
    "# General\n",
    "eigenvalues, eigenvectors = splinalg.eigs(A, k=10, which='LM')\n",
    "```\n",
    "\n",
    "### Graph Algorithms:\n",
    "```python\n",
    "# Shortest paths\n",
    "dist = csgraph.shortest_path(G, method='D')\n",
    "\n",
    "# Connected components\n",
    "n_comp, labels = csgraph.connected_components(G)\n",
    "\n",
    "# Minimum spanning tree\n",
    "mst = csgraph.minimum_spanning_tree(G)\n",
    "```\n",
    "\n",
    "### Sparse SVD:\n",
    "```python\n",
    "u, s, vt = splinalg.svds(A, k=10)\n",
    "```\n",
    "\n",
    "### Applications:\n",
    "- **PageRank**: Web page ranking\n",
    "- **Social networks**: Influence analysis\n",
    "- **Recommender systems**: Collaborative filtering\n",
    "- **Text mining**: Topic modeling (LSA)\n",
    "- **Network optimization**: Shortest paths, MST"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
