{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["# Numerical Method Selection Guide\n- Algorithm selection criteria, Performance comparison, Best practices\n- Real examples: Decision trees, Benchmarking"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["import numpy as np\nfrom scipy import optimize, integrate, linalg, sparse\nimport time\nprint('Method selection guide loaded')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Integration Method Selection\n\n**Decision factors**:\n- Smoothness of integrand\n- Accuracy requirements\n- Oscillatory behavior\n- Infinite limits"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["print('Integration Method Selection\\n')\n\nprint('METHOD SELECTION GUIDE:\\n')\nprint('1. QUAD (adaptive quadrature)')\nprint('   âœ“ General purpose, smooth functions')\nprint('   âœ“ Automatic error control')\nprint('   âœ“ First choice for most problems\\n')\n\nprint('2. ROMBERG (Richardson extrapolation)')\nprint('   âœ“ Very smooth functions')\nprint('   âœ“ Higher accuracy')\nprint('   âœ“ Slower than quad\\n')\n\nprint('3. FIXED_QUAD (Gaussian quadrature)')\nprint('   âœ“ Polynomial-like functions')\nprint('   âœ“ Fixed number of points')\nprint('   âœ“ Fast when function evaluations are cheap\\n')\n\nprint('4. SIMPSON/TRAPZ (composite rules)')\nprint('   âœ“ Data points given (not function)')\nprint('   âœ“ Simple, fast')\nprint('   âœ“ Lower accuracy\\n')\n\nprint('5. QUADRATURE (Gaussian, fixed order)')\nprint('   âœ“ Known weights and nodes')\nprint('   âœ“ Fastest for specific cases')"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["print('\\n\\nPERFORMANCE COMPARISON\\n')\n\ndef f(x):\n    return np.exp(-x**2)\n\nmethods = [\n    ('quad', lambda: integrate.quad(f, 0, 2)),\n    ('romberg', lambda: integrate.romberg(f, 0, 2)),\n    ('fixed_quad', lambda: integrate.fixed_quad(f, 0, 2, n=5))\n]\n\nfor name, method in methods:\n    start = time.time()\n    result, *_ = method()\n    elapsed = (time.time() - start) * 1e6\n    \n    print(f'{name:12s}: {result:.10f}  ({elapsed:.1f} Âµs)')\n\nprint('\\nAll methods agree to high precision!')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Optimization Method Selection\n\n**Decision tree**:\n1. Derivatives available? â†’ Use gradient methods\n2. Constraints? â†’ Choose appropriate solver\n3. Global minimum? â†’ Use global methods"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["print('\\nOptimization Method Selection\\n')\n\nprint('DECISION TREE:\\n')\nprint('â”Œâ”€ Do you have derivatives?')\nprint('â”‚')\nprint('â”œâ”€ YES â†’ Use gradient-based methods')\nprint('â”‚   â”œâ”€ Unconstrained: BFGS, L-BFGS-B')\nprint('â”‚   â”œâ”€ Box constraints: L-BFGS-B')\nprint('â”‚   â””â”€ General constraints: SLSQP, trust-constr')\nprint('â”‚')\nprint('â””â”€ NO â†’ Use derivative-free methods')\nprint('    â”œâ”€ Smooth function: Nelder-Mead, Powell')\nprint('    â”œâ”€ Noisy function: Nelder-Mead')\nprint('    â””â”€ Global search: differential_evolution\\n')\n\nprint('SPECIAL CASES:\\n')\nprint('â€¢ Least squares â†’ leastsq, least_squares')\nprint('â€¢ Linear programming â†’ linprog')\nprint('â€¢ Quadratic programming â†’ quadratic_assignment')\nprint('â€¢ Root finding â†’ root, fsolve')\nprint('â€¢ Global optimization â†’ differential_evolution, basinhopping')"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["print('\\n\\nMETHOD COMPARISON EXAMPLE\\n')\n\ndef rosenbrock(x):\n    return sum(100.0*(x[1:]-x[:-1]**2)**2 + (1-x[:-1])**2)\n\nx0 = np.array([0, 0])\n\nmethods = ['Nelder-Mead', 'Powell', 'BFGS', 'L-BFGS-B']\n\nprint('Minimizing Rosenbrock function:\\n')\nfor method in methods:\n    start = time.time()\n    result = optimize.minimize(rosenbrock, x0, method=method)\n    elapsed = (time.time() - start) * 1000\n    \n    print(f'{method:12s}: f={result.fun:.2e}, nfev={result.nfev:3d}, time={elapsed:.2f}ms')\n\nprint('\\nBFGS fastest for smooth functions with gradients')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Linear Algebra Method Selection\n\n**Key factors**:\n- Matrix properties (symmetric, positive definite, sparse)\n- Size\n- Condition number"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["print('\\nLinear Algebra Method Selection\\n')\n\nprint('MATRIX SOLUTION (Ax = b):\\n')\nprint('1. DENSE MATRICES')\nprint('   â”œâ”€ General: linalg.solve(A, b)')\nprint('   â”œâ”€ Symmetric: linalg.solve(A, b, assume_a=\\'pos\\')')\nprint('   â”œâ”€ Triangular: linalg.solve_triangular(A, b)')\nprint('   â””â”€ Banded: linalg.solve_banded(...)\\n')\n\nprint('2. SPARSE MATRICES')\nprint('   â”œâ”€ Direct: sparse.linalg.spsolve(A, b)')\nprint('   â”œâ”€ Iterative (large): sparse.linalg.cg(A, b)')\nprint('   â”œâ”€ Iterative (general): sparse.linalg.gmres(A, b)')\nprint('   â””â”€ Symmetric: sparse.linalg.minres(A, b)\\n')\n\nprint('EIGENVALUE PROBLEMS:\\n')\nprint('Dense:')\nprint('  â€¢ All: linalg.eig(A)')\nprint('  â€¢ Symmetric: linalg.eigh(A) [faster]')\nprint('  â€¢ Generalized: linalg.eig(A, B)\\n')\nprint('Sparse:')\nprint('  â€¢ Few eigenvalues: sparse.linalg.eigs(A, k=10)')\nprint('  â€¢ Symmetric: sparse.linalg.eigsh(A, k=10) [faster]')"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["print('\\n\\nPERFORMANCE: Dense vs Sparse\\n')\n\nn = 1000\n\n# Dense matrix\nA_dense = np.random.rand(n, n)\nb_dense = np.random.rand(n)\n\nstart = time.time()\nx_dense = linalg.solve(A_dense, b_dense)\ntime_dense = (time.time() - start) * 1000\n\n# Sparse matrix (tridiagonal)\ndiag = np.ones(n) * 2\noff_diag = -np.ones(n-1)\nA_sparse = sparse.diags([off_diag, diag, off_diag], [-1, 0, 1], format='csr')\nb_sparse = np.random.rand(n)\n\nstart = time.time()\nx_sparse = sparse.linalg.spsolve(A_sparse, b_sparse)\ntime_sparse = (time.time() - start) * 1000\n\nprint(f'Matrix size: {n}Ã—{n}\\n')\nprint(f'Dense solver: {time_dense:.2f} ms')\nprint(f'Sparse solver: {time_sparse:.2f} ms')\nprint(f'\\nSpeedup: {time_dense/time_sparse:.1f}x faster for sparse!')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Statistical Test Selection\n\n**Decision factors**:\n- Data distribution (normal vs non-normal)\n- Sample size\n- Number of groups\n- Paired vs independent"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["print('\\nStatistical Test Selection\\n')\n\nprint('COMPARING TWO GROUPS:\\n')\nprint('â”Œâ”€ Are data normally distributed?')\nprint('â”‚')\nprint('â”œâ”€ YES (normal) â†’')\nprint('â”‚   â”œâ”€ Independent samples: t-test (ttest_ind)')\nprint('â”‚   â””â”€ Paired samples: paired t-test (ttest_rel)')\nprint('â”‚')\nprint('â””â”€ NO (non-normal) â†’')\nprint('    â”œâ”€ Independent: Mann-Whitney U (mannwhitneyu)')\nprint('    â””â”€ Paired: Wilcoxon signed-rank (wilcoxon)\\n')\n\nprint('COMPARING 3+ GROUPS:\\n')\nprint('â”Œâ”€ Normal data?')\nprint('â”‚')\nprint('â”œâ”€ YES â†’ One-way ANOVA (f_oneway)')\nprint('â”‚   â””â”€ Post-hoc: Tukey HSD')\nprint('â”‚')\nprint('â””â”€ NO â†’ Kruskal-Wallis H (kruskal)')\nprint('    â””â”€ Post-hoc: Dunn test\\n')\n\nprint('CATEGORICAL DATA:\\n')\nprint('â€¢ Contingency table: Chi-square (chi2_contingency)')\nprint('â€¢ Goodness of fit: Chi-square (chisquare)')\nprint('â€¢ Proportions: Z-test (proportions_ztest)')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Best Practices Summary\n\n**General principles**:\n1. Start simple, increase complexity if needed\n2. Leverage problem structure (symmetry, sparsity)\n3. Profile before optimizing\n4. Use analytical derivatives when possible\n5. Validate results"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["print('\\nNumerical Computing Best Practices\\n')\n\nprint('1. NUMERICAL STABILITY')\nprint('   âœ“ Avoid subtracting nearly equal numbers')\nprint('   âœ“ Scale variables to similar magnitudes')\nprint('   âœ“ Use stable algorithms (QR vs normal equations)\\n')\n\nprint('2. PERFORMANCE')\nprint('   âœ“ Vectorize operations (avoid loops)')\nprint('   âœ“ Use appropriate data structures (sparse for sparse)')\nprint('   âœ“ Profile before optimizing')\nprint('   âœ“ Consider memory vs speed tradeoffs\\n')\n\nprint('3. ACCURACY')\nprint('   âœ“ Check condition numbers')\nprint('   âœ“ Use appropriate tolerances')\nprint('   âœ“ Verify convergence')\nprint('   âœ“ Compare multiple methods\\n')\n\nprint('4. ROBUSTNESS')\nprint('   âœ“ Handle edge cases')\nprint('   âœ“ Validate inputs')\nprint('   âœ“ Check for convergence failures')\nprint('   âœ“ Provide informative error messages\\n')\n\nprint('5. REPRODUCIBILITY')\nprint('   âœ“ Set random seeds')\nprint('   âœ“ Document algorithm choices')\nprint('   âœ“ Record scipy version')\nprint('   âœ“ Save configuration parameters')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Quick Reference Cards\n\n**Integration**: quad â†’ smooth, romberg â†’ very smooth, simpson â†’ data points\n\n**Optimization**: BFGS â†’ smooth+gradient, Nelder-Mead â†’ no gradient, DE â†’ global\n\n**Linear Algebra**: solve â†’ dense, spsolve â†’ sparse, eigh â†’ symmetric eigenvalues\n\n**Statistics**: t-test â†’ normal, Mann-Whitney â†’ non-normal, chi-square â†’ categorical\n\n**Signal**: butter â†’ general filter, savgol â†’ preserve features, welch â†’ PSD\n\n**Sparse**: csr â†’ arithmetic, lil â†’ construction, coo â†’ creation"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["print('\\nCONGRATULATIONS!\\n')\nprint('='*60)\nprint('You have completed the SciPy Mastery Series!')\nprint('='*60)\nprint()\nprint('You now have expertise in:')\nprint('  âœ“ Numerical integration and differentiation')\nprint('  âœ“ Optimization and root finding')\nprint('  âœ“ Linear algebra and sparse matrices')\nprint('  âœ“ Statistical analysis and hypothesis testing')\nprint('  âœ“ Signal and image processing')\nprint('  âœ“ Clustering and spatial algorithms')\nprint('  âœ“ Special functions')\nprint('  âœ“ File I/O and data formats')\nprint('  âœ“ Advanced methods (ODR, QMC, etc.)')\nprint()\nprint('Next steps:')\nprint('  1. Apply to your own problems')\nprint('  2. Explore scipy documentation')\nprint('  3. Read research papers on algorithms')\nprint('  4. Contribute to open source!')\nprint()\nprint('='*60)\nprint('Happy Scientific Computing! ðŸš€')\nprint('='*60)"]
    }
  ],
  "metadata": {
    "kernelspec": {"display_name": "Python 3", "name": "python3"},
    "language_info": {"version": "3.8.0"}
  },
  "nbformat": 4,
  "nbformat_minor": 5
}