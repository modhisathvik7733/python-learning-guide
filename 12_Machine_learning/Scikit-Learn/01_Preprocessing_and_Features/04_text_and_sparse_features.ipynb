{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Text and Sparse Features in Scikit-learn\n",
    "\n",
    "## Overview\n",
    "\n",
    "Text data cannot be directly used by ML algorithms - it must be converted to numerical features. Sklearn provides powerful tools for text vectorization:\n",
    "\n",
    "- **CountVectorizer**: Converts text to word count matrix\n",
    "- **TfidfVectorizer**: Converts text to TF-IDF weighted features\n",
    "- **HashingVectorizer**: Fast, memory-efficient hashing approach\n",
    "\n",
    "These create **sparse matrices** - efficient representations where most values are zero.\n",
    "\n",
    "## Why Sparse Matrices?\n",
    "\n",
    "Text data typically has:\n",
    "- **Large vocabulary**: Thousands of unique words\n",
    "- **Sparse representation**: Most documents use only a small subset of vocabulary\n",
    "- **Memory efficiency**: Sparse matrices store only non-zero values\n",
    "\n",
    "Example: \"The cat sat\" vs 10,000 word vocabulary = 99.97% zeros!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup and Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Corpus:\n",
      "1. This is the first document.\n",
      "2. This document is the second document.\n",
      "3. And this is the third one.\n",
      "4. Is this the first document?\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample text corpus\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "print(\"Sample Corpus:\")\n",
    "for i, doc in enumerate(corpus, 1):\n",
    "    print(f\"{i}. {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "countvectorizer",
   "metadata": {},
   "source": [
    "## 1. CountVectorizer - Bag of Words\n",
    "\n",
    "Converts text to a matrix of token counts (word frequency).\n",
    "\n",
    "**Key Parameters:**\n",
    "- `max_features`: Limit vocabulary size to top N words\n",
    "- `min_df`: Ignore words appearing in fewer than N documents\n",
    "- `max_df`: Ignore words appearing in more than N% of documents\n",
    "- `ngram_range`: Include n-grams (1,1)=unigrams, (1,2)=unigrams+bigrams\n",
    "- `stop_words`: Remove common words ('english' or custom list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "count-basic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer Results:\n",
      "Shape: (4, 9) (documents x vocabulary)\n",
      "Vocabulary size: 9\n",
      "Sparse matrix density: 58.33%\n",
      "\n",
      "Feature names (vocabulary): ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "\n",
      "Word Count Matrix:\n",
      "       and  document  first  is  one  second  the  third  this\n",
      "Doc 1    0         1      1   1    0       0    1      0     1\n",
      "Doc 2    0         2      0   1    0       1    1      0     1\n",
      "Doc 3    1         0      0   1    1       0    1      1     1\n",
      "Doc 4    0         1      1   1    0       0    1      0     1\n"
     ]
    }
   ],
   "source": [
    "# Basic CountVectorizer\n",
    "count_vec = CountVectorizer()\n",
    "X_counts = count_vec.fit_transform(corpus)\n",
    "\n",
    "print(\"CountVectorizer Results:\")\n",
    "print(f\"Shape: {X_counts.shape} (documents x vocabulary)\")\n",
    "print(f\"Vocabulary size: {len(count_vec.vocabulary_)}\")\n",
    "print(f\"Sparse matrix density: {X_counts.nnz / (X_counts.shape[0] * X_counts.shape[1]):.2%}\")\n",
    "print(f\"\\nFeature names (vocabulary): {count_vec.get_feature_names_out()}\")\n",
    "\n",
    "# Convert to dense for visualization\n",
    "df_counts = pd.DataFrame(\n",
    "    X_counts.toarray(),\n",
    "    columns=count_vec.get_feature_names_out(),\n",
    "    index=[f\"Doc {i+1}\" for i in range(len(corpus))]\n",
    ")\n",
    "print(\"\\nWord Count Matrix:\")\n",
    "print(df_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "count-advanced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced CountVectorizer (with bigrams, stop words removed):\n",
      "Vocabulary: ['document' 'document second' 'second' 'second document']\n",
      "\n",
      "Shape: (4, 4)\n",
      "\n",
      "Word Count Matrix (with n-grams):\n",
      "       document  document second  second  second document\n",
      "Doc 1         1                0       0                0\n",
      "Doc 2         2                1       1                1\n",
      "Doc 3         0                0       0                0\n",
      "Doc 4         1                0       0                0\n"
     ]
    }
   ],
   "source": [
    "# Advanced CountVectorizer with parameters\n",
    "count_vec_adv = CountVectorizer(\n",
    "    max_features=10,        # Keep only top 10 words\n",
    "    min_df=1,               # Word must appear in at least 1 doc\n",
    "    max_df=0.8,             # Ignore words in >80% of docs\n",
    "    ngram_range=(1, 2),     # Include unigrams and bigrams\n",
    "    stop_words='english'    # Remove English stop words\n",
    ")\n",
    "\n",
    "X_counts_adv = count_vec_adv.fit_transform(corpus)\n",
    "\n",
    "print(\"Advanced CountVectorizer (with bigrams, stop words removed):\")\n",
    "print(f\"Vocabulary: {count_vec_adv.get_feature_names_out()}\")\n",
    "print(f\"\\nShape: {X_counts_adv.shape}\")\n",
    "\n",
    "df_counts_adv = pd.DataFrame(\n",
    "    X_counts_adv.toarray(),\n",
    "    columns=count_vec_adv.get_feature_names_out(),\n",
    "    index=[f\"Doc {i+1}\" for i in range(len(corpus))]\n",
    ")\n",
    "print(\"\\nWord Count Matrix (with n-grams):\")\n",
    "print(df_counts_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tfidf",
   "metadata": {},
   "source": [
    "## 2. TfidfVectorizer - Term Frequency-Inverse Document Frequency\n",
    "\n",
    "**TF-IDF** weights words by their importance:\n",
    "- **TF (Term Frequency)**: How often word appears in document\n",
    "- **IDF (Inverse Document Frequency)**: How rare the word is across all documents\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\text{IDF}(t) = \\log\\left(\\frac{1 + n}{1 + \\text{df}(t)}\\right) + 1\n",
    "\\]\n",
    "\n",
    "**Intuition:**\n",
    "- Common words (\"the\", \"is\") ‚Üí Low TF-IDF\n",
    "- Rare, distinctive words ‚Üí High TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tfidf-basic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer Results:\n",
      "Shape: (4, 9)\n",
      "Vocabulary: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "\n",
      "TF-IDF Matrix (values are weighted):\n",
      "         and  document  first     is    one  second    the  third   this\n",
      "Doc 1  0.000     0.470   0.58  0.384  0.000   0.000  0.384  0.000  0.384\n",
      "Doc 2  0.000     0.688   0.00  0.281  0.000   0.539  0.281  0.000  0.281\n",
      "Doc 3  0.512     0.000   0.00  0.267  0.512   0.000  0.267  0.512  0.267\n",
      "Doc 4  0.000     0.470   0.58  0.384  0.000   0.000  0.384  0.000  0.384\n"
     ]
    }
   ],
   "source": [
    "# Basic TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vec.fit_transform(corpus)\n",
    "\n",
    "print(\"TfidfVectorizer Results:\")\n",
    "print(f\"Shape: {X_tfidf.shape}\")\n",
    "print(f\"Vocabulary: {tfidf_vec.get_feature_names_out()}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_tfidf = pd.DataFrame(\n",
    "    X_tfidf.toarray(),\n",
    "    columns=tfidf_vec.get_feature_names_out(),\n",
    "    index=[f\"Doc {i+1}\" for i in range(len(corpus))]\n",
    ")\n",
    "print(\"\\nTF-IDF Matrix (values are weighted):\")\n",
    "print(df_tfidf.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tfidf-comparison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison for word: 'document'\n",
      "\n",
      "Document | Count | TF-IDF\n",
      "-----------------------------------\n",
      "Doc 1    |   1   | 0.4698\n",
      "Doc 2    |   2   | 0.6876\n",
      "Doc 3    |   0   | 0.0000\n",
      "Doc 4    |   1   | 0.4698\n",
      "\n",
      "üí° Notice: 'document' appears frequently ‚Üí lower TF-IDF weights\n",
      "   Rare words get higher weights!\n"
     ]
    }
   ],
   "source": [
    "# Compare Count vs TF-IDF for a specific word\n",
    "word = 'document'\n",
    "\n",
    "if word in count_vec.vocabulary_:\n",
    "    word_idx = count_vec.vocabulary_[word]\n",
    "    \n",
    "    print(f\"Comparison for word: '{word}'\")\n",
    "    print(\"\\nDocument | Count | TF-IDF\")\n",
    "    print(\"-\" * 35)\n",
    "    for i in range(len(corpus)):\n",
    "        count_val = X_counts[i, word_idx]\n",
    "        tfidf_val = X_tfidf[i, word_idx]\n",
    "        print(f\"Doc {i+1}    |   {count_val}   | {tfidf_val:.4f}\")\n",
    "    \n",
    "    print(\"\\nüí° Notice: 'document' appears frequently ‚Üí lower TF-IDF weights\")\n",
    "    print(\"   Rare words get higher weights!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tfidf-advanced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced TF-IDF Configuration:\n",
      "Features: ['and' 'and this' 'document' 'document is' 'first' 'first document'\n",
      " 'is the' 'is this' 'one' 'second' 'second document' 'the first'\n",
      " 'the second' 'the third' 'third' 'third one' 'this document' 'this is'\n",
      " 'this the']\n",
      "\n",
      "IDF values (higher = rarer word):\n",
      "               term       idf\n",
      "0               and  1.916291\n",
      "1          and this  1.916291\n",
      "16    this document  1.916291\n",
      "15        third one  1.916291\n",
      "14            third  1.916291\n",
      "13        the third  1.916291\n",
      "12       the second  1.916291\n",
      "10  second document  1.916291\n",
      "9            second  1.916291\n",
      "8               one  1.916291\n",
      "7           is this  1.916291\n",
      "3       document is  1.916291\n",
      "18         this the  1.916291\n",
      "11        the first  1.510826\n",
      "5    first document  1.510826\n",
      "4             first  1.510826\n",
      "17          this is  1.510826\n",
      "6            is the  1.223144\n",
      "2          document  1.223144\n"
     ]
    }
   ],
   "source": [
    "# Advanced TfidfVectorizer\n",
    "tfidf_vec_adv = TfidfVectorizer(\n",
    "    max_features=50,\n",
    "    min_df=1,\n",
    "    max_df=0.8,\n",
    "    ngram_range=(1, 2),\n",
    "    sublinear_tf=True,      # Apply log scaling to TF\n",
    "    use_idf=True,           # Enable IDF weighting\n",
    "    smooth_idf=True,        # Smooth IDF weights\n",
    "    norm='l2'               # L2 normalization\n",
    ")\n",
    "\n",
    "X_tfidf_adv = tfidf_vec_adv.fit_transform(corpus)\n",
    "\n",
    "print(\"Advanced TF-IDF Configuration:\")\n",
    "print(f\"Features: {tfidf_vec_adv.get_feature_names_out()}\")\n",
    "print(f\"\\nIDF values (higher = rarer word):\")\n",
    "idf_df = pd.DataFrame({\n",
    "    'term': tfidf_vec_adv.get_feature_names_out(),\n",
    "    'idf': tfidf_vec_adv.idf_\n",
    "}).sort_values('idf', ascending=False)\n",
    "print(idf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hashing",
   "metadata": {},
   "source": [
    "## 3. HashingVectorizer - Memory-Efficient Alternative\n",
    "\n",
    "**Advantages:**\n",
    "- No vocabulary storage (memory efficient)\n",
    "- Fast - no need to fit vocabulary\n",
    "- Scalable to large datasets\n",
    "- Supports online/streaming learning\n",
    "\n",
    "**Disadvantages:**\n",
    "- Hash collisions (multiple words ‚Üí same feature)\n",
    "- No inverse transform (can't retrieve original words)\n",
    "- Fixed feature space size\n",
    "\n",
    "**When to use:**\n",
    "- Very large datasets (millions of documents)\n",
    "- Online/streaming scenarios\n",
    "- Memory constraints\n",
    "- Don't need to interpret features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hashing-basic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HashingVectorizer Results:\n",
      "Shape: (4, 20)\n",
      "No vocabulary stored (memory efficient!)\n",
      "\n",
      "Hash Matrix (first 3 docs, first 10 features):\n",
      "[[ 0.          0.          0.4472136   0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.35355339  0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.40824829  0.40824829  0.         -0.40824829 -0.40824829\n",
      "   0.          0.          0.          0.        ]]\n",
      "\n",
      "‚ö†Ô∏è Note: Cannot retrieve feature names (no vocabulary)\n",
      "‚úì But much faster and more memory efficient!\n"
     ]
    }
   ],
   "source": [
    "# HashingVectorizer\n",
    "hash_vec = HashingVectorizer(\n",
    "    n_features=20,          # Hash space size (trade-off: collisions vs memory)\n",
    "    alternate_sign=True     # Reduce impact of collisions\n",
    ")\n",
    "\n",
    "X_hash = hash_vec.transform(corpus)  # No fit() needed!\n",
    "\n",
    "print(\"HashingVectorizer Results:\")\n",
    "print(f\"Shape: {X_hash.shape}\")\n",
    "print(f\"No vocabulary stored (memory efficient!)\")\n",
    "print(f\"\\nHash Matrix (first 3 docs, first 10 features):\")\n",
    "print(X_hash[:3, :10].toarray())\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Note: Cannot retrieve feature names (no vocabulary)\")\n",
    "print(\"‚úì But much faster and more memory efficient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "real-dataset",
   "metadata": {},
   "source": [
    "## 4. Real-World Example: 20 Newsgroups Classification\n",
    "\n",
    "Let's classify news articles using text vectorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "newsgroups-load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 Newsgroups dataset...\n",
      "\n",
      "Training samples: 2254\n",
      "Test samples: 1499\n",
      "Categories: ['alt.atheism', 'comp.graphics', 'rec.sport.baseball', 'sci.space']\n",
      "\n",
      "Sample document (category: comp.graphics):\n",
      "\n",
      "A 68070 is just a 68010 with a built in MMU.  I don't even think that Moto.\n",
      "manufactures them.\n",
      "\n",
      "                                  - Ian Romanick\n",
      "                                    Dancing Fool of Epsilon...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load subset of categories\n",
    "categories = ['alt.atheism', 'sci.space', 'rec.sport.baseball', 'comp.graphics']\n",
    "\n",
    "print(\"Loading 20 Newsgroups dataset...\")\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    categories=categories,\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(\n",
    "    subset='test',\n",
    "    categories=categories,\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {len(newsgroups_train.data)}\")\n",
    "print(f\"Test samples: {len(newsgroups_test.data)}\")\n",
    "print(f\"Categories: {newsgroups_train.target_names}\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nSample document (category: {newsgroups_train.target_names[newsgroups_train.target[0]]}):\")\n",
    "print(newsgroups_train.data[0][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "newsgroups-countvec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Method 1: CountVectorizer + Multinomial Naive Bayes\n",
      "============================================================\n",
      "Vocabulary size: 5000\n",
      "Training matrix shape: (2254, 5000)\n",
      "Sparsity: 99.11%\n",
      "\n",
      "Accuracy: 0.8666\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.81      0.86      0.83       319\n",
      "     comp.graphics       0.90      0.89      0.89       389\n",
      "rec.sport.baseball       0.85      0.92      0.88       397\n",
      "         sci.space       0.90      0.80      0.85       394\n",
      "\n",
      "          accuracy                           0.87      1499\n",
      "         macro avg       0.87      0.87      0.86      1499\n",
      "      weighted avg       0.87      0.87      0.87      1499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method 1: CountVectorizer + Naive Bayes\n",
    "print(\"=\" * 60)\n",
    "print(\"Method 1: CountVectorizer + Multinomial Naive Bayes\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.7,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_test_counts = count_vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "print(f\"Vocabulary size: {len(count_vectorizer.vocabulary_)}\")\n",
    "print(f\"Training matrix shape: {X_train_counts.shape}\")\n",
    "print(f\"Sparsity: {(1 - X_train_counts.nnz / (X_train_counts.shape[0] * X_train_counts.shape[1])):.2%}\")\n",
    "\n",
    "# Train classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_counts, newsgroups_train.target)\n",
    "\n",
    "# Predict\n",
    "y_pred_nb = nb_classifier.predict(X_test_counts)\n",
    "accuracy_nb = accuracy_score(newsgroups_test.target, y_pred_nb)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy_nb:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(newsgroups_test.target, y_pred_nb, \n",
    "                          target_names=newsgroups_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "newsgroups-tfidf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Method 2: TF-IDF + Logistic Regression\n",
      "============================================================\n",
      "Vocabulary size: 5000\n",
      "Training matrix shape: (2254, 5000)\n",
      "\n",
      "Accuracy: 0.8512\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.86      0.76      0.81       319\n",
      "     comp.graphics       0.90      0.88      0.89       389\n",
      "rec.sport.baseball       0.83      0.92      0.88       397\n",
      "         sci.space       0.82      0.83      0.82       394\n",
      "\n",
      "          accuracy                           0.85      1499\n",
      "         macro avg       0.85      0.85      0.85      1499\n",
      "      weighted avg       0.85      0.85      0.85      1499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method 2: TF-IDF + Logistic Regression\n",
    "print(\"=\" * 60)\n",
    "print(\"Method 2: TF-IDF + Logistic Regression\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.7,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "print(f\"Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "print(f\"Training matrix shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "# Train classifier\n",
    "lr_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_classifier.fit(X_train_tfidf, newsgroups_train.target)\n",
    "\n",
    "# Predict\n",
    "y_pred_lr = lr_classifier.predict(X_test_tfidf)\n",
    "accuracy_lr = accuracy_score(newsgroups_test.target, y_pred_lr)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy_lr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(newsgroups_test.target, y_pred_lr,\n",
    "                          target_names=newsgroups_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "newsgroups-hashing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Method 3: HashingVectorizer + Logistic Regression\n",
      "============================================================\n",
      "Hash space size: 4096\n",
      "Training matrix shape: (2254, 4096)\n",
      "No vocabulary stored (instant processing!)\n",
      "\n",
      "Accuracy: 0.8245\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.82      0.74      0.77       319\n",
      "     comp.graphics       0.88      0.87      0.87       389\n",
      "rec.sport.baseball       0.80      0.88      0.84       397\n",
      "         sci.space       0.80      0.80      0.80       394\n",
      "\n",
      "          accuracy                           0.82      1499\n",
      "         macro avg       0.82      0.82      0.82      1499\n",
      "      weighted avg       0.83      0.82      0.82      1499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method 3: HashingVectorizer + Logistic Regression\n",
    "print(\"=\" * 60)\n",
    "print(\"Method 3: HashingVectorizer + Logistic Regression\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "hash_vectorizer = HashingVectorizer(\n",
    "    n_features=2**12,  # 4096 features\n",
    "    alternate_sign=True,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "X_train_hash = hash_vectorizer.transform(newsgroups_train.data)\n",
    "X_test_hash = hash_vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "print(f\"Hash space size: {X_train_hash.shape[1]}\")\n",
    "print(f\"Training matrix shape: {X_train_hash.shape}\")\n",
    "print(\"No vocabulary stored (instant processing!)\")\n",
    "\n",
    "# Train classifier\n",
    "lr_hash = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_hash.fit(X_train_hash, newsgroups_train.target)\n",
    "\n",
    "# Predict\n",
    "y_pred_hash = lr_hash.predict(X_test_hash)\n",
    "accuracy_hash = accuracy_score(newsgroups_test.target, y_pred_hash)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy_hash:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(newsgroups_test.target, y_pred_hash,\n",
    "                          target_names=newsgroups_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "comparison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMPARISON SUMMARY\n",
      "============================================================\n",
      "               Method  Accuracy  Features\n",
      "CountVec + NaiveBayes  0.866578      5000\n",
      "      TF-IDF + LogReg  0.851234      5000\n",
      "     Hashing + LogReg  0.824550      4096\n",
      "\n",
      "üí° Insights:\n",
      "  - TF-IDF often performs best for text classification\n",
      "  - HashingVectorizer trades slight accuracy for speed/memory\n",
      "  - CountVectorizer + Naive Bayes is fast and interpretable\n"
     ]
    }
   ],
   "source": [
    "# Compare all methods\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Method': ['CountVec + NaiveBayes', 'TF-IDF + LogReg', 'Hashing + LogReg'],\n",
    "    'Accuracy': [accuracy_nb, accuracy_lr, accuracy_hash],\n",
    "    'Features': [X_train_counts.shape[1], X_train_tfidf.shape[1], X_train_hash.shape[1]]\n",
    "})\n",
    "\n",
    "print(results.to_string(index=False))\n",
    "print(\"\\nüí° Insights:\")\n",
    "print(\"  - TF-IDF often performs best for text classification\")\n",
    "print(\"  - HashingVectorizer trades slight accuracy for speed/memory\")\n",
    "print(\"  - CountVectorizer + Naive Bayes is fast and interpretable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-importance",
   "metadata": {},
   "source": [
    "## 5. Analyzing Important Features\n",
    "\n",
    "Extract most important words per category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "top-features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Features per Category (TF-IDF + LogReg):\n",
      "============================================================\n",
      "\n",
      "alt.atheism:\n",
      "  god                  2.6814\n",
      "  religion             2.3182\n",
      "  people               1.9922\n",
      "  bible                1.7714\n",
      "  atheism              1.5777\n",
      "  morality             1.4381\n",
      "  islam                1.3838\n",
      "  religious            1.3806\n",
      "  atheists             1.3455\n",
      "  islamic              1.3451\n",
      "\n",
      "comp.graphics:\n",
      "  graphics             3.3654\n",
      "  image                2.1807\n",
      "  file                 2.1225\n",
      "  3d                   1.7733\n",
      "  hi                   1.7632\n",
      "  computer             1.7391\n",
      "  files                1.6648\n",
      "  looking              1.5061\n",
      "  ftp                  1.4112\n",
      "  code                 1.3874\n",
      "\n",
      "rec.sport.baseball:\n",
      "  baseball             2.8102\n",
      "  year                 2.3092\n",
      "  game                 2.2411\n",
      "  team                 2.1919\n",
      "  games                2.0488\n",
      "  players              1.8281\n",
      "  stadium              1.6624\n",
      "  jewish               1.5904\n",
      "  league               1.5670\n",
      "  season               1.5657\n",
      "\n",
      "sci.space:\n",
      "  space                4.6317\n",
      "  orbit                2.0502\n",
      "  nasa                 1.9670\n",
      "  moon                 1.7824\n",
      "  launch               1.7755\n",
      "  earth                1.5731\n",
      "  shuttle              1.5042\n",
      "  spacecraft           1.4794\n",
      "  solar                1.2207\n",
      "  lunar                1.2130\n"
     ]
    }
   ],
   "source": [
    "# Get top features for TF-IDF model\n",
    "def show_top_features(classifier, vectorizer, categories, n=10):\n",
    "    \"\"\"Show top N features (words) for each category\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    for i, category in enumerate(categories):\n",
    "        if hasattr(classifier, 'coef_'):\n",
    "            top_indices = np.argsort(classifier.coef_[i])[-n:][::-1]\n",
    "            top_features = [feature_names[idx] for idx in top_indices]\n",
    "            top_scores = classifier.coef_[i][top_indices]\n",
    "            \n",
    "            print(f\"\\n{category}:\")\n",
    "            for feat, score in zip(top_features, top_scores):\n",
    "                print(f\"  {feat:20s} {score:.4f}\")\n",
    "\n",
    "print(\"Top 10 Features per Category (TF-IDF + LogReg):\")\n",
    "print(\"=\" * 60)\n",
    "show_top_features(lr_classifier, tfidf_vectorizer, newsgroups_test.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sparse-operations",
   "metadata": {},
   "source": [
    "## 6. Working with Sparse Matrices\n",
    "\n",
    "Understanding sparse matrix operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sparse-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse Matrix Operations:\n",
      "Matrix shape: (100, 1000)\n",
      "Matrix type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Number of non-zero elements: 4990\n",
      "Sparsity: 95.01%\n",
      "\n",
      "Memory Usage:\n",
      "  Sparse: 0.06 MB\n",
      "  Dense:  0.76 MB\n",
      "  Savings: 92.5%\n",
      "\n",
      "Common Operations:\n",
      "  Get element: X_sparse[0, 10] = 0.0000\n",
      "  Slice row: (1, 1000)\n",
      "  Convert to dense: (100, 1000)\n",
      "  Matrix multiplication: X_sparse @ X_sparse.T shape = (100, 100)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
    "\n",
    "# Create sample sparse matrix\n",
    "vec = TfidfVectorizer(max_features=1000)\n",
    "X_sparse = vec.fit_transform(newsgroups_train.data[:100])\n",
    "\n",
    "print(\"Sparse Matrix Operations:\")\n",
    "print(f\"Matrix shape: {X_sparse.shape}\")\n",
    "print(f\"Matrix type: {type(X_sparse)}\")\n",
    "print(f\"Number of non-zero elements: {X_sparse.nnz}\")\n",
    "print(f\"Sparsity: {(1 - X_sparse.nnz / (X_sparse.shape[0] * X_sparse.shape[1])):.2%}\")\n",
    "\n",
    "# Memory comparison\n",
    "import sys\n",
    "sparse_size = (X_sparse.data.nbytes + X_sparse.indices.nbytes + X_sparse.indptr.nbytes) / 1024**2\n",
    "dense_size = X_sparse.toarray().nbytes / 1024**2\n",
    "\n",
    "print(f\"\\nMemory Usage:\")\n",
    "print(f\"  Sparse: {sparse_size:.2f} MB\")\n",
    "print(f\"  Dense:  {dense_size:.2f} MB\")\n",
    "print(f\"  Savings: {(1 - sparse_size/dense_size):.1%}\")\n",
    "\n",
    "# Common operations\n",
    "print(f\"\\nCommon Operations:\")\n",
    "print(f\"  Get element: X_sparse[0, 10] = {X_sparse[0, 10]:.4f}\")\n",
    "print(f\"  Slice row: {X_sparse[0].shape}\")\n",
    "print(f\"  Convert to dense: {X_sparse.toarray().shape}\")\n",
    "print(f\"  Matrix multiplication: X_sparse @ X_sparse.T shape = {(X_sparse @ X_sparse.T).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Choosing the Right Vectorizer\n",
    "\n",
    "| Vectorizer | Best For | Pros | Cons |\n",
    "|------------|----------|------|------|\n",
    "| **CountVectorizer** | Small datasets, interpretability | Simple, fast, interpretable | Doesn't weight importance |\n",
    "| **TfidfVectorizer** | Most text classification tasks | Weights word importance | Slightly slower |\n",
    "| **HashingVectorizer** | Large/streaming data | Memory efficient, fast | Hash collisions, not invertible |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always use stop words** (`stop_words='english'`)\n",
    "2. **Set min_df** to filter rare words (noise)\n",
    "3. **Set max_df** to filter very common words\n",
    "4. **Use max_features** to control vocabulary size\n",
    "5. **Try n-grams** (bigrams can capture phrases)\n",
    "6. **Keep sparse format** (don't convert to dense unless necessary)\n",
    "7. **TF-IDF usually outperforms** simple counts\n",
    "\n",
    "### Parameter Tuning Tips\n",
    "\n",
    "```python\n",
    "# Good starting point\n",
    "TfidfVectorizer(\n",
    "    max_features=5000,      # Reasonable vocabulary size\n",
    "    min_df=5,               # Remove very rare words\n",
    "    max_df=0.7,             # Remove very common words\n",
    "    ngram_range=(1, 2),     # Include bigrams\n",
    "    stop_words='english',   # Remove stop words\n",
    "    sublinear_tf=True       # Dampen term frequency\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
