{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Feature Engineering with Sklearn\n",
        "\n",
        "## What is Feature Engineering?\n",
        "\n",
        "**Definition**: Creating new features from existing data to improve model performance\n",
        "\n",
        "**Why Important?**\n",
        "> \"Applied machine learning is basically feature engineering\" - Andrew Ng\n",
        "\n",
        "Good features can:\n",
        "- Improve model accuracy\n",
        "- Reduce training time\n",
        "- Make models more interpretable\n",
        "\n",
        "## Topics Covered\n",
        "\n",
        "1. **Polynomial Features**: Create interaction terms and powers\n",
        "2. **Binning/Discretization**: Convert continuous \u2192 categorical\n",
        "3. **Date/Time Features**: Extract useful components from dates\n",
        "4. **Custom Transformations**: Domain-specific features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures, KBinsDiscretizer, FunctionTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "polynomial-features",
      "metadata": {},
      "source": [
        "## 1. Polynomial Features\n",
        "\n",
        "**Definition**: Creates polynomial and interaction features from original features\n",
        "\n",
        "**Example**: For features [a, b]\n",
        "- Degree 1: [1, a, b]\n",
        "- Degree 2: [1, a, b, a\u00b2, ab, b\u00b2]\n",
        "- Degree 3: [1, a, b, a\u00b2, ab, b\u00b2, a\u00b3, a\u00b2b, ab\u00b2, b\u00b3]\n",
        "\n",
        "**Why Use?**\n",
        "- Capture non-linear relationships\n",
        "- Model interactions between features\n",
        "- Make linear models more powerful\n",
        "\n",
        "**Example Use Case**: \n",
        "- House price = area \u00d7 price_per_sqft (interaction)\n",
        "- Profit might depend on price\u00b2 (non-linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "poly-simple",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple example: 2 features\n",
        "X = np.array([\n",
        "    [2, 3],\n",
        "    [4, 5]\n",
        "])\n",
        "\n",
        "print(\"Original Features:\")\n",
        "print(\"[a, b]\")\n",
        "print(X)\n",
        "\n",
        "# Create polynomial features (degree 2)\n",
        "poly = PolynomialFeatures(degree=2, include_bias=True)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "print(\"\\nPolynomial Features (degree=2):\")\n",
        "print(\"Feature names:\", poly.get_feature_names_out(['a', 'b']))\n",
        "print(X_poly)\n",
        "\n",
        "# Verify calculations for first row [2, 3]\n",
        "print(\"\\nVerification for [2, 3]:\")\n",
        "print(f\"  1:      {1}\")\n",
        "print(f\"  a:      {2}\")\n",
        "print(f\"  b:      {3}\")\n",
        "print(f\"  a\u00b2:     {2**2}\")\n",
        "print(f\"  a\u00d7b:    {2*3}\")\n",
        "print(f\"  b\u00b2:     {3**2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "poly-params",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Different parameters\n",
        "X_sample = np.array([[2, 3]])\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"POLYNOMIAL FEATURES - DIFFERENT CONFIGURATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Degree 3\n",
        "poly_d3 = PolynomialFeatures(degree=3, include_bias=False)\n",
        "X_d3 = poly_d3.fit_transform(X_sample)\n",
        "print(\"\\nDegree=3, include_bias=False:\")\n",
        "print(f\"Features ({X_d3.shape[1]}): {poly_d3.get_feature_names_out(['a', 'b'])}\")\n",
        "print(X_d3)\n",
        "\n",
        "# Only interaction features (no powers)\n",
        "poly_interact = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
        "X_interact = poly_interact.fit_transform(X_sample)\n",
        "print(\"\\nDegree=2, interaction_only=True:\")\n",
        "print(f\"Features ({X_interact.shape[1]}): {poly_interact.get_feature_names_out(['a', 'b'])}\")\n",
        "print(X_interact)\n",
        "print(\"(Only a, b, a\u00d7b - no a\u00b2, b\u00b2)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "poly-regression",
      "metadata": {},
      "source": [
        "### Polynomial Regression Example\n",
        "\n",
        "Fit non-linear data using polynomial features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "poly-regression-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate non-linear data\n",
        "np.random.seed(42)\n",
        "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "y = 0.5 * X**2 + X + 2 + np.random.randn(100, 1) * 3\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Dataset: y = 0.5*x\u00b2 + x + 2 + noise\")\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Test samples: {X_test.shape[0]}\")\n",
        "\n",
        "# Compare: Linear vs Polynomial Regression\n",
        "results = {}\n",
        "\n",
        "for degree in [1, 2, 3, 5]:\n",
        "    # Create polynomial features\n",
        "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    X_train_poly = poly.fit_transform(X_train)\n",
        "    X_test_poly = poly.transform(X_test)\n",
        "    \n",
        "    # Train linear regression on polynomial features\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train_poly, y_train)\n",
        "    \n",
        "    # Evaluate\n",
        "    train_score = model.score(X_train_poly, y_train)\n",
        "    test_score = model.score(X_test_poly, y_test)\n",
        "    \n",
        "    results[degree] = {'train_r2': train_score, 'test_r2': test_score}\n",
        "    \n",
        "    print(f\"\\nDegree {degree}:\")\n",
        "    print(f\"  Features: {X_train_poly.shape[1]}\")\n",
        "    print(f\"  Train R\u00b2: {train_score:.4f}\")\n",
        "    print(f\"  Test R\u00b2:  {test_score:.4f}\")\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "for i, degree in enumerate([1, 2, 5]):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    \n",
        "    # Transform data\n",
        "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    X_train_poly = poly.fit_transform(X_train)\n",
        "    \n",
        "    # Fit model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train_poly, y_train)\n",
        "    \n",
        "    # Plot\n",
        "    X_plot = np.linspace(0, 10, 300).reshape(-1, 1)\n",
        "    X_plot_poly = poly.transform(X_plot)\n",
        "    y_plot = model.predict(X_plot_poly)\n",
        "    \n",
        "    plt.scatter(X_train, y_train, alpha=0.5, s=20, label='Training data')\n",
        "    plt.plot(X_plot, y_plot, 'r-', linewidth=2, label=f'Degree {degree}')\n",
        "    plt.title(f'Polynomial Degree {degree}\\nTest R\u00b2 = {results[degree][\"test_r2\"]:.3f}')\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('y')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Observation:\")\n",
        "print(\"  - Degree 1 (linear): Underfitting\")\n",
        "print(\"  - Degree 2: Good fit (matches true relationship)\")\n",
        "print(\"  - Degree 5: Might overfit (too flexible)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "binning",
      "metadata": {},
      "source": [
        "## 2. Binning / Discretization\n",
        "\n",
        "**Definition**: Convert continuous values into discrete bins/categories\n",
        "\n",
        "**Example**: Age \u2192 Age Groups\n",
        "- Age 25 \u2192 '18-30'\n",
        "- Age 45 \u2192 '31-50'\n",
        "- Age 65 \u2192 '51+'\n",
        "\n",
        "**Why Use?**\n",
        "- Reduce noise from continuous features\n",
        "- Handle non-linear relationships\n",
        "- Make models more robust\n",
        "- Create interpretable categories\n",
        "\n",
        "## Binning Strategies\n",
        "\n",
        "| Strategy | Description | When to Use |\n",
        "|----------|-------------|-------------|\n",
        "| **uniform** | Equal-width bins | Data is uniformly distributed |\n",
        "| **quantile** | Equal-frequency bins | Data is skewed |\n",
        "| **kmeans** | Cluster-based bins | Complex distributions |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "binning-basic",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample data: ages\n",
        "ages = np.array([18, 22, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80]).reshape(-1, 1)\n",
        "\n",
        "print(\"Original Ages:\")\n",
        "print(ages.flatten())\n",
        "\n",
        "# Strategy 1: Uniform (equal-width bins)\n",
        "binner_uniform = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='uniform')\n",
        "ages_uniform = binner_uniform.fit_transform(ages)\n",
        "\n",
        "print(\"\\nUniform Binning (4 bins):\")\n",
        "print(\"Bin edges:\", binner_uniform.bin_edges_[0])\n",
        "print(\"Binned:\", ages_uniform.flatten())\n",
        "\n",
        "# Strategy 2: Quantile (equal-frequency bins)\n",
        "binner_quantile = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='quantile')\n",
        "ages_quantile = binner_quantile.fit_transform(ages)\n",
        "\n",
        "print(\"\\nQuantile Binning (4 bins):\")\n",
        "print(\"Bin edges:\", binner_quantile.bin_edges_[0])\n",
        "print(\"Binned:\", ages_quantile.flatten())\n",
        "\n",
        "# Create comparison dataframe\n",
        "comparison = pd.DataFrame({\n",
        "    'Age': ages.flatten(),\n",
        "    'Uniform_Bin': ages_uniform.flatten().astype(int),\n",
        "    'Quantile_Bin': ages_quantile.flatten().astype(int)\n",
        "})\n",
        "\n",
        "print(\"\\nComparison:\")\n",
        "print(comparison)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "binning-encode",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Different encoding options\n",
        "print(\"=\"*60)\n",
        "print(\"BINNING ENCODING OPTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "sample_ages = np.array([22, 35, 48, 61]).reshape(-1, 1)\n",
        "print(f\"\\nSample ages: {sample_ages.flatten()}\")\n",
        "\n",
        "# encode='ordinal' - Bin number\n",
        "binner_ord = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
        "binned_ord = binner_ord.fit_transform(sample_ages)\n",
        "print(\"\\nencode='ordinal':\")\n",
        "print(f\"Result: {binned_ord.flatten()}\")\n",
        "print(\"Meaning: Which bin (0, 1, 2)\")\n",
        "\n",
        "# encode='onehot' - Binary columns\n",
        "binner_ohe = KBinsDiscretizer(n_bins=3, encode='onehot', strategy='uniform')\n",
        "binned_ohe = binner_ohe.fit_transform(sample_ages).toarray()\n",
        "print(\"\\nencode='onehot':\")\n",
        "print(binned_ohe)\n",
        "print(\"Meaning: One-hot encoded bins\")\n",
        "\n",
        "# encode='onehot-dense' - Same as onehot but dense array\n",
        "binner_dense = KBinsDiscretizer(n_bins=3, encode='onehot-dense', strategy='uniform')\n",
        "binned_dense = binner_dense.fit_transform(sample_ages)\n",
        "print(\"\\nencode='onehot-dense':\")\n",
        "print(binned_dense)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pandas-cut",
      "metadata": {},
      "source": [
        "### Pandas Alternative: pd.cut() and pd.qcut()\n",
        "\n",
        "For quick exploratory analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pandas-cut-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample salary data\n",
        "salaries = pd.Series([25000, 35000, 45000, 55000, 65000, 75000, 85000, 95000, 120000, 150000])\n",
        "\n",
        "print(\"Original Salaries:\")\n",
        "print(salaries.values)\n",
        "\n",
        "# pd.cut - Fixed bin edges\n",
        "bins_cut = pd.cut(salaries, bins=3, labels=['Low', 'Medium', 'High'])\n",
        "print(\"\\npd.cut (equal-width):\")\n",
        "print(bins_cut)\n",
        "\n",
        "# pd.qcut - Quantile-based\n",
        "bins_qcut = pd.qcut(salaries, q=3, labels=['Low', 'Medium', 'High'])\n",
        "print(\"\\npd.qcut (equal-frequency):\")\n",
        "print(bins_qcut)\n",
        "\n",
        "# Custom bin edges\n",
        "bins_custom = pd.cut(\n",
        "    salaries, \n",
        "    bins=[0, 50000, 80000, 200000],\n",
        "    labels=['Junior', 'Mid-level', 'Senior']\n",
        ")\n",
        "print(\"\\nCustom bins:\")\n",
        "print(bins_custom)\n",
        "\n",
        "# Value counts\n",
        "print(\"\\nDistribution:\")\n",
        "print(bins_custom.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "datetime-features",
      "metadata": {},
      "source": [
        "## 3. Date/Time Feature Engineering\n",
        "\n",
        "**Problem**: Dates are often stored as strings or timestamps\n",
        "\n",
        "**Solution**: Extract meaningful temporal features\n",
        "\n",
        "**Common Date Features**:\n",
        "- Year, Month, Day\n",
        "- Day of week (Monday=0, Sunday=6)\n",
        "- Is weekend?\n",
        "- Quarter (Q1, Q2, Q3, Q4)\n",
        "- Is month start/end?\n",
        "- Days since/until event\n",
        "- Hour, Minute (for timestamps)\n",
        "- Is holiday?\n",
        "\n",
        "**Why Important?**\n",
        "- Capture seasonality\n",
        "- Detect patterns (e.g., higher sales on weekends)\n",
        "- Time-based trends"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "datetime-basic",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample date data\n",
        "dates = pd.date_range(start='2025-01-01', end='2025-12-31', freq='7D')\n",
        "df_dates = pd.DataFrame({'date': dates})\n",
        "\n",
        "print(\"Original Dates:\")\n",
        "print(df_dates.head(10))\n",
        "print(f\"\\nTotal dates: {len(df_dates)}\")\n",
        "\n",
        "# Extract date features\n",
        "df_dates['year'] = df_dates['date'].dt.year\n",
        "df_dates['month'] = df_dates['date'].dt.month\n",
        "df_dates['day'] = df_dates['date'].dt.day\n",
        "df_dates['day_of_week'] = df_dates['date'].dt.dayofweek  # Monday=0, Sunday=6\n",
        "df_dates['day_name'] = df_dates['date'].dt.day_name()\n",
        "df_dates['week_of_year'] = df_dates['date'].dt.isocalendar().week\n",
        "df_dates['quarter'] = df_dates['date'].dt.quarter\n",
        "df_dates['is_weekend'] = df_dates['day_of_week'].isin([5, 6]).astype(int)\n",
        "df_dates['is_month_start'] = df_dates['date'].dt.is_month_start.astype(int)\n",
        "df_dates['is_month_end'] = df_dates['date'].dt.is_month_end.astype(int)\n",
        "\n",
        "print(\"\\nExtracted Features:\")\n",
        "print(df_dates.head(10))\n",
        "\n",
        "print(\"\\nFeature Statistics:\")\n",
        "print(f\"Unique months: {df_dates['month'].nunique()}\")\n",
        "print(f\"Weekend dates: {df_dates['is_weekend'].sum()}\")\n",
        "print(f\"Weekday dates: {(1-df_dates['is_weekend']).sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "datetime-advanced",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced date features\n",
        "print(\"=\"*60)\n",
        "print(\"ADVANCED DATE FEATURES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Days since reference date\n",
        "reference_date = pd.Timestamp('2025-01-01')\n",
        "df_dates['days_since_start'] = (df_dates['date'] - reference_date).dt.days\n",
        "\n",
        "# Days until end of year\n",
        "end_of_year = pd.Timestamp('2025-12-31')\n",
        "df_dates['days_until_end'] = (end_of_year - df_dates['date']).dt.days\n",
        "\n",
        "# Cyclical encoding for month (preserves circular nature)\n",
        "df_dates['month_sin'] = np.sin(2 * np.pi * df_dates['month'] / 12)\n",
        "df_dates['month_cos'] = np.cos(2 * np.pi * df_dates['month'] / 12)\n",
        "\n",
        "# Cyclical encoding for day of week\n",
        "df_dates['dow_sin'] = np.sin(2 * np.pi * df_dates['day_of_week'] / 7)\n",
        "df_dates['dow_cos'] = np.cos(2 * np.pi * df_dates['day_of_week'] / 7)\n",
        "\n",
        "print(\"\\nAdvanced Features:\")\n",
        "print(df_dates[[\n",
        "    'date', 'days_since_start', 'days_until_end', \n",
        "    'month_sin', 'month_cos', 'dow_sin', 'dow_cos'\n",
        "]].head(10))\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Why Cyclical Encoding?\")\n",
        "print(\"  - December (12) and January (1) are close in time\")\n",
        "print(\"  - Standard encoding: |12 - 1| = 11 (far apart)\")\n",
        "print(\"  - Cyclical encoding: Captures circular relationship\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "datetime-ml",
      "metadata": {},
      "source": [
        "### Real Example: Predicting Sales with Date Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sales-prediction",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic sales data\n",
        "np.random.seed(42)\n",
        "dates = pd.date_range(start='2024-01-01', end='2025-12-31', freq='D')\n",
        "n_days = len(dates)\n",
        "\n",
        "# Create sales with patterns\n",
        "sales = (\n",
        "    100 +  # Base sales\n",
        "    30 * np.sin(2 * np.pi * np.arange(n_days) / 365) +  # Yearly seasonality\n",
        "    20 * np.sin(2 * np.pi * np.arange(n_days) / 7) +    # Weekly pattern\n",
        "    np.random.randn(n_days) * 10  # Noise\n",
        ")\n",
        "\n",
        "# Add weekend boost\n",
        "day_of_week = pd.to_datetime(dates).dayofweek\n",
        "weekend_boost = np.where(day_of_week.isin([5, 6]), 25, 0)\n",
        "sales = sales + weekend_boost\n",
        "\n",
        "# Create dataframe\n",
        "df_sales = pd.DataFrame({'date': dates, 'sales': sales})\n",
        "\n",
        "print(\"Sales Data Sample:\")\n",
        "print(df_sales.head(10))\n",
        "print(f\"\\nDate range: {df_sales['date'].min()} to {df_sales['date'].max()}\")\n",
        "print(f\"Average sales: ${df_sales['sales'].mean():.2f}\")\n",
        "\n",
        "# Extract features\n",
        "df_sales['day_of_week'] = df_sales['date'].dt.dayofweek\n",
        "df_sales['month'] = df_sales['date'].dt.month\n",
        "df_sales['quarter'] = df_sales['date'].dt.quarter\n",
        "df_sales['is_weekend'] = df_sales['day_of_week'].isin([5, 6]).astype(int)\n",
        "df_sales['day_of_year'] = df_sales['date'].dt.dayofyear\n",
        "\n",
        "# Cyclical features\n",
        "df_sales['dow_sin'] = np.sin(2 * np.pi * df_sales['day_of_week'] / 7)\n",
        "df_sales['dow_cos'] = np.cos(2 * np.pi * df_sales['day_of_week'] / 7)\n",
        "df_sales['month_sin'] = np.sin(2 * np.pi * df_sales['month'] / 12)\n",
        "df_sales['month_cos'] = np.cos(2 * np.pi * df_sales['month'] / 12)\n",
        "\n",
        "# Prepare for modeling\n",
        "feature_cols = ['day_of_week', 'month', 'quarter', 'is_weekend', \n",
        "                'day_of_year', 'dow_sin', 'dow_cos', 'month_sin', 'month_cos']\n",
        "X = df_sales[feature_cols]\n",
        "y = df_sales['sales']\n",
        "\n",
        "# Train-test split (last 3 months for testing)\n",
        "split_date = '2025-10-01'\n",
        "train_mask = df_sales['date'] < split_date\n",
        "test_mask = df_sales['date'] >= split_date\n",
        "\n",
        "X_train, y_train = X[train_mask], y[train_mask]\n",
        "X_test, y_test = X[test_mask], y[test_mask]\n",
        "\n",
        "print(f\"\\nTraining samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "train_r2 = r2_score(y_train, y_pred_train)\n",
        "test_r2 = r2_score(y_test, y_pred_test)\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL PERFORMANCE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Train R\u00b2: {train_r2:.4f}\")\n",
        "print(f\"Test R\u00b2:  {test_r2:.4f}\")\n",
        "print(f\"Test RMSE: ${test_rmse:.2f}\")\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'coefficient': model.coef_\n",
        "}).sort_values('coefficient', key=abs, ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importance (by coefficient magnitude):\")\n",
        "print(feature_importance)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "custom-transformers",
      "metadata": {},
      "source": [
        "## 4. Custom Transformations\n",
        "\n",
        "**Definition**: Create domain-specific features using FunctionTransformer\n",
        "\n",
        "**Use Cases**:\n",
        "- Log transformation: `log(x)` for right-skewed data\n",
        "- Square root: `\u221ax` for count data\n",
        "- Reciprocal: `1/x` for rates\n",
        "- Domain formulas: BMI = weight / height\u00b2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "custom-transform",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: House price features\n",
        "house_data = pd.DataFrame({\n",
        "    'area_sqft': [1000, 1500, 2000, 2500, 3000],\n",
        "    'price': [200000, 280000, 350000, 420000, 480000],\n",
        "    'bedrooms': [2, 3, 3, 4, 4],\n",
        "    'age_years': [5, 10, 15, 20, 25]\n",
        "})\n",
        "\n",
        "print(\"Original Data:\")\n",
        "print(house_data)\n",
        "\n",
        "# Create custom features\n",
        "house_data['price_per_sqft'] = house_data['price'] / house_data['area_sqft']\n",
        "house_data['sqft_per_bedroom'] = house_data['area_sqft'] / house_data['bedrooms']\n",
        "house_data['log_price'] = np.log(house_data['price'])\n",
        "house_data['age_squared'] = house_data['age_years'] ** 2\n",
        "\n",
        "print(\"\\nWith Engineered Features:\")\n",
        "print(house_data)\n",
        "\n",
        "# Using FunctionTransformer\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"USING FunctionTransformer\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Log transformer\n",
        "log_transformer = FunctionTransformer(np.log1p, validate=True)  # log1p = log(1+x)\n",
        "prices = house_data[['price']].values\n",
        "prices_log = log_transformer.fit_transform(prices)\n",
        "\n",
        "print(\"\\nLog transformation:\")\n",
        "print(f\"Original: {prices.flatten()}\")\n",
        "print(f\"Log(1+x): {prices_log.flatten()}\")\n",
        "\n",
        "# Square root transformer\n",
        "sqrt_transformer = FunctionTransformer(np.sqrt, validate=True)\n",
        "area = house_data[['area_sqft']].values\n",
        "area_sqrt = sqrt_transformer.fit_transform(area)\n",
        "\n",
        "print(\"\\nSquare root transformation:\")\n",
        "print(f\"Original: {area.flatten()}\")\n",
        "print(f\"Sqrt:     {area_sqrt.flatten()}\")\n",
        "\n",
        "# Custom function\n",
        "def price_category(X):\n",
        "    \"\"\"Categorize price: 0=low, 1=medium, 2=high\"\"\"\n",
        "    return np.select(\n",
        "        [X < 250000, X < 400000],\n",
        "        [0, 1],\n",
        "        default=2\n",
        "    ).reshape(-1, 1)\n",
        "\n",
        "category_transformer = FunctionTransformer(price_category, validate=True)\n",
        "price_categories = category_transformer.fit_transform(prices)\n",
        "\n",
        "print(\"\\nCustom categorization:\")\n",
        "print(f\"Prices:     {prices.flatten()}\")\n",
        "print(f\"Categories: {price_categories.flatten()}\")\n",
        "print(\"  0 = Low (<$250k), 1 = Medium (<$400k), 2 = High\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feature-interactions",
      "metadata": {},
      "source": [
        "## 5. Feature Interactions\n",
        "\n",
        "**Definition**: Combining features to capture relationships\n",
        "\n",
        "**Examples**:\n",
        "- BMI = weight / height\u00b2\n",
        "- Debt-to-Income Ratio = debt / income\n",
        "- Price per unit = total_price / quantity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "interactions",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Customer analytics\n",
        "customer_data = pd.DataFrame({\n",
        "    'total_purchases': [10, 25, 50, 100, 5],\n",
        "    'total_spent': [500, 1200, 2500, 5000, 200],\n",
        "    'days_active': [30, 90, 180, 365, 15],\n",
        "    'returns': [1, 2, 3, 5, 1]\n",
        "})\n",
        "\n",
        "print(\"Original Customer Data:\")\n",
        "print(customer_data)\n",
        "\n",
        "# Create interaction features\n",
        "customer_data['avg_purchase_value'] = (\n",
        "    customer_data['total_spent'] / customer_data['total_purchases']\n",
        ")\n",
        "customer_data['purchases_per_day'] = (\n",
        "    customer_data['total_purchases'] / customer_data['days_active']\n",
        ")\n",
        "customer_data['return_rate'] = (\n",
        "    customer_data['returns'] / customer_data['total_purchases']\n",
        ")\n",
        "customer_data['spending_per_day'] = (\n",
        "    customer_data['total_spent'] / customer_data['days_active']\n",
        ")\n",
        "\n",
        "print(\"\\nWith Interaction Features:\")\n",
        "print(customer_data.round(2))\n",
        "\n",
        "print(\"\\n\ud83d\udca1 These features reveal:\")\n",
        "print(\"  - avg_purchase_value: Customer spending power\")\n",
        "print(\"  - purchases_per_day: Engagement level\")\n",
        "print(\"  - return_rate: Product satisfaction\")\n",
        "print(\"  - spending_per_day: Revenue potential\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Feature Engineering Best Practices\n",
        "\n",
        "### 1. Domain Knowledge\n",
        "- Understand your data and business context\n",
        "- Talk to domain experts\n",
        "- Research common features in your field\n",
        "\n",
        "### 2. Iterative Process\n",
        "```\n",
        "1. Create features\n",
        "2. Train model\n",
        "3. Check feature importance\n",
        "4. Remove useless features\n",
        "5. Create new features\n",
        "6. Repeat\n",
        "```\n",
        "\n",
        "### 3. Avoid Data Leakage\n",
        "\u274c Don't use future information  \n",
        "\u274c Don't use target-derived features  \n",
        "\u2705 Only use past/current information  \n",
        "\n",
        "### 4. Feature Scaling\n",
        "After creating features, remember to scale!\n",
        "\n",
        "### 5. Feature Selection\n",
        "More features \u2260 Better model\n",
        "- Remove highly correlated features\n",
        "- Use feature importance\n",
        "- Try feature selection methods\n",
        "\n",
        "## Quick Reference\n",
        "\n",
        "```python\n",
        "# Polynomial Features\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Binning\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "binner = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
        "X_binned = binner.fit_transform(X)\n",
        "\n",
        "# Date Features (Pandas)\n",
        "df['year'] = df['date'].dt.year\n",
        "df['month'] = df['date'].dt.month\n",
        "df['day_of_week'] = df['date'].dt.dayofweek\n",
        "df['is_weekend'] = df['date'].dt.dayofweek.isin([5, 6])\n",
        "\n",
        "# Custom Transformation\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "log_transformer = FunctionTransformer(np.log1p)\n",
        "X_log = log_transformer.fit_transform(X)\n",
        "```\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **Polynomial Features**: Capture non-linearity and interactions\n",
        "2. **Binning**: Convert continuous to categorical, reduce noise\n",
        "3. **Date Features**: Extract temporal patterns (day, month, weekend, etc.)\n",
        "4. **Cyclical Encoding**: Use sin/cos for circular features (months, days)\n",
        "5. **Custom Features**: Create domain-specific features\n",
        "6. **Interactions**: Combine features to reveal relationships\n",
        "7. **Always validate**: Check if engineered features improve performance"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}