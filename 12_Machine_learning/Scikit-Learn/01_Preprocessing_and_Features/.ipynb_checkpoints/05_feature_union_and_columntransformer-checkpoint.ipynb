{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# FeatureUnion and ColumnTransformer\n",
        "\n",
        "## Overview\n",
        "\n",
        "Real-world datasets often have **mixed data types**:\n",
        "- Numerical features (age, price, ratings)\n",
        "- Categorical features (gender, city, product type)\n",
        "- Text features (reviews, descriptions)\n",
        "- Datetime features (timestamps)\n",
        "\n",
        "Sklearn provides two powerful tools for handling heterogeneous data:\n",
        "\n",
        "### FeatureUnion (Legacy)\n",
        "- Applies multiple transformers in parallel\n",
        "- Concatenates results horizontally\n",
        "- Works on the **same input data**\n",
        "- Useful for combining different feature extraction methods\n",
        "\n",
        "### ColumnTransformer (Modern, Recommended)\n",
        "- Applies different transformers to **different columns**\n",
        "- Perfect for mixed data types\n",
        "- More intuitive and flexible\n",
        "- Introduced in sklearn 0.20+\n",
        "\n",
        "**Rule of Thumb**: Use **ColumnTransformer** for column-specific preprocessing!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
        "\n",
        "print(\"\u2713 Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sample-data",
      "metadata": {},
      "source": [
        "## Sample Dataset with Mixed Types\n",
        "\n",
        "Let's create a realistic dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-data",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample employee dataset\n",
        "np.random.seed(42)\n",
        "\n",
        "n_samples = 1000\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'age': np.random.randint(22, 65, n_samples),\n",
        "    'experience_years': np.random.randint(0, 40, n_samples),\n",
        "    'department': np.random.choice(['Engineering', 'Sales', 'Marketing', 'HR'], n_samples),\n",
        "    'education': np.random.choice(['Bachelor', 'Master', 'PhD'], n_samples),\n",
        "    'skills': [\n",
        "        np.random.choice(['Python Java SQL', 'Leadership Communication', \n",
        "                         'Marketing Analytics SEO', 'HR Management Recruiting'], 1)[0] \n",
        "        for _ in range(n_samples)\n",
        "    ],\n",
        "    'rating': np.random.uniform(1, 5, n_samples),\n",
        "    'remote_work': np.random.choice(['Yes', 'No'], n_samples),\n",
        "})\n",
        "\n",
        "# Target: High performer (binary classification)\n",
        "data['high_performer'] = (\n",
        "    (data['rating'] > 3.5) & \n",
        "    (data['experience_years'] > 5)\n",
        ").astype(int)\n",
        "\n",
        "print(\"Employee Dataset:\")\n",
        "print(data.head(10))\n",
        "print(f\"\\nShape: {data.shape}\")\n",
        "print(f\"Target distribution: {data['high_performer'].value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "data-types",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify column types\n",
        "numeric_features = ['age', 'experience_years', 'rating']\n",
        "categorical_features = ['department', 'education', 'remote_work']\n",
        "text_features = ['skills']\n",
        "\n",
        "print(\"Feature Types:\")\n",
        "print(f\"  Numeric: {numeric_features}\")\n",
        "print(f\"  Categorical: {categorical_features}\")\n",
        "print(f\"  Text: {text_features}\")\n",
        "print(f\"\\nData types:\\n{data.dtypes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "featureunion",
      "metadata": {},
      "source": [
        "## 1. FeatureUnion - Combining Feature Extractors\n",
        "\n",
        "**Use Case**: Apply multiple transformations to the **same input** and combine results.\n",
        "\n",
        "Example: Extract both unigrams and bigrams from text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "featureunion-basic",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Combine unigrams and bigrams from skills column\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Custom transformer to select text column\n",
        "class TextSelector(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, column):\n",
        "        self.column = column\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        return X[self.column]\n",
        "\n",
        "# Create FeatureUnion\n",
        "text_features_union = FeatureUnion([\n",
        "    ('unigrams', Pipeline([\n",
        "        ('selector', TextSelector('skills')),\n",
        "        ('vectorizer', CountVectorizer(ngram_range=(1, 1), max_features=50))\n",
        "    ])),\n",
        "    ('bigrams', Pipeline([\n",
        "        ('selector', TextSelector('skills')),\n",
        "        ('vectorizer', CountVectorizer(ngram_range=(2, 2), max_features=50))\n",
        "    ]))\n",
        "])\n",
        "\n",
        "# Transform\n",
        "X_union = text_features_union.fit_transform(data)\n",
        "\n",
        "print(\"FeatureUnion Results:\")\n",
        "print(f\"Output shape: {X_union.shape}\")\n",
        "print(f\"  - Unigrams: 50 features\")\n",
        "print(f\"  - Bigrams: 50 features\")\n",
        "print(f\"  - Total: {X_union.shape[1]} features (concatenated)\")\n",
        "print(f\"\\nMatrix type: {type(X_union)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "columntransformer-intro",
      "metadata": {},
      "source": [
        "## 2. ColumnTransformer - The Modern Approach\n",
        "\n",
        "**Best for**: Applying different transformations to different column groups.\n",
        "\n",
        "### Basic Syntax\n",
        "\n",
        "```python\n",
        "ColumnTransformer([\n",
        "    ('name', transformer, columns),\n",
        "    ('name2', transformer2, columns2),\n",
        "], remainder='drop')  # or 'passthrough'\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "columntransformer-basic",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features)\n",
        "    ],\n",
        "    remainder='drop'  # Drop text column for now\n",
        ")\n",
        "\n",
        "# Prepare data\n",
        "X = data.drop('high_performer', axis=1)\n",
        "y = data['high_performer']\n",
        "\n",
        "# Transform\n",
        "X_transformed = preprocessor.fit_transform(X)\n",
        "\n",
        "print(\"ColumnTransformer Results:\")\n",
        "print(f\"Original shape: {X.shape}\")\n",
        "print(f\"Transformed shape: {X_transformed.shape}\")\n",
        "print(f\"\\nBreakdown:\")\n",
        "print(f\"  - Numeric (scaled): {len(numeric_features)} features\")\n",
        "print(f\"  - Categorical (one-hot): {X_transformed.shape[1] - len(numeric_features)} features\")\n",
        "print(f\"  - Text (dropped): 0 features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "get-feature-names",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature names after transformation\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "print(f\"Feature names after transformation ({len(feature_names)} total):\")\n",
        "print(feature_names[:20])  # Show first 20\n",
        "print(\"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "complete-pipeline",
      "metadata": {},
      "source": [
        "## 3. Complete Pipeline: All Feature Types\n",
        "\n",
        "Let's process **numeric, categorical, AND text** features together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "complete-preprocessor",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete preprocessing pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "complete_preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        # Numeric: Impute missing + scale\n",
        "        ('num', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), numeric_features),\n",
        "        \n",
        "        # Categorical: Impute + one-hot encode\n",
        "        ('cat', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('encoder', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
        "        ]), categorical_features),\n",
        "        \n",
        "        # Text: TF-IDF vectorization\n",
        "        ('text', TfidfVectorizer(max_features=50, stop_words='english'), 'skills')\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# Transform data\n",
        "X_complete = complete_preprocessor.fit_transform(X)\n",
        "\n",
        "print(\"Complete Preprocessing Results:\")\n",
        "print(f\"Original columns: {X.shape[1]}\")\n",
        "print(f\"Transformed features: {X_complete.shape[1]}\")\n",
        "print(f\"\\nFeature breakdown:\")\n",
        "print(f\"  - Numeric (scaled): 3 features\")\n",
        "print(f\"  - Categorical (one-hot): ~8 features\")\n",
        "print(f\"  - Text (TF-IDF): 50 features\")\n",
        "print(f\"  - Total: {X_complete.shape[1]} features\")\n",
        "print(f\"\\nOutput type: {type(X_complete)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "full-pipeline",
      "metadata": {},
      "source": [
        "## 4. Full ML Pipeline: Preprocessing + Model\n",
        "\n",
        "Combine preprocessing and modeling in one pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ml-pipeline",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Test samples: {X_test.shape[0]}\")\n",
        "\n",
        "# Create full pipeline\n",
        "full_pipeline = Pipeline([\n",
        "    ('preprocessor', complete_preprocessor),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "# Train (preprocessor + model in one step!)\n",
        "print(\"\\nTraining pipeline...\")\n",
        "full_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = full_pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cross-validation",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-validation with full pipeline\n",
        "cv_scores = cross_val_score(\n",
        "    full_pipeline, X, y, cv=5, scoring='accuracy'\n",
        ")\n",
        "\n",
        "print(\"5-Fold Cross-Validation Scores:\")\n",
        "print(f\"  Scores: {cv_scores}\")\n",
        "print(f\"  Mean: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "real-dataset",
      "metadata": {},
      "source": [
        "## 5. Real-World Example: Titanic Dataset\n",
        "\n",
        "Classic dataset with mixed types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "titanic-load",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Titanic dataset\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "print(\"Loading Titanic dataset...\")\n",
        "titanic = fetch_openml('titanic', version=1, as_frame=True, parser='auto')\n",
        "df = titanic.frame\n",
        "\n",
        "# Clean and prepare\n",
        "df = df[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'survived']].copy()\n",
        "df['survived'] = df['survived'].astype(int)\n",
        "\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
        "print(f\"\\nSurvival rate: {df['survived'].mean():.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "titanic-preprocessing",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define features by type\n",
        "numeric_cols = ['age', 'sibsp', 'parch', 'fare']\n",
        "categorical_cols = ['pclass', 'sex', 'embarked']\n",
        "\n",
        "# Create preprocessor\n",
        "titanic_preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='median')),  # Handle missing values\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), numeric_cols),\n",
        "        \n",
        "        ('cat', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "            ('encoder', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
        "        ]), categorical_cols)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# Prepare data\n",
        "X_titanic = df.drop('survived', axis=1)\n",
        "y_titanic = df['survived']\n",
        "\n",
        "# Split\n",
        "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(\n",
        "    X_titanic, y_titanic, test_size=0.2, random_state=42, stratify=y_titanic\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train_t.shape}\")\n",
        "print(f\"Test set: {X_test_t.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "titanic-models",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare multiple models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "}\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TITANIC SURVIVAL PREDICTION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Create pipeline\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', titanic_preprocessor),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "    \n",
        "    # Train\n",
        "    pipeline.fit(X_train_t, y_train_t)\n",
        "    \n",
        "    # Predict\n",
        "    y_pred = pipeline.predict(X_test_t)\n",
        "    \n",
        "    # Evaluate\n",
        "    accuracy = accuracy_score(y_test_t, y_pred)\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    \n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(pipeline, X_titanic, y_titanic, cv=5)\n",
        "    print(f\"  CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "advanced-columntransformer",
      "metadata": {},
      "source": [
        "## 6. Advanced ColumnTransformer Techniques\n",
        "\n",
        "### Using `make_column_transformer` (Simpler Syntax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "make-column-transformer",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "# Simpler syntax (no need for tuples with names)\n",
        "preprocessor_simple = make_column_transformer(\n",
        "    (StandardScaler(), numeric_cols),\n",
        "    (OneHotEncoder(drop='first', sparse_output=False), categorical_cols),\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "X_simple = preprocessor_simple.fit_transform(X_titanic)\n",
        "print(f\"Transformed shape: {X_simple.shape}\")\n",
        "print(f\"Feature names: {preprocessor_simple.get_feature_names_out()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "remainder",
      "metadata": {},
      "source": [
        "### Using `remainder` Parameter\n",
        "\n",
        "- `remainder='drop'`: Drop untransformed columns (default)\n",
        "- `remainder='passthrough'`: Keep untransformed columns as-is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "remainder-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare remainder options\n",
        "preprocessor_drop = ColumnTransformer(\n",
        "    [('num', StandardScaler(), numeric_cols)],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "preprocessor_pass = ColumnTransformer(\n",
        "    [('num', StandardScaler(), numeric_cols)],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "X_drop = preprocessor_drop.fit_transform(X_titanic)\n",
        "X_pass = preprocessor_pass.fit_transform(X_titanic)\n",
        "\n",
        "print(\"Remainder Comparison:\")\n",
        "print(f\"  Original: {X_titanic.shape[1]} columns\")\n",
        "print(f\"  remainder='drop': {X_drop.shape[1]} columns (only scaled numeric)\")\n",
        "print(f\"  remainder='passthrough': {X_pass.shape[1]} columns (numeric + original categorical)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "column-selectors",
      "metadata": {},
      "source": [
        "### Using Column Selectors (Advanced)\n",
        "\n",
        "Select columns by data type automatically:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "column-selector",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.compose import make_column_selector\n",
        "\n",
        "# Automatic column selection by dtype\n",
        "preprocessor_auto = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), make_column_selector(dtype_include=np.number)),\n",
        "        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), \n",
        "         make_column_selector(dtype_include=object))\n",
        "    ]\n",
        ")\n",
        "\n",
        "X_auto = preprocessor_auto.fit_transform(X_titanic)\n",
        "\n",
        "print(\"Automatic Column Selection:\")\n",
        "print(f\"  Input shape: {X_titanic.shape}\")\n",
        "print(f\"  Output shape: {X_auto.shape}\")\n",
        "print(f\"\\n  Numeric columns (auto-detected): {X_titanic.select_dtypes(include=np.number).columns.tolist()}\")\n",
        "print(f\"  Object columns (auto-detected): {X_titanic.select_dtypes(include=object).columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "custom-transformer",
      "metadata": {},
      "source": [
        "## 7. Creating Custom Transformers\n",
        "\n",
        "Build your own transformers for specialized preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "custom-transformer-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Custom transformer: Log transformation\n",
        "class LogTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, offset=1):\n",
        "        self.offset = offset\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        return np.log(X + self.offset)\n",
        "\n",
        "# Custom transformer: Age binning\n",
        "class AgeBinner(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "        X_copy['age_group'] = pd.cut(\n",
        "            X_copy['age'], \n",
        "            bins=[0, 18, 30, 50, 100],\n",
        "            labels=['child', 'young', 'adult', 'senior']\n",
        "        )\n",
        "        return X_copy\n",
        "\n",
        "# Use custom transformers in pipeline\n",
        "custom_preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('log_fare', LogTransformer(), ['fare']),\n",
        "        ('standard_age', StandardScaler(), ['age']),\n",
        "        ('cat', OneHotEncoder(sparse_output=False), ['sex', 'embarked'])\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# Test custom preprocessor\n",
        "X_custom = custom_preprocessor.fit_transform(X_titanic.fillna(0))\n",
        "print(f\"Custom preprocessing output shape: {X_custom.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comparison",
      "metadata": {},
      "source": [
        "## 8. FeatureUnion vs ColumnTransformer Comparison\n",
        "\n",
        "### When to Use Each\n",
        "\n",
        "| Scenario | Use |\n",
        "|----------|-----|\n",
        "| Different transformations on **different columns** | **ColumnTransformer** |\n",
        "| Multiple transformations on **same columns** | **FeatureUnion** |\n",
        "| Mixed data types (numeric, categorical, text) | **ColumnTransformer** |\n",
        "| Extracting different text features (unigrams + bigrams) | **FeatureUnion** |\n",
        "| Modern, clean syntax | **ColumnTransformer** |\n",
        "| Need to select specific columns | **ColumnTransformer** |\n",
        "\n",
        "### Example Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comparison-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample text data\n",
        "text_data = pd.DataFrame({\n",
        "    'review': [\n",
        "        'great product amazing quality',\n",
        "        'terrible waste of money',\n",
        "        'good value for price'\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"Scenario: Extract both word counts AND character n-grams from same text\\n\")\n",
        "\n",
        "# FeatureUnion: Multiple transformations on SAME column\n",
        "text_union = FeatureUnion([\n",
        "    ('word_count', CountVectorizer()),\n",
        "    ('char_ngrams', CountVectorizer(analyzer='char', ngram_range=(2, 3)))\n",
        "])\n",
        "\n",
        "X_text = text_union.fit_transform(text_data['review'])\n",
        "print(f\"FeatureUnion output: {X_text.shape}\")\n",
        "print(\"  \u2192 Combines word-level and character-level features from same text\\n\")\n",
        "\n",
        "# ColumnTransformer: Different transformations on DIFFERENT columns\n",
        "mixed_data = pd.DataFrame({\n",
        "    'price': [100, 200, 150],\n",
        "    'category': ['A', 'B', 'A'],\n",
        "    'description': ['great product', 'terrible item', 'good value']\n",
        "})\n",
        "\n",
        "col_trans = ColumnTransformer([\n",
        "    ('scale_price', StandardScaler(), ['price']),\n",
        "    ('encode_cat', OneHotEncoder(sparse_output=False), ['category']),\n",
        "    ('vectorize_text', CountVectorizer(), 'description')\n",
        "])\n",
        "\n",
        "X_mixed = col_trans.fit_transform(mixed_data)\n",
        "print(f\"ColumnTransformer output: {X_mixed.shape}\")\n",
        "print(\"  \u2192 Different preprocessing for each column type\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "best-practices",
      "metadata": {},
      "source": [
        "## Best Practices\n",
        "\n",
        "### 1. Always Use Pipelines\n",
        "```python\n",
        "# \u2713 GOOD: Everything in one pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', ColumnTransformer(...)),\n",
        "    ('model', RandomForestClassifier())\n",
        "])\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# \u2717 BAD: Manual preprocessing (risk of data leakage!)\n",
        "X_scaled = scaler.fit_transform(X_train)  # DON'T DO THIS\n",
        "```\n",
        "\n",
        "### 2. Handle Missing Values\n",
        "```python\n",
        "Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),  # Always impute first\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "```\n",
        "\n",
        "### 3. Use `handle_unknown='ignore'` for OneHotEncoder\n",
        "```python\n",
        "OneHotEncoder(drop='first', handle_unknown='ignore')  # Prevents errors on new categories\n",
        "```\n",
        "\n",
        "### 4. Set `sparse_output=False` When Needed\n",
        "```python\n",
        "# If mixing sparse and dense transformers:\n",
        "OneHotEncoder(sparse_output=False)  # Returns dense array\n",
        "```\n",
        "\n",
        "### 5. Use `make_column_selector` for Dynamic Selection\n",
        "```python\n",
        "ColumnTransformer([\n",
        "    ('num', StandardScaler(), make_column_selector(dtype_include=np.number)),\n",
        "    ('cat', OneHotEncoder(), make_column_selector(dtype_include=object))\n",
        "])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### ColumnTransformer (Modern, Recommended)\n",
        "- \u2713 Apply different transformations to different columns\n",
        "- \u2713 Perfect for mixed data types\n",
        "- \u2713 Clean, intuitive syntax\n",
        "- \u2713 Integrates seamlessly with Pipeline\n",
        "- \u2713 Prevents data leakage\n",
        "\n",
        "### FeatureUnion (Legacy, Specific Use Cases)\n",
        "- \u2713 Multiple transformations on same input\n",
        "- \u2713 Combining different feature extraction methods\n",
        "- \u2717 Less intuitive for mixed data types\n",
        "- \u2192 Use ColumnTransformer instead for most cases\n",
        "\n",
        "### Complete Workflow Template\n",
        "\n",
        "```python\n",
        "# 1. Define column groups\n",
        "numeric_cols = ['age', 'income']\n",
        "categorical_cols = ['gender', 'city']\n",
        "text_cols = 'description'\n",
        "\n",
        "# 2. Create preprocessor\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
        "    ]), categorical_cols),\n",
        "    ('text', TfidfVectorizer(max_features=100), text_cols)\n",
        "])\n",
        "\n",
        "# 3. Create full pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# 4. Train and evaluate\n",
        "pipeline.fit(X_train, y_train)\n",
        "pipeline.score(X_test, y_test)\n",
        "```\n",
        "\n",
        "This approach:\n",
        "- Prevents data leakage\n",
        "- Makes code reproducible\n",
        "- Enables easy hyperparameter tuning\n",
        "- Simplifies deployment"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}