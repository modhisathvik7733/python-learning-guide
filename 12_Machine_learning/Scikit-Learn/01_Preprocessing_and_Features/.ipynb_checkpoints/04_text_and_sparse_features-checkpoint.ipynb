{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Text and Sparse Features in Scikit-learn\n",
        "\n",
        "## Overview\n",
        "\n",
        "Text data cannot be directly used by ML algorithms - it must be converted to numerical features. Sklearn provides powerful tools for text vectorization:\n",
        "\n",
        "- **CountVectorizer**: Converts text to word count matrix\n",
        "- **TfidfVectorizer**: Converts text to TF-IDF weighted features\n",
        "- **HashingVectorizer**: Fast, memory-efficient hashing approach\n",
        "\n",
        "These create **sparse matrices** - efficient representations where most values are zero.\n",
        "\n",
        "## Why Sparse Matrices?\n",
        "\n",
        "Text data typically has:\n",
        "- **Large vocabulary**: Thousands of unique words\n",
        "- **Sparse representation**: Most documents use only a small subset of vocabulary\n",
        "- **Memory efficiency**: Sparse matrices store only non-zero values\n",
        "\n",
        "Example: \"The cat sat\" vs 10,000 word vocabulary = 99.97% zeros!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup and Sample Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "from scipy.sparse import csr_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample text corpus\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "\n",
        "print(\"Sample Corpus:\")\n",
        "for i, doc in enumerate(corpus, 1):\n",
        "    print(f\"{i}. {doc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "countvectorizer",
      "metadata": {},
      "source": [
        "## 1. CountVectorizer - Bag of Words\n",
        "\n",
        "Converts text to a matrix of token counts (word frequency).\n",
        "\n",
        "**Key Parameters:**\n",
        "- `max_features`: Limit vocabulary size to top N words\n",
        "- `min_df`: Ignore words appearing in fewer than N documents\n",
        "- `max_df`: Ignore words appearing in more than N% of documents\n",
        "- `ngram_range`: Include n-grams (1,1)=unigrams, (1,2)=unigrams+bigrams\n",
        "- `stop_words`: Remove common words ('english' or custom list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "count-basic",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic CountVectorizer\n",
        "count_vec = CountVectorizer()\n",
        "X_counts = count_vec.fit_transform(corpus)\n",
        "\n",
        "print(\"CountVectorizer Results:\")\n",
        "print(f\"Shape: {X_counts.shape} (documents x vocabulary)\")\n",
        "print(f\"Vocabulary size: {len(count_vec.vocabulary_)}\")\n",
        "print(f\"Sparse matrix density: {X_counts.nnz / (X_counts.shape[0] * X_counts.shape[1]):.2%}\")\n",
        "print(f\"\\nFeature names (vocabulary): {count_vec.get_feature_names_out()}\")\n",
        "\n",
        "# Convert to dense for visualization\n",
        "df_counts = pd.DataFrame(\n",
        "    X_counts.toarray(),\n",
        "    columns=count_vec.get_feature_names_out(),\n",
        "    index=[f\"Doc {i+1}\" for i in range(len(corpus))]\n",
        ")\n",
        "print(\"\\nWord Count Matrix:\")\n",
        "print(df_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "count-advanced",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced CountVectorizer with parameters\n",
        "count_vec_adv = CountVectorizer(\n",
        "    max_features=10,        # Keep only top 10 words\n",
        "    min_df=1,               # Word must appear in at least 1 doc\n",
        "    max_df=0.8,             # Ignore words in >80% of docs\n",
        "    ngram_range=(1, 2),     # Include unigrams and bigrams\n",
        "    stop_words='english'    # Remove English stop words\n",
        ")\n",
        "\n",
        "X_counts_adv = count_vec_adv.fit_transform(corpus)\n",
        "\n",
        "print(\"Advanced CountVectorizer (with bigrams, stop words removed):\")\n",
        "print(f\"Vocabulary: {count_vec_adv.get_feature_names_out()}\")\n",
        "print(f\"\\nShape: {X_counts_adv.shape}\")\n",
        "\n",
        "df_counts_adv = pd.DataFrame(\n",
        "    X_counts_adv.toarray(),\n",
        "    columns=count_vec_adv.get_feature_names_out(),\n",
        "    index=[f\"Doc {i+1}\" for i in range(len(corpus))]\n",
        ")\n",
        "print(\"\\nWord Count Matrix (with n-grams):\")\n",
        "print(df_counts_adv)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tfidf",
      "metadata": {},
      "source": [
        "## 2. TfidfVectorizer - Term Frequency-Inverse Document Frequency\n",
        "\n",
        "**TF-IDF** weights words by their importance:\n",
        "- **TF (Term Frequency)**: How often word appears in document\n",
        "- **IDF (Inverse Document Frequency)**: How rare the word is across all documents\n",
        "\n",
        "**Formula:**\n",
        "\\[\n",
        "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "\\text{IDF}(t) = \\log\\left(\\frac{1 + n}{1 + \\text{df}(t)}\\right) + 1\n",
        "\\]\n",
        "\n",
        "**Intuition:**\n",
        "- Common words (\"the\", \"is\") \u2192 Low TF-IDF\n",
        "- Rare, distinctive words \u2192 High TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tfidf-basic",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic TfidfVectorizer\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vec.fit_transform(corpus)\n",
        "\n",
        "print(\"TfidfVectorizer Results:\")\n",
        "print(f\"Shape: {X_tfidf.shape}\")\n",
        "print(f\"Vocabulary: {tfidf_vec.get_feature_names_out()}\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_tfidf = pd.DataFrame(\n",
        "    X_tfidf.toarray(),\n",
        "    columns=tfidf_vec.get_feature_names_out(),\n",
        "    index=[f\"Doc {i+1}\" for i in range(len(corpus))]\n",
        ")\n",
        "print(\"\\nTF-IDF Matrix (values are weighted):\")\n",
        "print(df_tfidf.round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tfidf-comparison",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare Count vs TF-IDF for a specific word\n",
        "word = 'document'\n",
        "\n",
        "if word in count_vec.vocabulary_:\n",
        "    word_idx = count_vec.vocabulary_[word]\n",
        "    \n",
        "    print(f\"Comparison for word: '{word}'\")\n",
        "    print(\"\\nDocument | Count | TF-IDF\")\n",
        "    print(\"-\" * 35)\n",
        "    for i in range(len(corpus)):\n",
        "        count_val = X_counts[i, word_idx]\n",
        "        tfidf_val = X_tfidf[i, word_idx]\n",
        "        print(f\"Doc {i+1}    |   {count_val}   | {tfidf_val:.4f}\")\n",
        "    \n",
        "    print(\"\\n\ud83d\udca1 Notice: 'document' appears frequently \u2192 lower TF-IDF weights\")\n",
        "    print(\"   Rare words get higher weights!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tfidf-advanced",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced TfidfVectorizer\n",
        "tfidf_vec_adv = TfidfVectorizer(\n",
        "    max_features=50,\n",
        "    min_df=1,\n",
        "    max_df=0.8,\n",
        "    ngram_range=(1, 2),\n",
        "    sublinear_tf=True,      # Apply log scaling to TF\n",
        "    use_idf=True,           # Enable IDF weighting\n",
        "    smooth_idf=True,        # Smooth IDF weights\n",
        "    norm='l2'               # L2 normalization\n",
        ")\n",
        "\n",
        "X_tfidf_adv = tfidf_vec_adv.fit_transform(corpus)\n",
        "\n",
        "print(\"Advanced TF-IDF Configuration:\")\n",
        "print(f\"Features: {tfidf_vec_adv.get_feature_names_out()}\")\n",
        "print(f\"\\nIDF values (higher = rarer word):\")\n",
        "idf_df = pd.DataFrame({\n",
        "    'term': tfidf_vec_adv.get_feature_names_out(),\n",
        "    'idf': tfidf_vec_adv.idf_\n",
        "}).sort_values('idf', ascending=False)\n",
        "print(idf_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hashing",
      "metadata": {},
      "source": [
        "## 3. HashingVectorizer - Memory-Efficient Alternative\n",
        "\n",
        "**Advantages:**\n",
        "- No vocabulary storage (memory efficient)\n",
        "- Fast - no need to fit vocabulary\n",
        "- Scalable to large datasets\n",
        "- Supports online/streaming learning\n",
        "\n",
        "**Disadvantages:**\n",
        "- Hash collisions (multiple words \u2192 same feature)\n",
        "- No inverse transform (can't retrieve original words)\n",
        "- Fixed feature space size\n",
        "\n",
        "**When to use:**\n",
        "- Very large datasets (millions of documents)\n",
        "- Online/streaming scenarios\n",
        "- Memory constraints\n",
        "- Don't need to interpret features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hashing-basic",
      "metadata": {},
      "outputs": [],
      "source": [
        "# HashingVectorizer\n",
        "hash_vec = HashingVectorizer(\n",
        "    n_features=20,          # Hash space size (trade-off: collisions vs memory)\n",
        "    alternate_sign=True     # Reduce impact of collisions\n",
        ")\n",
        "\n",
        "X_hash = hash_vec.transform(corpus)  # No fit() needed!\n",
        "\n",
        "print(\"HashingVectorizer Results:\")\n",
        "print(f\"Shape: {X_hash.shape}\")\n",
        "print(f\"No vocabulary stored (memory efficient!)\")\n",
        "print(f\"\\nHash Matrix (first 3 docs, first 10 features):\")\n",
        "print(X_hash[:3, :10].toarray())\n",
        "\n",
        "print(\"\\n\u26a0\ufe0f Note: Cannot retrieve feature names (no vocabulary)\")\n",
        "print(\"\u2713 But much faster and more memory efficient!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "real-dataset",
      "metadata": {},
      "source": [
        "## 4. Real-World Example: 20 Newsgroups Classification\n",
        "\n",
        "Let's classify news articles using text vectorization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "newsgroups-load",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load subset of categories\n",
        "categories = ['alt.atheism', 'sci.space', 'rec.sport.baseball', 'comp.graphics']\n",
        "\n",
        "print(\"Loading 20 Newsgroups dataset...\")\n",
        "newsgroups_train = fetch_20newsgroups(\n",
        "    subset='train',\n",
        "    categories=categories,\n",
        "    remove=('headers', 'footers', 'quotes'),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "newsgroups_test = fetch_20newsgroups(\n",
        "    subset='test',\n",
        "    categories=categories,\n",
        "    remove=('headers', 'footers', 'quotes'),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining samples: {len(newsgroups_train.data)}\")\n",
        "print(f\"Test samples: {len(newsgroups_test.data)}\")\n",
        "print(f\"Categories: {newsgroups_train.target_names}\")\n",
        "\n",
        "# Show sample\n",
        "print(f\"\\nSample document (category: {newsgroups_train.target_names[newsgroups_train.target[0]]}):\")\n",
        "print(newsgroups_train.data[0][:300] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "newsgroups-countvec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 1: CountVectorizer + Naive Bayes\n",
        "print(\"=\" * 60)\n",
        "print(\"Method 1: CountVectorizer + Multinomial Naive Bayes\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "count_vectorizer = CountVectorizer(\n",
        "    max_features=5000,\n",
        "    min_df=5,\n",
        "    max_df=0.7,\n",
        "    stop_words='english'\n",
        ")\n",
        "\n",
        "X_train_counts = count_vectorizer.fit_transform(newsgroups_train.data)\n",
        "X_test_counts = count_vectorizer.transform(newsgroups_test.data)\n",
        "\n",
        "print(f\"Vocabulary size: {len(count_vectorizer.vocabulary_)}\")\n",
        "print(f\"Training matrix shape: {X_train_counts.shape}\")\n",
        "print(f\"Sparsity: {(1 - X_train_counts.nnz / (X_train_counts.shape[0] * X_train_counts.shape[1])):.2%}\")\n",
        "\n",
        "# Train classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_counts, newsgroups_train.target)\n",
        "\n",
        "# Predict\n",
        "y_pred_nb = nb_classifier.predict(X_test_counts)\n",
        "accuracy_nb = accuracy_score(newsgroups_test.target, y_pred_nb)\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy_nb:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(newsgroups_test.target, y_pred_nb, \n",
        "                          target_names=newsgroups_test.target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "newsgroups-tfidf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 2: TF-IDF + Logistic Regression\n",
        "print(\"=\" * 60)\n",
        "print(\"Method 2: TF-IDF + Logistic Regression\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    min_df=5,\n",
        "    max_df=0.7,\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 2),\n",
        "    sublinear_tf=True\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(newsgroups_train.data)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(newsgroups_test.data)\n",
        "\n",
        "print(f\"Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
        "print(f\"Training matrix shape: {X_train_tfidf.shape}\")\n",
        "\n",
        "# Train classifier\n",
        "lr_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_classifier.fit(X_train_tfidf, newsgroups_train.target)\n",
        "\n",
        "# Predict\n",
        "y_pred_lr = lr_classifier.predict(X_test_tfidf)\n",
        "accuracy_lr = accuracy_score(newsgroups_test.target, y_pred_lr)\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy_lr:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(newsgroups_test.target, y_pred_lr,\n",
        "                          target_names=newsgroups_test.target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "newsgroups-hashing",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 3: HashingVectorizer + Logistic Regression\n",
        "print(\"=\" * 60)\n",
        "print(\"Method 3: HashingVectorizer + Logistic Regression\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "hash_vectorizer = HashingVectorizer(\n",
        "    n_features=2**12,  # 4096 features\n",
        "    alternate_sign=True,\n",
        "    stop_words='english'\n",
        ")\n",
        "\n",
        "X_train_hash = hash_vectorizer.transform(newsgroups_train.data)\n",
        "X_test_hash = hash_vectorizer.transform(newsgroups_test.data)\n",
        "\n",
        "print(f\"Hash space size: {X_train_hash.shape[1]}\")\n",
        "print(f\"Training matrix shape: {X_train_hash.shape}\")\n",
        "print(\"No vocabulary stored (instant processing!)\")\n",
        "\n",
        "# Train classifier\n",
        "lr_hash = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_hash.fit(X_train_hash, newsgroups_train.target)\n",
        "\n",
        "# Predict\n",
        "y_pred_hash = lr_hash.predict(X_test_hash)\n",
        "accuracy_hash = accuracy_score(newsgroups_test.target, y_pred_hash)\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy_hash:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(newsgroups_test.target, y_pred_hash,\n",
        "                          target_names=newsgroups_test.target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comparison",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all methods\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"COMPARISON SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    'Method': ['CountVec + NaiveBayes', 'TF-IDF + LogReg', 'Hashing + LogReg'],\n",
        "    'Accuracy': [accuracy_nb, accuracy_lr, accuracy_hash],\n",
        "    'Features': [X_train_counts.shape[1], X_train_tfidf.shape[1], X_train_hash.shape[1]]\n",
        "})\n",
        "\n",
        "print(results.to_string(index=False))\n",
        "print(\"\\n\ud83d\udca1 Insights:\")\n",
        "print(\"  - TF-IDF often performs best for text classification\")\n",
        "print(\"  - HashingVectorizer trades slight accuracy for speed/memory\")\n",
        "print(\"  - CountVectorizer + Naive Bayes is fast and interpretable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feature-importance",
      "metadata": {},
      "source": [
        "## 5. Analyzing Important Features\n",
        "\n",
        "Extract most important words per category:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "top-features",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get top features for TF-IDF model\n",
        "def show_top_features(classifier, vectorizer, categories, n=10):\n",
        "    \"\"\"Show top N features (words) for each category\"\"\"\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    \n",
        "    for i, category in enumerate(categories):\n",
        "        if hasattr(classifier, 'coef_'):\n",
        "            top_indices = np.argsort(classifier.coef_[i])[-n:][::-1]\n",
        "            top_features = [feature_names[idx] for idx in top_indices]\n",
        "            top_scores = classifier.coef_[i][top_indices]\n",
        "            \n",
        "            print(f\"\\n{category}:\")\n",
        "            for feat, score in zip(top_features, top_scores):\n",
        "                print(f\"  {feat:20s} {score:.4f}\")\n",
        "\n",
        "print(\"Top 10 Features per Category (TF-IDF + LogReg):\")\n",
        "print(\"=\" * 60)\n",
        "show_top_features(lr_classifier, tfidf_vectorizer, newsgroups_test.target_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sparse-operations",
      "metadata": {},
      "source": [
        "## 6. Working with Sparse Matrices\n",
        "\n",
        "Understanding sparse matrix operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sparse-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
        "\n",
        "# Create sample sparse matrix\n",
        "vec = TfidfVectorizer(max_features=1000)\n",
        "X_sparse = vec.fit_transform(newsgroups_train.data[:100])\n",
        "\n",
        "print(\"Sparse Matrix Operations:\")\n",
        "print(f\"Matrix shape: {X_sparse.shape}\")\n",
        "print(f\"Matrix type: {type(X_sparse)}\")\n",
        "print(f\"Number of non-zero elements: {X_sparse.nnz}\")\n",
        "print(f\"Sparsity: {(1 - X_sparse.nnz / (X_sparse.shape[0] * X_sparse.shape[1])):.2%}\")\n",
        "\n",
        "# Memory comparison\n",
        "import sys\n",
        "sparse_size = (X_sparse.data.nbytes + X_sparse.indices.nbytes + X_sparse.indptr.nbytes) / 1024**2\n",
        "dense_size = X_sparse.toarray().nbytes / 1024**2\n",
        "\n",
        "print(f\"\\nMemory Usage:\")\n",
        "print(f\"  Sparse: {sparse_size:.2f} MB\")\n",
        "print(f\"  Dense:  {dense_size:.2f} MB\")\n",
        "print(f\"  Savings: {(1 - sparse_size/dense_size):.1%}\")\n",
        "\n",
        "# Common operations\n",
        "print(f\"\\nCommon Operations:\")\n",
        "print(f\"  Get element: X_sparse[0, 10] = {X_sparse[0, 10]:.4f}\")\n",
        "print(f\"  Slice row: {X_sparse[0].shape}\")\n",
        "print(f\"  Convert to dense: {X_sparse.toarray().shape}\")\n",
        "print(f\"  Matrix multiplication: X_sparse @ X_sparse.T shape = {(X_sparse @ X_sparse.T).shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### Choosing the Right Vectorizer\n",
        "\n",
        "| Vectorizer | Best For | Pros | Cons |\n",
        "|------------|----------|------|------|\n",
        "| **CountVectorizer** | Small datasets, interpretability | Simple, fast, interpretable | Doesn't weight importance |\n",
        "| **TfidfVectorizer** | Most text classification tasks | Weights word importance | Slightly slower |\n",
        "| **HashingVectorizer** | Large/streaming data | Memory efficient, fast | Hash collisions, not invertible |\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. **Always use stop words** (`stop_words='english'`)\n",
        "2. **Set min_df** to filter rare words (noise)\n",
        "3. **Set max_df** to filter very common words\n",
        "4. **Use max_features** to control vocabulary size\n",
        "5. **Try n-grams** (bigrams can capture phrases)\n",
        "6. **Keep sparse format** (don't convert to dense unless necessary)\n",
        "7. **TF-IDF usually outperforms** simple counts\n",
        "\n",
        "### Parameter Tuning Tips\n",
        "\n",
        "```python\n",
        "# Good starting point\n",
        "TfidfVectorizer(\n",
        "    max_features=5000,      # Reasonable vocabulary size\n",
        "    min_df=5,               # Remove very rare words\n",
        "    max_df=0.7,             # Remove very common words\n",
        "    ngram_range=(1, 2),     # Include bigrams\n",
        "    stop_words='english',   # Remove stop words\n",
        "    sublinear_tf=True       # Dampen term frequency\n",
        ")\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}