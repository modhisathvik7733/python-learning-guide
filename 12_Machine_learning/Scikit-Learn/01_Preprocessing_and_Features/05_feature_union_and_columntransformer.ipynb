{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# FeatureUnion and ColumnTransformer\n",
    "\n",
    "## Overview\n",
    "\n",
    "Real-world datasets often have **mixed data types**:\n",
    "- Numerical features (age, price, ratings)\n",
    "- Categorical features (gender, city, product type)\n",
    "- Text features (reviews, descriptions)\n",
    "- Datetime features (timestamps)\n",
    "\n",
    "Sklearn provides two powerful tools for handling heterogeneous data:\n",
    "\n",
    "### FeatureUnion (Legacy)\n",
    "- Applies multiple transformers in parallel\n",
    "- Concatenates results horizontally\n",
    "- Works on the **same input data**\n",
    "- Useful for combining different feature extraction methods\n",
    "\n",
    "### ColumnTransformer (Modern, Recommended)\n",
    "- Applies different transformers to **different columns**\n",
    "- Perfect for mixed data types\n",
    "- More intuitive and flexible\n",
    "- Introduced in sklearn 0.20+\n",
    "\n",
    "**Rule of Thumb**: Use **ColumnTransformer** for column-specific preprocessing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample-data",
   "metadata": {},
   "source": [
    "## Sample Dataset with Mixed Types\n",
    "\n",
    "Let's create a realistic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "create-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee Dataset:\n",
      "   age  experience_years   department education                    skills  \\\n",
      "0   60                 4           HR       PhD  Leadership Communication   \n",
      "1   50                 9           HR       PhD  HR Management Recruiting   \n",
      "2   36                32    Marketing  Bachelor   Marketing Analytics SEO   \n",
      "3   64                37        Sales       PhD  Leadership Communication   \n",
      "4   29                12  Engineering  Bachelor           Python Java SQL   \n",
      "5   42                30        Sales    Master  Leadership Communication   \n",
      "6   60                35    Marketing       PhD           Python Java SQL   \n",
      "7   40                23           HR    Master  Leadership Communication   \n",
      "8   44                14        Sales    Master   Marketing Analytics SEO   \n",
      "9   32                28  Engineering       PhD           Python Java SQL   \n",
      "\n",
      "     rating remote_work  high_performer  \n",
      "0  4.485874          No               0  \n",
      "1  3.715914         Yes               1  \n",
      "2  2.647459         Yes               0  \n",
      "3  4.014379          No               1  \n",
      "4  1.814669          No               0  \n",
      "5  2.310209         Yes               0  \n",
      "6  3.186741          No               0  \n",
      "7  3.301638          No               0  \n",
      "8  1.088566         Yes               0  \n",
      "9  4.780457         Yes               1  \n",
      "\n",
      "Shape: (1000, 8)\n",
      "Target distribution: {0: 660, 1: 340}\n"
     ]
    }
   ],
   "source": [
    "# Create sample employee dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 1000\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'age': np.random.randint(22, 65, n_samples),\n",
    "    'experience_years': np.random.randint(0, 40, n_samples),\n",
    "    'department': np.random.choice(['Engineering', 'Sales', 'Marketing', 'HR'], n_samples),\n",
    "    'education': np.random.choice(['Bachelor', 'Master', 'PhD'], n_samples),\n",
    "    'skills': [\n",
    "        np.random.choice(['Python Java SQL', 'Leadership Communication', \n",
    "                         'Marketing Analytics SEO', 'HR Management Recruiting'], 1)[0] \n",
    "        for _ in range(n_samples)\n",
    "    ],\n",
    "    'rating': np.random.uniform(1, 5, n_samples),\n",
    "    'remote_work': np.random.choice(['Yes', 'No'], n_samples),\n",
    "})\n",
    "\n",
    "# Target: High performer (binary classification)\n",
    "data['high_performer'] = (\n",
    "    (data['rating'] > 3.5) & \n",
    "    (data['experience_years'] > 5)\n",
    ").astype(int)\n",
    "\n",
    "print(\"Employee Dataset:\")\n",
    "print(data.head(10))\n",
    "print(f\"\\nShape: {data.shape}\")\n",
    "print(f\"Target distribution: {data['high_performer'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "data-types",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Types:\n",
      "  Numeric: ['age', 'experience_years', 'rating']\n",
      "  Categorical: ['department', 'education', 'remote_work']\n",
      "  Text: ['skills']\n",
      "\n",
      "Data types:\n",
      "age                   int64\n",
      "experience_years      int64\n",
      "department           object\n",
      "education            object\n",
      "skills               object\n",
      "rating              float64\n",
      "remote_work          object\n",
      "high_performer        int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Identify column types\n",
    "numeric_features = ['age', 'experience_years', 'rating']\n",
    "categorical_features = ['department', 'education', 'remote_work']\n",
    "text_features = ['skills']\n",
    "\n",
    "print(\"Feature Types:\")\n",
    "print(f\"  Numeric: {numeric_features}\")\n",
    "print(f\"  Categorical: {categorical_features}\")\n",
    "print(f\"  Text: {text_features}\")\n",
    "print(f\"\\nData types:\\n{data.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featureunion",
   "metadata": {},
   "source": [
    "## 1. FeatureUnion - Combining Feature Extractors\n",
    "\n",
    "**Use Case**: Apply multiple transformations to the **same input** and combine results.\n",
    "\n",
    "Example: Extract both unigrams and bigrams from text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "featureunion-basic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureUnion Results:\n",
      "Output shape: (1000, 18)\n",
      "  - Unigrams: 50 features\n",
      "  - Bigrams: 50 features\n",
      "  - Total: 18 features (concatenated)\n",
      "\n",
      "Matrix type: <class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# Example: Combine unigrams and bigrams from skills column\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Custom transformer to select text column\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.column]\n",
    "\n",
    "# Create FeatureUnion\n",
    "text_features_union = FeatureUnion([\n",
    "    ('unigrams', Pipeline([\n",
    "        ('selector', TextSelector('skills')),\n",
    "        ('vectorizer', CountVectorizer(ngram_range=(1, 1), max_features=50))\n",
    "    ])),\n",
    "    ('bigrams', Pipeline([\n",
    "        ('selector', TextSelector('skills')),\n",
    "        ('vectorizer', CountVectorizer(ngram_range=(2, 2), max_features=50))\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Transform\n",
    "X_union = text_features_union.fit_transform(data)\n",
    "\n",
    "print(\"FeatureUnion Results:\")\n",
    "print(f\"Output shape: {X_union.shape}\")\n",
    "print(f\"  - Unigrams: 50 features\")\n",
    "print(f\"  - Bigrams: 50 features\")\n",
    "print(f\"  - Total: {X_union.shape[1]} features (concatenated)\")\n",
    "print(f\"\\nMatrix type: {type(X_union)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "columntransformer-intro",
   "metadata": {},
   "source": [
    "## 2. ColumnTransformer - The Modern Approach\n",
    "\n",
    "**Best for**: Applying different transformations to different column groups.\n",
    "\n",
    "### Basic Syntax\n",
    "\n",
    "```python\n",
    "ColumnTransformer([\n",
    "    ('name', transformer, columns),\n",
    "    ('name2', transformer2, columns2),\n",
    "], remainder='drop')  # or 'passthrough'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "columntransformer-basic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColumnTransformer Results:\n",
      "Original shape: (1000, 7)\n",
      "Transformed shape: (1000, 9)\n",
      "\n",
      "Breakdown:\n",
      "  - Numeric (scaled): 3 features\n",
      "  - Categorical (one-hot): 6 features\n",
      "  - Text (dropped): 0 features\n"
     ]
    }
   ],
   "source": [
    "# Basic ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features)\n",
    "    ],\n",
    "    remainder='drop'  # Drop text column for now\n",
    ")\n",
    "\n",
    "# Prepare data\n",
    "X = data.drop('high_performer', axis=1)\n",
    "y = data['high_performer']\n",
    "\n",
    "# Transform\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "print(\"ColumnTransformer Results:\")\n",
    "print(f\"Original shape: {X.shape}\")\n",
    "print(f\"Transformed shape: {X_transformed.shape}\")\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"  - Numeric (scaled): {len(numeric_features)} features\")\n",
    "print(f\"  - Categorical (one-hot): {X_transformed.shape[1] - len(numeric_features)} features\")\n",
    "print(f\"  - Text (dropped): 0 features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "get-feature-names",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names after transformation (9 total):\n",
      "['num__age' 'num__experience_years' 'num__rating' 'cat__department_HR'\n",
      " 'cat__department_Marketing' 'cat__department_Sales'\n",
      " 'cat__education_Master' 'cat__education_PhD' 'cat__remote_work_Yes']\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Get feature names after transformation\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "print(f\"Feature names after transformation ({len(feature_names)} total):\")\n",
    "print(feature_names[:20])  # Show first 20\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-pipeline",
   "metadata": {},
   "source": [
    "## 3. Complete Pipeline: All Feature Types\n",
    "\n",
    "Let's process **numeric, categorical, AND text** features together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "complete-preprocessor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Preprocessing Results:\n",
      "Original columns: 7\n",
      "Transformed features: 20\n",
      "\n",
      "Feature breakdown:\n",
      "  - Numeric (scaled): 3 features\n",
      "  - Categorical (one-hot): ~8 features\n",
      "  - Text (TF-IDF): 50 features\n",
      "  - Total: 20 features\n",
      "\n",
      "Output type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Complete preprocessing pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "complete_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Numeric: Impute missing + scale\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numeric_features),\n",
    "        \n",
    "        # Categorical: Impute + one-hot encode\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "        ]), categorical_features),\n",
    "        \n",
    "        # Text: TF-IDF vectorization\n",
    "        ('text', TfidfVectorizer(max_features=50, stop_words='english'), 'skills')\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Transform data\n",
    "X_complete = complete_preprocessor.fit_transform(X)\n",
    "\n",
    "print(\"Complete Preprocessing Results:\")\n",
    "print(f\"Original columns: {X.shape[1]}\")\n",
    "print(f\"Transformed features: {X_complete.shape[1]}\")\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"  - Numeric (scaled): 3 features\")\n",
    "print(f\"  - Categorical (one-hot): ~8 features\")\n",
    "print(f\"  - Text (TF-IDF): 50 features\")\n",
    "print(f\"  - Total: {X_complete.shape[1]} features\")\n",
    "print(f\"\\nOutput type: {type(X_complete)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "full-pipeline",
   "metadata": {},
   "source": [
    "## 4. Full ML Pipeline: Preprocessing + Model\n",
    "\n",
    "Combine preprocessing and modeling in one pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ml-pipeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 800\n",
      "Test samples: 200\n",
      "\n",
      "Training pipeline...\n",
      "\n",
      "Test Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       132\n",
      "           1       1.00      1.00      1.00        68\n",
      "\n",
      "    accuracy                           1.00       200\n",
      "   macro avg       1.00      1.00      1.00       200\n",
      "weighted avg       1.00      1.00      1.00       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "\n",
    "# Create full pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', complete_preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Train (preprocessor + model in one step!)\n",
    "print(\"\\nTraining pipeline...\")\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = full_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cross-validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold Cross-Validation Scores:\n",
      "  Scores: [1. 1. 1. 1. 1.]\n",
      "  Mean: 1.0000 (+/- 0.0000)\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation with full pipeline\n",
    "cv_scores = cross_val_score(\n",
    "    full_pipeline, X, y, cv=5, scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(\"5-Fold Cross-Validation Scores:\")\n",
    "print(f\"  Scores: {cv_scores}\")\n",
    "print(f\"  Mean: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "real-dataset",
   "metadata": {},
   "source": [
    "## 5. Real-World Example: Titanic Dataset\n",
    "\n",
    "Classic dataset with mixed types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "titanic-load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Titanic dataset...\n",
      "\n",
      "Dataset shape: (1309, 8)\n",
      "\n",
      "First few rows:\n",
      "   pclass     sex      age  sibsp  parch      fare embarked  survived\n",
      "0       1  female  29.0000      0      0  211.3375        S         1\n",
      "1       1    male   0.9167      1      2  151.5500        S         1\n",
      "2       1  female   2.0000      1      2  151.5500        S         0\n",
      "3       1    male  30.0000      1      2  151.5500        S         0\n",
      "4       1  female  25.0000      1      2  151.5500        S         0\n",
      "\n",
      "Missing values:\n",
      "pclass        0\n",
      "sex           0\n",
      "age         263\n",
      "sibsp         0\n",
      "parch         0\n",
      "fare          1\n",
      "embarked      2\n",
      "survived      0\n",
      "dtype: int64\n",
      "\n",
      "Survival rate: 38.20%\n"
     ]
    }
   ],
   "source": [
    "# Load Titanic dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "print(\"Loading Titanic dataset...\")\n",
    "titanic = fetch_openml('titanic', version=1, as_frame=True, parser='auto')\n",
    "df = titanic.frame\n",
    "\n",
    "# Clean and prepare\n",
    "df = df[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'survived']].copy()\n",
    "df['survived'] = df['survived'].astype(int)\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
    "print(f\"\\nSurvival rate: {df['survived'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "titanic-preprocessing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (1047, 7)\n",
      "Test set: (262, 7)\n"
     ]
    }
   ],
   "source": [
    "# Define features by type\n",
    "numeric_cols = ['age', 'sibsp', 'parch', 'fare']\n",
    "categorical_cols = ['pclass', 'sex', 'embarked']\n",
    "\n",
    "# Create preprocessor\n",
    "titanic_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),  # Handle missing values\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numeric_cols),\n",
    "        \n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('encoder', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "        ]), categorical_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Prepare data\n",
    "X_titanic = df.drop('survived', axis=1)\n",
    "y_titanic = df['survived']\n",
    "\n",
    "# Split\n",
    "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(\n",
    "    X_titanic, y_titanic, test_size=0.2, random_state=42, stratify=y_titanic\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_t.shape}\")\n",
    "print(f\"Test set: {X_test_t.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "titanic-models",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TITANIC SURVIVAL PREDICTION\n",
      "============================================================\n",
      "\n",
      "Logistic Regression:\n",
      "  Accuracy: 0.8053\n",
      "  CV Score: 0.6486 (+/- 0.0832)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest:\n",
      "  Accuracy: 0.7824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CV Score: 0.6585 (+/- 0.0815)\n"
     ]
    }
   ],
   "source": [
    "# Compare multiple models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TITANIC SURVIVAL PREDICTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', titanic_preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    # Train\n",
    "    pipeline.fit(X_train_t, y_train_t)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = pipeline.predict(X_test_t)\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test_t, y_pred)\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(pipeline, X_titanic, y_titanic, cv=5)\n",
    "    print(f\"  CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-columntransformer",
   "metadata": {},
   "source": [
    "## 6. Advanced ColumnTransformer Techniques\n",
    "\n",
    "### Using `make_column_transformer` (Simpler Syntax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "make-column-transformer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed shape: (1309, 10)\n",
      "Feature names: ['standardscaler__age' 'standardscaler__sibsp' 'standardscaler__parch'\n",
      " 'standardscaler__fare' 'onehotencoder__pclass_2'\n",
      " 'onehotencoder__pclass_3' 'onehotencoder__sex_male'\n",
      " 'onehotencoder__embarked_Q' 'onehotencoder__embarked_S'\n",
      " 'onehotencoder__embarked_nan']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "# Simpler syntax (no need for tuples with names)\n",
    "preprocessor_simple = make_column_transformer(\n",
    "    (StandardScaler(), numeric_cols),\n",
    "    (OneHotEncoder(drop='first', sparse_output=False), categorical_cols),\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "X_simple = preprocessor_simple.fit_transform(X_titanic)\n",
    "print(f\"Transformed shape: {X_simple.shape}\")\n",
    "print(f\"Feature names: {preprocessor_simple.get_feature_names_out()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remainder",
   "metadata": {},
   "source": [
    "### Using `remainder` Parameter\n",
    "\n",
    "- `remainder='drop'`: Drop untransformed columns (default)\n",
    "- `remainder='passthrough'`: Keep untransformed columns as-is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "remainder-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remainder Comparison:\n",
      "  Original: 7 columns\n",
      "  remainder='drop': 4 columns (only scaled numeric)\n",
      "  remainder='passthrough': 7 columns (numeric + original categorical)\n"
     ]
    }
   ],
   "source": [
    "# Compare remainder options\n",
    "preprocessor_drop = ColumnTransformer(\n",
    "    [('num', StandardScaler(), numeric_cols)],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "preprocessor_pass = ColumnTransformer(\n",
    "    [('num', StandardScaler(), numeric_cols)],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_drop = preprocessor_drop.fit_transform(X_titanic)\n",
    "X_pass = preprocessor_pass.fit_transform(X_titanic)\n",
    "\n",
    "print(\"Remainder Comparison:\")\n",
    "print(f\"  Original: {X_titanic.shape[1]} columns\")\n",
    "print(f\"  remainder='drop': {X_drop.shape[1]} columns (only scaled numeric)\")\n",
    "print(f\"  remainder='passthrough': {X_pass.shape[1]} columns (numeric + original categorical)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "column-selectors",
   "metadata": {},
   "source": [
    "### Using Column Selectors (Advanced)\n",
    "\n",
    "Select columns by data type automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "column-selector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic Column Selection:\n",
      "  Input shape: (1309, 7)\n",
      "  Output shape: (1309, 5)\n",
      "\n",
      "  Numeric columns (auto-detected): ['pclass', 'age', 'sibsp', 'parch', 'fare']\n",
      "  Object columns (auto-detected): []\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import make_column_selector\n",
    "\n",
    "# Automatic column selection by dtype\n",
    "preprocessor_auto = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), make_column_selector(dtype_include=np.number)),\n",
    "        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), \n",
    "         make_column_selector(dtype_include=object))\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_auto = preprocessor_auto.fit_transform(X_titanic)\n",
    "\n",
    "print(\"Automatic Column Selection:\")\n",
    "print(f\"  Input shape: {X_titanic.shape}\")\n",
    "print(f\"  Output shape: {X_auto.shape}\")\n",
    "print(f\"\\n  Numeric columns (auto-detected): {X_titanic.select_dtypes(include=np.number).columns.tolist()}\")\n",
    "print(f\"  Object columns (auto-detected): {X_titanic.select_dtypes(include=object).columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-transformer",
   "metadata": {},
   "source": [
    "## 7. Creating Custom Transformers\n",
    "\n",
    "Build your own transformers for specialized preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "custom-transformer-code",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot setitem on a Categorical with a new category (0), set the categories first",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 39\u001b[0m\n\u001b[1;32m     29\u001b[0m custom_preprocessor \u001b[38;5;241m=\u001b[39m ColumnTransformer(\n\u001b[1;32m     30\u001b[0m     transformers\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     31\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_fare\u001b[39m\u001b[38;5;124m'\u001b[39m, LogTransformer(), [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfare\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     remainder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Test custom preprocessor\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m X_custom \u001b[38;5;241m=\u001b[39m custom_preprocessor\u001b[38;5;241m.\u001b[39mfit_transform(X_titanic\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustom preprocessing output shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_custom\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/generic.py:7434\u001b[0m, in \u001b[0;36mNDFrame.fillna\u001b[0;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[1;32m   7432\u001b[0m         new_data \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[1;32m   7433\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 7434\u001b[0m         new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mfillna(\n\u001b[1;32m   7435\u001b[0m             value\u001b[38;5;241m=\u001b[39mvalue, limit\u001b[38;5;241m=\u001b[39mlimit, inplace\u001b[38;5;241m=\u001b[39minplace, downcast\u001b[38;5;241m=\u001b[39mdowncast\n\u001b[1;32m   7436\u001b[0m         )\n\u001b[1;32m   7437\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ABCDataFrame) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   7438\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotna(), value)\u001b[38;5;241m.\u001b[39m_mgr\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/internals/base.py:186\u001b[0m, in \u001b[0;36mDataManager.fillna\u001b[0;34m(self, value, limit, inplace, downcast)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Do this validation even if we go through one of the no-op paths\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     limit \u001b[38;5;241m=\u001b[39m libalgos\u001b[38;5;241m.\u001b[39mvalidate_limit(\u001b[38;5;28;01mNone\u001b[39;00m, limit\u001b[38;5;241m=\u001b[39mlimit)\n\u001b[0;32m--> 186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_with_block(\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfillna\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    188\u001b[0m     value\u001b[38;5;241m=\u001b[39mvalue,\n\u001b[1;32m    189\u001b[0m     limit\u001b[38;5;241m=\u001b[39mlimit,\n\u001b[1;32m    190\u001b[0m     inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[1;32m    191\u001b[0m     downcast\u001b[38;5;241m=\u001b[39mdowncast,\n\u001b[1;32m    192\u001b[0m     using_cow\u001b[38;5;241m=\u001b[39musing_copy_on_write(),\n\u001b[1;32m    193\u001b[0m     already_warned\u001b[38;5;241m=\u001b[39m_AlreadyWarned(),\n\u001b[1;32m    194\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/internals/managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/internals/blocks.py:2334\u001b[0m, in \u001b[0;36mExtensionBlock.fillna\u001b[0;34m(self, value, limit, inplace, downcast, using_cow, already_warned)\u001b[0m\n\u001b[1;32m   2331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   2332\u001b[0m     \u001b[38;5;66;03m# 3rd party EA that has not implemented copy keyword yet\u001b[39;00m\n\u001b[1;32m   2333\u001b[0m     refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2334\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mfillna(value\u001b[38;5;241m=\u001b[39mvalue, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, limit\u001b[38;5;241m=\u001b[39mlimit)\n\u001b[1;32m   2335\u001b[0m     \u001b[38;5;66;03m# issue the warning *after* retrying, in case the TypeError\u001b[39;00m\n\u001b[1;32m   2336\u001b[0m     \u001b[38;5;66;03m#  was caused by an invalid fill_value\u001b[39;00m\n\u001b[1;32m   2337\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2338\u001b[0m         \u001b[38;5;66;03m# GH#53278\u001b[39;00m\n\u001b[1;32m   2339\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtensionArray.fillna added a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keyword in pandas \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2345\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   2346\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/arrays/_mixins.py:376\u001b[0m, in \u001b[0;36mNDArrayBackedExtensionArray.fillna\u001b[0;34m(self, value, method, limit, copy)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;66;03m# We validate the fill_value even if there is nothing to fill\u001b[39;00m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_setitem_value(value)\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m copy:\n\u001b[1;32m    379\u001b[0m         new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m[:]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/arrays/categorical.py:1589\u001b[0m, in \u001b[0;36mCategorical._validate_setitem_value\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   1587\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_listlike(value)\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_scalar(value)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/arrays/categorical.py:1614\u001b[0m, in \u001b[0;36mCategorical._validate_scalar\u001b[0;34m(self, fill_value)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbox_scalar(fill_value)\n\u001b[1;32m   1613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1614\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1615\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot setitem on a Categorical with a new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1616\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfill_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), set the categories first\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1617\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1618\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fill_value\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot setitem on a Categorical with a new category (0), set the categories first"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Custom transformer: Log transformation\n",
    "class LogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, offset=1):\n",
    "        self.offset = offset\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.log(X + self.offset)\n",
    "\n",
    "# Custom transformer: Age binning\n",
    "class AgeBinner(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy['age_group'] = pd.cut(\n",
    "            X_copy['age'], \n",
    "            bins=[0, 18, 30, 50, 100],\n",
    "            labels=['child', 'young', 'adult', 'senior']\n",
    "        )\n",
    "        return X_copy\n",
    "\n",
    "# Use custom transformers in pipeline\n",
    "custom_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('log_fare', LogTransformer(), ['fare']),\n",
    "        ('standard_age', StandardScaler(), ['age']),\n",
    "        ('cat', OneHotEncoder(sparse_output=False), ['sex', 'embarked'])\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Test custom preprocessor\n",
    "X_custom = custom_preprocessor.fit_transform(X_titanic.fillna(0))\n",
    "print(f\"Custom preprocessing output shape: {X_custom.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "## 8. FeatureUnion vs ColumnTransformer Comparison\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "| Scenario | Use |\n",
    "|----------|-----|\n",
    "| Different transformations on **different columns** | **ColumnTransformer** |\n",
    "| Multiple transformations on **same columns** | **FeatureUnion** |\n",
    "| Mixed data types (numeric, categorical, text) | **ColumnTransformer** |\n",
    "| Extracting different text features (unigrams + bigrams) | **FeatureUnion** |\n",
    "| Modern, clean syntax | **ColumnTransformer** |\n",
    "| Need to select specific columns | **ColumnTransformer** |\n",
    "\n",
    "### Example Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text data\n",
    "text_data = pd.DataFrame({\n",
    "    'review': [\n",
    "        'great product amazing quality',\n",
    "        'terrible waste of money',\n",
    "        'good value for price'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Scenario: Extract both word counts AND character n-grams from same text\\n\")\n",
    "\n",
    "# FeatureUnion: Multiple transformations on SAME column\n",
    "text_union = FeatureUnion([\n",
    "    ('word_count', CountVectorizer()),\n",
    "    ('char_ngrams', CountVectorizer(analyzer='char', ngram_range=(2, 3)))\n",
    "])\n",
    "\n",
    "X_text = text_union.fit_transform(text_data['review'])\n",
    "print(f\"FeatureUnion output: {X_text.shape}\")\n",
    "print(\"  → Combines word-level and character-level features from same text\\n\")\n",
    "\n",
    "# ColumnTransformer: Different transformations on DIFFERENT columns\n",
    "mixed_data = pd.DataFrame({\n",
    "    'price': [100, 200, 150],\n",
    "    'category': ['A', 'B', 'A'],\n",
    "    'description': ['great product', 'terrible item', 'good value']\n",
    "})\n",
    "\n",
    "col_trans = ColumnTransformer([\n",
    "    ('scale_price', StandardScaler(), ['price']),\n",
    "    ('encode_cat', OneHotEncoder(sparse_output=False), ['category']),\n",
    "    ('vectorize_text', CountVectorizer(), 'description')\n",
    "])\n",
    "\n",
    "X_mixed = col_trans.fit_transform(mixed_data)\n",
    "print(f\"ColumnTransformer output: {X_mixed.shape}\")\n",
    "print(\"  → Different preprocessing for each column type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best-practices",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### 1. Always Use Pipelines\n",
    "```python\n",
    "# ✓ GOOD: Everything in one pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer(...)),\n",
    "    ('model', RandomForestClassifier())\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# ✗ BAD: Manual preprocessing (risk of data leakage!)\n",
    "X_scaled = scaler.fit_transform(X_train)  # DON'T DO THIS\n",
    "```\n",
    "\n",
    "### 2. Handle Missing Values\n",
    "```python\n",
    "Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Always impute first\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "```\n",
    "\n",
    "### 3. Use `handle_unknown='ignore'` for OneHotEncoder\n",
    "```python\n",
    "OneHotEncoder(drop='first', handle_unknown='ignore')  # Prevents errors on new categories\n",
    "```\n",
    "\n",
    "### 4. Set `sparse_output=False` When Needed\n",
    "```python\n",
    "# If mixing sparse and dense transformers:\n",
    "OneHotEncoder(sparse_output=False)  # Returns dense array\n",
    "```\n",
    "\n",
    "### 5. Use `make_column_selector` for Dynamic Selection\n",
    "```python\n",
    "ColumnTransformer([\n",
    "    ('num', StandardScaler(), make_column_selector(dtype_include=np.number)),\n",
    "    ('cat', OneHotEncoder(), make_column_selector(dtype_include=object))\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### ColumnTransformer (Modern, Recommended)\n",
    "- ✓ Apply different transformations to different columns\n",
    "- ✓ Perfect for mixed data types\n",
    "- ✓ Clean, intuitive syntax\n",
    "- ✓ Integrates seamlessly with Pipeline\n",
    "- ✓ Prevents data leakage\n",
    "\n",
    "### FeatureUnion (Legacy, Specific Use Cases)\n",
    "- ✓ Multiple transformations on same input\n",
    "- ✓ Combining different feature extraction methods\n",
    "- ✗ Less intuitive for mixed data types\n",
    "- → Use ColumnTransformer instead for most cases\n",
    "\n",
    "### Complete Workflow Template\n",
    "\n",
    "```python\n",
    "# 1. Define column groups\n",
    "numeric_cols = ['age', 'income']\n",
    "categorical_cols = ['gender', 'city']\n",
    "text_cols = 'description'\n",
    "\n",
    "# 2. Create preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]), numeric_cols),\n",
    "    ('cat', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "    ]), categorical_cols),\n",
    "    ('text', TfidfVectorizer(max_features=100), text_cols)\n",
    "])\n",
    "\n",
    "# 3. Create full pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# 4. Train and evaluate\n",
    "pipeline.fit(X_train, y_train)\n",
    "pipeline.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "This approach:\n",
    "- Prevents data leakage\n",
    "- Makes code reproducible\n",
    "- Enables easy hyperparameter tuning\n",
    "- Simplifies deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
