{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Ensemble Methods: Stacking and Voting\n",
        "\n",
        "## Overview\n",
        "\n",
        "**Ensemble learning** combines multiple models to create a stronger predictor than any individual model.\n",
        "\n",
        "### Core Idea\n",
        "\n",
        "*\"The wisdom of crowds\"* - Multiple diverse models making predictions together often outperform a single model.\n",
        "\n",
        "### Types of Ensemble Methods\n",
        "\n",
        "```\n",
        "Ensemble Methods\n",
        "\u251c\u2500\u2500 Bagging (Bootstrap Aggregating)\n",
        "\u2502   \u2514\u2500\u2500 Random Forest, Bagging Classifier/Regressor\n",
        "\u251c\u2500\u2500 Boosting\n",
        "\u2502   \u2514\u2500\u2500 AdaBoost, Gradient Boosting, XGBoost, LightGBM\n",
        "\u2514\u2500\u2500 Stacking & Voting (This notebook)\n",
        "    \u251c\u2500\u2500 Voting: Simple combination of predictions\n",
        "    \u2514\u2500\u2500 Stacking: Meta-model learns optimal combination\n",
        "```\n",
        "\n",
        "## Topics Covered\n",
        "\n",
        "### 1. Voting Ensembles\n",
        "- Hard voting (majority vote for classification)\n",
        "- Soft voting (average probabilities)\n",
        "- Voting for regression (average predictions)\n",
        "- When voting works best\n",
        "\n",
        "### 2. Stacking Ensembles\n",
        "- Two-level architecture (base models + meta-model)\n",
        "- Cross-validation predictions\n",
        "- Preventing overfitting in stacking\n",
        "- Feature augmentation strategies\n",
        "\n",
        "### 3. Practical Applications\n",
        "- Classification tasks\n",
        "- Regression tasks\n",
        "- Model diversity importance\n",
        "- Hyperparameter tuning for ensembles\n",
        "\n",
        "### 4. Comparison and Best Practices\n",
        "- When to use voting vs stacking\n",
        "- Computational considerations\n",
        "- Interpretability tradeoffs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from time import time\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, RandomForestRegressor,\n",
        "    GradientBoostingClassifier, GradientBoostingRegressor,\n",
        "    VotingClassifier, VotingRegressor,\n",
        "    StackingClassifier, StackingRegressor\n",
        ")\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Utilities\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report,\n",
        "    mean_squared_error, mean_absolute_error, r2_score\n",
        ")\n",
        "from sklearn.datasets import (\n",
        "    load_breast_cancer, load_wine, load_iris,\n",
        "    load_diabetes, make_classification, make_regression\n",
        ")\n",
        "\n",
        "np.random.seed(42)\n",
        "sns.set_style('whitegrid')\n",
        "print(\"\u2713 Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ensemble-fundamentals",
      "metadata": {},
      "source": [
        "## 1. Ensemble Learning Fundamentals\n",
        "\n",
        "### 1.1 Why Ensembles Work\n",
        "\n",
        "**Key Principle**: If individual models make **different** errors, combining them reduces overall error.\n",
        "\n",
        "**Requirements for Effective Ensembles**:\n",
        "1. **Diversity**: Models should be different (different algorithms, hyperparameters, or training data)\n",
        "2. **Accuracy**: Base models should be better than random guessing\n",
        "3. **Independence**: Models should make uncorrelated errors\n",
        "\n",
        "### Mathematical Intuition\n",
        "\n",
        "**Example**: Binary classification with 3 classifiers, each 70% accurate.\n",
        "\n",
        "**Majority voting probability of being correct**:\n",
        "\\[\n",
        "P(\\text{correct}) = P(\\text{all 3 correct}) + P(\\text{exactly 2 correct})\n",
        "\\]\n",
        "\\[\n",
        "= (0.7)^3 + 3 \\times (0.7)^2 \\times (0.3) = 0.343 + 0.441 = 0.784\n",
        "\\]\n",
        "\n",
        "**Result**: 78.4% ensemble accuracy vs 70% individual accuracy!\n",
        "\n",
        "*(Assumes independence - real models may be correlated)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ensemble-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Ensemble Voting Mathematics\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Simulate voting with different base accuracies\n",
        "def voting_accuracy(base_acc, n_models):\n",
        "    \"\"\"Calculate expected accuracy of majority voting.\"\"\"\n",
        "    from math import comb\n",
        "    # Need at least (n_models // 2 + 1) models to be correct\n",
        "    threshold = n_models // 2 + 1\n",
        "    prob_correct = 0\n",
        "    for k in range(threshold, n_models + 1):\n",
        "        prob_correct += comb(n_models, k) * (base_acc ** k) * ((1 - base_acc) ** (n_models - k))\n",
        "    return prob_correct\n",
        "\n",
        "# Test different scenarios\n",
        "base_accuracies = [0.6, 0.65, 0.7, 0.75, 0.8]\n",
        "n_models_list = [3, 5, 7, 9]\n",
        "\n",
        "print(\"Expected Ensemble Accuracy (Majority Voting)\")\n",
        "print(\"Assumes models make independent errors\\n\")\n",
        "\n",
        "results = []\n",
        "for base_acc in base_accuracies:\n",
        "    row = {'Base Accuracy': f\"{base_acc:.2f}\"}\n",
        "    for n in n_models_list:\n",
        "        ensemble_acc = voting_accuracy(base_acc, n)\n",
        "        row[f'{n} Models'] = f\"{ensemble_acc:.3f}\"\n",
        "        if n == 3:\n",
        "            row['Improvement'] = f\"+{(ensemble_acc - base_acc)*100:.1f}%\"\n",
        "    results.append(row)\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Key Insights:\")\n",
        "print(\"   - More models \u2192 Better ensemble accuracy\")\n",
        "print(\"   - Better base models \u2192 Better ensembles\")\n",
        "print(\"   - Diminishing returns with too many models\")\n",
        "print(\"   - Real ensembles: models are correlated (less improvement)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "voting-classifier",
      "metadata": {},
      "source": [
        "## 2. Voting Classifier\n",
        "\n",
        "### Concept\n",
        "\n",
        "**Voting Classifier** combines predictions from multiple models using:\n",
        "\n",
        "1. **Hard Voting**: Majority vote (mode of predictions)\n",
        "   \\[\n",
        "   \\hat{y} = \\text{mode}(\\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_n)\n",
        "   \\]\n",
        "\n",
        "2. **Soft Voting**: Average of predicted probabilities (often better)\n",
        "   \\[\n",
        "   \\hat{y} = \\arg\\max_c \\frac{1}{n}\\sum_{i=1}^{n} P_i(y=c)\n",
        "   \\]\n",
        "\n",
        "### 2.1 Simple Example with Diverse Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "voting-classifier-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load breast cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X_cancer = cancer.data\n",
        "y_cancer = cancer.target\n",
        "\n",
        "print(\"Voting Classifier Demo - Breast Cancer Dataset\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_cancer.shape[0]}\")\n",
        "print(f\"Features: {X_cancer.shape[1]}\")\n",
        "print(f\"Classes: {cancer.target_names}\")\n",
        "\n",
        "# Split and scale\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "voting-models",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define diverse base models\n",
        "print(\"\\nTraining Individual Models...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create diverse models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=10000, random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
        "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
        "    'Naive Bayes': GaussianNB()\n",
        "}\n",
        "\n",
        "# Train and evaluate individual models\n",
        "individual_results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Train\n",
        "    start_time = time()\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    train_time = time() - start_time\n",
        "    \n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    \n",
        "    # Evaluate\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    \n",
        "    individual_results.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1,\n",
        "        'Train Time (s)': train_time\n",
        "    })\n",
        "    \n",
        "    print(f\"{name:20} - Accuracy: {accuracy:.4f}, F1: {f1:.4f}, Time: {train_time:.3f}s\")\n",
        "\n",
        "individual_df = pd.DataFrame(individual_results)\n",
        "print(\"\\n\" + individual_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "voting-hard-soft",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create voting ensembles\n",
        "print(\"\\n\\nCreating Voting Ensembles...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Prepare estimators for voting\n",
        "estimators = [(name, model) for name, model in models.items()]\n",
        "\n",
        "# Hard voting\n",
        "voting_hard = VotingClassifier(\n",
        "    estimators=estimators,\n",
        "    voting='hard'\n",
        ")\n",
        "\n",
        "start_time = time()\n",
        "voting_hard.fit(X_train_scaled, y_train)\n",
        "hard_train_time = time() - start_time\n",
        "\n",
        "y_pred_hard = voting_hard.predict(X_test_scaled)\n",
        "hard_accuracy = accuracy_score(y_test, y_pred_hard)\n",
        "hard_f1 = f1_score(y_test, y_pred_hard)\n",
        "\n",
        "print(f\"Hard Voting - Accuracy: {hard_accuracy:.4f}, F1: {hard_f1:.4f}, Time: {hard_train_time:.3f}s\")\n",
        "\n",
        "# Soft voting\n",
        "voting_soft = VotingClassifier(\n",
        "    estimators=estimators,\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "start_time = time()\n",
        "voting_soft.fit(X_train_scaled, y_train)\n",
        "soft_train_time = time() - start_time\n",
        "\n",
        "y_pred_soft = voting_soft.predict(X_test_scaled)\n",
        "soft_accuracy = accuracy_score(y_test, y_pred_soft)\n",
        "soft_f1 = f1_score(y_test, y_pred_soft)\n",
        "\n",
        "print(f\"Soft Voting - Accuracy: {soft_accuracy:.4f}, F1: {soft_f1:.4f}, Time: {soft_train_time:.3f}s\")\n",
        "\n",
        "# Compare with best individual model\n",
        "best_individual = individual_df.loc[individual_df['Accuracy'].idxmax()]\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Comparison Summary:\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Best Individual Model: {best_individual['Model']} ({best_individual['Accuracy']:.4f})\")\n",
        "print(f\"Hard Voting:           {hard_accuracy:.4f} ({'+' if hard_accuracy > best_individual['Accuracy'] else ''}{(hard_accuracy - best_individual['Accuracy'])*100:.2f}%)\")\n",
        "print(f\"Soft Voting:           {soft_accuracy:.4f} ({'+' if soft_accuracy > best_individual['Accuracy'] else ''}{(soft_accuracy - best_individual['Accuracy'])*100:.2f}%)\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Soft voting typically performs better by using probability information!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "voting-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy comparison\n",
        "all_models = list(models.keys()) + ['Hard Voting', 'Soft Voting']\n",
        "all_accuracies = individual_df['Accuracy'].tolist() + [hard_accuracy, soft_accuracy]\n",
        "colors = ['skyblue'] * len(models) + ['orange', 'green']\n",
        "\n",
        "axes[0].barh(all_models, all_accuracies, color=colors, alpha=0.8)\n",
        "axes[0].set_xlabel('Accuracy')\n",
        "axes[0].set_title('Model Comparison: Individual vs Voting Ensembles')\n",
        "axes[0].axvline(x=best_individual['Accuracy'], color='red', linestyle='--', \n",
        "                alpha=0.5, label='Best Individual')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3, axis='x')\n",
        "\n",
        "# Confusion matrix for soft voting\n",
        "cm = confusion_matrix(y_test, y_pred_soft)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=cancer.target_names,\n",
        "            yticklabels=cancer.target_names,\n",
        "            ax=axes[1])\n",
        "axes[1].set_ylabel('True Label')\n",
        "axes[1].set_xlabel('Predicted Label')\n",
        "axes[1].set_title('Soft Voting - Confusion Matrix')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "voting-regressor",
      "metadata": {},
      "source": [
        "## 3. Voting Regressor\n",
        "\n",
        "### Concept\n",
        "\n",
        "**Voting Regressor** averages predictions from multiple regression models:\n",
        "\n",
        "\\[\n",
        "\\hat{y} = \\frac{1}{n}\\sum_{i=1}^{n} \\hat{y}_i\n",
        "\\]\n",
        "\n",
        "Or weighted average:\n",
        "\\[\n",
        "\\hat{y} = \\sum_{i=1}^{n} w_i \\hat{y}_i \\quad \\text{where} \\quad \\sum w_i = 1\n",
        "\\]\n",
        "\n",
        "### 3.1 Regression Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "voting-regressor-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X_diabetes = diabetes.data\n",
        "y_diabetes = diabetes.target\n",
        "\n",
        "print(\"Voting Regressor Demo - Diabetes Dataset\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_diabetes.shape[0]}\")\n",
        "print(f\"Features: {X_diabetes.shape[1]}\")\n",
        "\n",
        "# Split data\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_diabetes, y_diabetes, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set: {X_train_reg.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test_reg.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "voting-regressor-models",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define regression models\n",
        "print(\"\\nTraining Regression Models...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "reg_models = {\n",
        "    'Ridge': Ridge(alpha=1.0, random_state=42),\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
        "    'KNN': KNeighborsRegressor(n_neighbors=5)\n",
        "}\n",
        "\n",
        "# Train and evaluate\n",
        "reg_results = []\n",
        "\n",
        "for name, model in reg_models.items():\n",
        "    model.fit(X_train_reg, y_train_reg)\n",
        "    y_pred_reg = model.predict(X_test_reg)\n",
        "    \n",
        "    mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
        "    mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
        "    r2 = r2_score(y_test_reg, y_pred_reg)\n",
        "    \n",
        "    reg_results.append({\n",
        "        'Model': name,\n",
        "        'MSE': mse,\n",
        "        'MAE': mae,\n",
        "        'R\u00b2 Score': r2\n",
        "    })\n",
        "    \n",
        "    print(f\"{name:20} - R\u00b2: {r2:.4f}, MSE: {mse:.2f}, MAE: {mae:.2f}\")\n",
        "\n",
        "reg_df = pd.DataFrame(reg_results)\n",
        "\n",
        "# Create voting regressor\n",
        "print(\"\\n\\nCreating Voting Regressor...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "voting_reg = VotingRegressor(\n",
        "    estimators=[(name, model) for name, model in reg_models.items()]\n",
        ")\n",
        "\n",
        "voting_reg.fit(X_train_reg, y_train_reg)\n",
        "y_pred_voting = voting_reg.predict(X_test_reg)\n",
        "\n",
        "voting_mse = mean_squared_error(y_test_reg, y_pred_voting)\n",
        "voting_mae = mean_absolute_error(y_test_reg, y_pred_voting)\n",
        "voting_r2 = r2_score(y_test_reg, y_pred_voting)\n",
        "\n",
        "print(f\"Voting Regressor - R\u00b2: {voting_r2:.4f}, MSE: {voting_mse:.2f}, MAE: {voting_mae:.2f}\")\n",
        "\n",
        "# Compare\n",
        "best_individual_reg = reg_df.loc[reg_df['R\u00b2 Score'].idxmax()]\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Comparison:\")\n",
        "print(f\"Best Individual: {best_individual_reg['Model']} (R\u00b2 = {best_individual_reg['R\u00b2 Score']:.4f})\")\n",
        "print(f\"Voting Ensemble: R\u00b2 = {voting_r2:.4f} ({'+' if voting_r2 > best_individual_reg['R\u00b2 Score'] else ''}{(voting_r2 - best_individual_reg['R\u00b2 Score']):.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "stacking-intro",
      "metadata": {},
      "source": [
        "## 4. Stacking (Stacked Generalization)\n",
        "\n",
        "### Concept\n",
        "\n",
        "**Stacking** is a more sophisticated ensemble method that uses a **meta-model** to learn how to best combine base model predictions.\n",
        "\n",
        "### Architecture\n",
        "\n",
        "```\n",
        "Level 0 (Base Models):        Level 1 (Meta-Model):\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502   Model 1    \u2502\u2500\u2500\u2510\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502   Model 2    \u2502\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192\u2502 Meta-Model   \u2502\u2500\u2500\u2192 Final Prediction\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n",
        "\u2502   Model 3    \u2502\u2500\u2500\u2518\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "```\n",
        "\n",
        "### Process\n",
        "\n",
        "1. **Split training data** into folds (cross-validation)\n",
        "2. **Train base models** on each fold\n",
        "3. **Generate meta-features**: Base model predictions on held-out data\n",
        "4. **Train meta-model** on meta-features\n",
        "5. **Predict**: Base models \u2192 Meta-model \u2192 Final prediction\n",
        "\n",
        "### Key Differences from Voting\n",
        "\n",
        "| Aspect | Voting | Stacking |\n",
        "|--------|--------|----------|\n",
        "| Combination | Fixed (average/vote) | Learned by meta-model |\n",
        "| Complexity | Simple | More complex |\n",
        "| Performance | Good | Often better |\n",
        "| Overfitting Risk | Low | Higher (needs CV) |\n",
        "| Interpretability | High | Lower |\n",
        "\n",
        "### 4.1 Stacking Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "stacking-classifier-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Stacking Classifier Demo\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Define base models (level 0)\n",
        "base_models = [\n",
        "    ('lr', LogisticRegression(max_iter=10000, random_state=42)),\n",
        "    ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),\n",
        "    ('svm', SVC(kernel='rbf', probability=True, random_state=42)),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
        "]\n",
        "\n",
        "# Define meta-model (level 1)\n",
        "meta_model = LogisticRegression(max_iter=10000, random_state=42)\n",
        "\n",
        "# Create stacking classifier\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=base_models,\n",
        "    final_estimator=meta_model,\n",
        "    cv=5,  # Cross-validation folds for generating meta-features\n",
        "    stack_method='auto',  # Use predict_proba if available\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"Base Models (Level 0):\")\n",
        "for name, model in base_models:\n",
        "    print(f\"  - {name}: {model.__class__.__name__}\")\n",
        "\n",
        "print(f\"\\nMeta-Model (Level 1): {meta_model.__class__.__name__}\")\n",
        "print(f\"Cross-validation folds: 5\")\n",
        "\n",
        "# Train stacking classifier\n",
        "print(\"\\nTraining Stacking Classifier...\")\n",
        "start_time = time()\n",
        "stacking_clf.fit(X_train_scaled, y_train)\n",
        "stacking_train_time = time() - start_time\n",
        "\n",
        "# Predict\n",
        "y_pred_stacking = stacking_clf.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate\n",
        "stacking_accuracy = accuracy_score(y_test, y_pred_stacking)\n",
        "stacking_precision = precision_score(y_test, y_pred_stacking)\n",
        "stacking_recall = recall_score(y_test, y_pred_stacking)\n",
        "stacking_f1 = f1_score(y_test, y_pred_stacking)\n",
        "\n",
        "print(f\"\\nStacking Classifier Results:\")\n",
        "print(f\"  Accuracy:  {stacking_accuracy:.4f}\")\n",
        "print(f\"  Precision: {stacking_precision:.4f}\")\n",
        "print(f\"  Recall:    {stacking_recall:.4f}\")\n",
        "print(f\"  F1-Score:  {stacking_f1:.4f}\")\n",
        "print(f\"  Train Time: {stacking_train_time:.3f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "stacking-comparison",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all ensemble methods\n",
        "print(\"\\n\\nComprehensive Comparison: All Methods\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "comparison_data = [\n",
        "    {'Method': 'Best Individual', 'Accuracy': best_individual['Accuracy'], \n",
        "     'F1-Score': best_individual['F1-Score']},\n",
        "    {'Method': 'Hard Voting', 'Accuracy': hard_accuracy, 'F1-Score': hard_f1},\n",
        "    {'Method': 'Soft Voting', 'Accuracy': soft_accuracy, 'F1-Score': soft_f1},\n",
        "    {'Method': 'Stacking', 'Accuracy': stacking_accuracy, 'F1-Score': stacking_f1}\n",
        "]\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "methods = comparison_df['Method']\n",
        "accuracies = comparison_df['Accuracy']\n",
        "f1_scores = comparison_df['F1-Score']\n",
        "\n",
        "x = np.arange(len(methods))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax.bar(x - width/2, accuracies, width, label='Accuracy', alpha=0.8)\n",
        "bars2 = ax.bar(x + width/2, f1_scores, width, label='F1-Score', alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('Method')\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Ensemble Methods Comparison')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(methods, rotation=15, ha='right')\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3, axis='y')\n",
        "ax.set_ylim([0.9, 1.0])\n",
        "\n",
        "# Add value labels on bars\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.3f}',\n",
        "                ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Stacking often achieves the best performance by learning optimal combination!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "stacking-regressor",
      "metadata": {},
      "source": [
        "### 4.2 Stacking Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "stacking-regressor-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Stacking Regressor Demo\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Define base models\n",
        "base_regressors = [\n",
        "    ('ridge', Ridge(alpha=1.0, random_state=42)),\n",
        "    ('rf', RandomForestRegressor(n_estimators=50, random_state=42)),\n",
        "    ('gb', GradientBoostingRegressor(n_estimators=50, random_state=42))\n",
        "]\n",
        "\n",
        "# Meta-model\n",
        "meta_regressor = Ridge(alpha=1.0, random_state=42)\n",
        "\n",
        "# Create stacking regressor\n",
        "stacking_reg = StackingRegressor(\n",
        "    estimators=base_regressors,\n",
        "    final_estimator=meta_regressor,\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"Base Models:\")\n",
        "for name, model in base_regressors:\n",
        "    print(f\"  - {name}: {model.__class__.__name__}\")\n",
        "\n",
        "print(f\"\\nMeta-Model: {meta_regressor.__class__.__name__}\")\n",
        "\n",
        "# Train\n",
        "print(\"\\nTraining Stacking Regressor...\")\n",
        "stacking_reg.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Predict\n",
        "y_pred_stacking_reg = stacking_reg.predict(X_test_reg)\n",
        "\n",
        "# Evaluate\n",
        "stacking_mse = mean_squared_error(y_test_reg, y_pred_stacking_reg)\n",
        "stacking_mae = mean_absolute_error(y_test_reg, y_pred_stacking_reg)\n",
        "stacking_r2 = r2_score(y_test_reg, y_pred_stacking_reg)\n",
        "\n",
        "print(f\"\\nStacking Regressor Results:\")\n",
        "print(f\"  R\u00b2 Score: {stacking_r2:.4f}\")\n",
        "print(f\"  MSE:      {stacking_mse:.2f}\")\n",
        "print(f\"  MAE:      {stacking_mae:.2f}\")\n",
        "\n",
        "# Compare with voting and best individual\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Regression Ensemble Comparison:\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Best Individual: {best_individual_reg['Model']} (R\u00b2 = {best_individual_reg['R\u00b2 Score']:.4f})\")\n",
        "print(f\"Voting:          R\u00b2 = {voting_r2:.4f}\")\n",
        "print(f\"Stacking:        R\u00b2 = {stacking_r2:.4f}\")\n",
        "\n",
        "if stacking_r2 > voting_r2:\n",
        "    print(\"\\n\u2713 Stacking outperforms voting!\")\n",
        "else:\n",
        "    print(\"\\n\u26a0 In this case, voting performs similarly to stacking\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feature-augmentation",
      "metadata": {},
      "source": [
        "### 4.3 Stacking with Feature Augmentation\n",
        "\n",
        "**Advanced Strategy**: Include original features along with base model predictions as input to meta-model.\n",
        "\n",
        "**Benefits**:\n",
        "- Meta-model can learn direct feature relationships\n",
        "- Better performance when base models miss important patterns\n",
        "\n",
        "**Parameter**: `passthrough=True`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "stacking-passthrough",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Stacking with Feature Augmentation (passthrough=True)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create stacking with passthrough\n",
        "stacking_passthrough = StackingClassifier(\n",
        "    estimators=base_models,\n",
        "    final_estimator=LogisticRegression(max_iter=10000, random_state=42),\n",
        "    cv=5,\n",
        "    passthrough=True,  # Include original features\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"Training with passthrough=True...\")\n",
        "print(\"Meta-model receives: [base_predictions + original_features]\\n\")\n",
        "\n",
        "stacking_passthrough.fit(X_train_scaled, y_train)\n",
        "y_pred_passthrough = stacking_passthrough.predict(X_test_scaled)\n",
        "\n",
        "passthrough_accuracy = accuracy_score(y_test, y_pred_passthrough)\n",
        "passthrough_f1 = f1_score(y_test, y_pred_passthrough)\n",
        "\n",
        "print(\"Results:\")\n",
        "print(f\"  Stacking (standard):    Accuracy = {stacking_accuracy:.4f}, F1 = {stacking_f1:.4f}\")\n",
        "print(f\"  Stacking (passthrough): Accuracy = {passthrough_accuracy:.4f}, F1 = {passthrough_f1:.4f}\")\n",
        "\n",
        "if passthrough_accuracy > stacking_accuracy:\n",
        "    print(\"\\n\u2713 Passthrough improved performance!\")\n",
        "else:\n",
        "    print(\"\\n\u2192 Standard stacking is sufficient for this dataset\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Use passthrough when:\")\n",
        "print(\"   - Base models might miss important feature relationships\")\n",
        "print(\"   - Meta-model is complex enough to handle extra features\")\n",
        "print(\"   - You have sufficient training data to avoid overfitting\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "model-diversity",
      "metadata": {},
      "source": [
        "## 5. Importance of Model Diversity\n",
        "\n",
        "### 5.1 Testing Diversity Impact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "diversity-test",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Model Diversity Experiment\")\n",
        "print(\"=\"*70)\n",
        "print(\"Testing: Similar models vs Diverse models\\n\")\n",
        "\n",
        "# Scenario 1: Low diversity (all tree-based)\n",
        "low_diversity = [\n",
        "    ('dt1', DecisionTreeClassifier(max_depth=5, random_state=42)),\n",
        "    ('dt2', DecisionTreeClassifier(max_depth=10, random_state=43)),\n",
        "    ('dt3', DecisionTreeClassifier(max_depth=15, random_state=44))\n",
        "]\n",
        "\n",
        "voting_low_div = VotingClassifier(estimators=low_diversity, voting='hard')\n",
        "voting_low_div.fit(X_train_scaled, y_train)\n",
        "y_pred_low = voting_low_div.predict(X_test_scaled)\n",
        "accuracy_low = accuracy_score(y_test, y_pred_low)\n",
        "\n",
        "print(\"Low Diversity Ensemble (3 Decision Trees):\")\n",
        "for name, model in low_diversity:\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    ind_acc = accuracy_score(y_test, model.predict(X_test_scaled))\n",
        "    print(f\"  {name}: {ind_acc:.4f}\")\n",
        "print(f\"  Ensemble: {accuracy_low:.4f}\\n\")\n",
        "\n",
        "# Scenario 2: High diversity (different algorithms)\n",
        "high_diversity = [\n",
        "    ('lr', LogisticRegression(max_iter=10000, random_state=42)),\n",
        "    ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),\n",
        "    ('svm', SVC(kernel='rbf', random_state=42))\n",
        "]\n",
        "\n",
        "voting_high_div = VotingClassifier(estimators=high_diversity, voting='hard')\n",
        "voting_high_div.fit(X_train_scaled, y_train)\n",
        "y_pred_high = voting_high_div.predict(X_test_scaled)\n",
        "accuracy_high = accuracy_score(y_test, y_pred_high)\n",
        "\n",
        "print(\"High Diversity Ensemble (LogReg + RF + SVM):\")\n",
        "for name, model in high_diversity:\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    ind_acc = accuracy_score(y_test, model.predict(X_test_scaled))\n",
        "    print(f\"  {name}: {ind_acc:.4f}\")\n",
        "print(f\"  Ensemble: {accuracy_high:.4f}\\n\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"Low Diversity Ensemble:  {accuracy_low:.4f}\")\n",
        "print(f\"High Diversity Ensemble: {accuracy_high:.4f}\")\n",
        "print(f\"Improvement: {(accuracy_high - accuracy_low)*100:+.2f}%\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Diverse models make different errors \u2192 Better ensemble!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hyperparameter-tuning",
      "metadata": {},
      "source": [
        "## 6. Hyperparameter Tuning for Ensembles\n",
        "\n",
        "### 6.1 Grid Search for Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ensemble-grid-search",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Hyperparameter Tuning for Stacking Ensemble\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create stacking classifier\n",
        "stacking_tuning = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('lr', LogisticRegression(max_iter=10000, random_state=42)),\n",
        "        ('rf', RandomForestClassifier(random_state=42))\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(max_iter=10000, random_state=42),\n",
        "    cv=3\n",
        ")\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'rf__n_estimators': [50, 100],\n",
        "    'rf__max_depth': [10, 20],\n",
        "    'final_estimator__C': [0.1, 1.0, 10.0]\n",
        "}\n",
        "\n",
        "print(\"Parameter Grid:\")\n",
        "for param, values in param_grid.items():\n",
        "    print(f\"  {param}: {values}\")\n",
        "\n",
        "print(\"\\nPerforming Grid Search (this may take a minute)...\")\n",
        "\n",
        "# Grid search\n",
        "grid_search = GridSearchCV(\n",
        "    stacking_tuning,\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "start_time = time()\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "grid_time = time() - start_time\n",
        "\n",
        "print(f\"\\nGrid Search completed in {grid_time:.2f}s\")\n",
        "print(f\"\\nBest Parameters:\")\n",
        "for param, value in grid_search.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "print(f\"\\nBest CV Score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred_tuned = grid_search.predict(X_test_scaled)\n",
        "tuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
        "\n",
        "print(f\"Test Accuracy: {tuned_accuracy:.4f}\")\n",
        "print(f\"\\n\ud83d\udca1 Grid search can significantly improve ensemble performance!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "weighted-voting",
      "metadata": {},
      "source": [
        "## 7. Weighted Voting\n",
        "\n",
        "### Concept\n",
        "\n",
        "Give more weight to better-performing models:\n",
        "\n",
        "\\[\n",
        "\\hat{y} = \\sum_{i=1}^{n} w_i \\hat{y}_i\n",
        "\\]\n",
        "\n",
        "### 7.1 Implementing Weighted Voting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "weighted-voting-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Weighted Voting Demo\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Train individual models and assess performance\n",
        "print(\"Evaluating individual models for weight assignment...\\n\")\n",
        "\n",
        "weighted_models = [\n",
        "    ('lr', LogisticRegression(max_iter=10000, random_state=42)),\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('svm', SVC(probability=True, random_state=42))\n",
        "]\n",
        "\n",
        "# Calculate weights based on cross-validation accuracy\n",
        "cv_scores = []\n",
        "for name, model in weighted_models:\n",
        "    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
        "    cv_score = scores.mean()\n",
        "    cv_scores.append(cv_score)\n",
        "    print(f\"{name:5} - CV Accuracy: {cv_score:.4f}\")\n",
        "\n",
        "# Convert to weights (normalized)\n",
        "weights = np.array(cv_scores) / np.sum(cv_scores)\n",
        "\n",
        "print(f\"\\nAssigned Weights (normalized):\")\n",
        "for (name, _), weight in zip(weighted_models, weights):\n",
        "    print(f\"  {name}: {weight:.3f}\")\n",
        "\n",
        "# Create voting classifiers with and without weights\n",
        "voting_unweighted = VotingClassifier(\n",
        "    estimators=weighted_models,\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "voting_weighted = VotingClassifier(\n",
        "    estimators=weighted_models,\n",
        "    voting='soft',\n",
        "    weights=weights\n",
        ")\n",
        "\n",
        "# Train and evaluate\n",
        "voting_unweighted.fit(X_train_scaled, y_train)\n",
        "voting_weighted.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_unweighted = voting_unweighted.predict(X_test_scaled)\n",
        "y_pred_weighted = voting_weighted.predict(X_test_scaled)\n",
        "\n",
        "acc_unweighted = accuracy_score(y_test, y_pred_unweighted)\n",
        "acc_weighted = accuracy_score(y_test, y_pred_weighted)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Results:\")\n",
        "print(f\"  Unweighted Voting: {acc_unweighted:.4f}\")\n",
        "print(f\"  Weighted Voting:   {acc_weighted:.4f}\")\n",
        "print(f\"  Improvement:       {(acc_weighted - acc_unweighted)*100:+.2f}%\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Weighted voting gives more influence to better models!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "best-practices",
      "metadata": {},
      "source": [
        "## 8. Best Practices and Guidelines\n",
        "\n",
        "### 8.1 Decision Guide: When to Use Each Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decision-guide",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Ensemble Method Selection Guide\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "guide = [\n",
        "    {\n",
        "        'Method': 'Voting (Hard)',\n",
        "        'When to Use': 'Simple classification, diverse models',\n",
        "        'Pros': 'Fast, simple, robust',\n",
        "        'Cons': 'Ignores probability information',\n",
        "        'Best For': 'Quick baseline ensemble'\n",
        "    },\n",
        "    {\n",
        "        'Method': 'Voting (Soft)',\n",
        "        'When to Use': 'Models output probabilities',\n",
        "        'Pros': 'Uses probability info, often better',\n",
        "        'Cons': 'Requires probability calibration',\n",
        "        'Best For': 'Classification with calibrated models'\n",
        "    },\n",
        "    {\n",
        "        'Method': 'Voting (Regression)',\n",
        "        'When to Use': 'Regression tasks, diverse models',\n",
        "        'Pros': 'Simple, reduces variance',\n",
        "        'Cons': 'Equal weights may not be optimal',\n",
        "        'Best For': 'Regression with similar model quality'\n",
        "    },\n",
        "    {\n",
        "        'Method': 'Weighted Voting',\n",
        "        'When to Use': 'Models have different quality',\n",
        "        'Pros': 'Optimizes contribution of each model',\n",
        "        'Cons': 'Needs validation set for weights',\n",
        "        'Best For': 'Unequal model performance'\n",
        "    },\n",
        "    {\n",
        "        'Method': 'Stacking',\n",
        "        'When to Use': 'Need maximum performance',\n",
        "        'Pros': 'Learns optimal combination, flexible',\n",
        "        'Cons': 'Complex, higher overfitting risk',\n",
        "        'Best For': 'Competitions, critical applications'\n",
        "    },\n",
        "    {\n",
        "        'Method': 'Stacking (passthrough)',\n",
        "        'When to Use': 'Complex patterns, sufficient data',\n",
        "        'Pros': 'Meta-model sees original features',\n",
        "        'Cons': 'More parameters, needs more data',\n",
        "        'Best For': 'Large datasets with complex relationships'\n",
        "    },\n",
        "]\n",
        "\n",
        "guide_df = pd.DataFrame(guide)\n",
        "print(guide_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "practical-tips",
      "metadata": {},
      "source": [
        "### 8.2 Common Pitfalls and Solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pitfalls",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nCommon Pitfalls and Solutions\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "pitfalls = [\n",
        "    (\"\u274c Pitfall\", \"\u2713 Solution\"),\n",
        "    (\"-\" * 40, \"-\" * 40),\n",
        "    (\"Using similar models (low diversity)\", \"Mix different algorithm types\"),\n",
        "    (\"Not using cross-validation in stacking\", \"Always use cv parameter (5-10 folds)\"),\n",
        "    (\"Overfitting meta-model\", \"Use simple meta-model (linear/logistic)\"),\n",
        "    (\"Ignoring model calibration\", \"Calibrate probabilities before soft voting\"),\n",
        "    (\"Using too many base models\", \"3-5 diverse models usually sufficient\"),\n",
        "    (\"Not standardizing features\", \"Standardize before training ensembles\"),\n",
        "    (\"Equal weights for unequal models\", \"Use weighted voting or stacking\"),\n",
        "    (\"Stacking with leakage\", \"Never train meta-model on training preds\"),\n",
        "    (\"Ignoring computational cost\", \"Consider inference time for production\"),\n",
        "]\n",
        "\n",
        "for pitfall, solution in pitfalls:\n",
        "    print(f\"{pitfall:<45} {solution}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Summary and Quick Reference\n",
        "\n",
        "### Quick Reference\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import VotingClassifier, VotingRegressor\n",
        "from sklearn.ensemble import StackingClassifier, StackingRegressor\n",
        "\n",
        "# ===== VOTING =====\n",
        "\n",
        "# Hard Voting (Classification)\n",
        "voting_hard = VotingClassifier(\n",
        "    estimators=[('model1', clf1), ('model2', clf2)],\n",
        "    voting='hard'\n",
        ")\n",
        "\n",
        "# Soft Voting (Classification)\n",
        "voting_soft = VotingClassifier(\n",
        "    estimators=[('model1', clf1), ('model2', clf2)],\n",
        "    voting='soft',\n",
        "    weights=[1, 2]  # Optional weights\n",
        ")\n",
        "\n",
        "# Voting Regressor\n",
        "voting_reg = VotingRegressor(\n",
        "    estimators=[('model1', reg1), ('model2', reg2)],\n",
        "    weights=[1, 1]  # Optional\n",
        ")\n",
        "\n",
        "# ===== STACKING =====\n",
        "\n",
        "# Stacking Classifier\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[('model1', clf1), ('model2', clf2)],\n",
        "    final_estimator=LogisticRegression(),\n",
        "    cv=5,  # Cross-validation folds\n",
        "    stack_method='auto',  # or 'predict_proba', 'decision_function'\n",
        "    passthrough=False,  # True to include original features\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Stacking Regressor\n",
        "stacking_reg = StackingRegressor(\n",
        "    estimators=[('model1', reg1), ('model2', reg2)],\n",
        "    final_estimator=Ridge(),\n",
        "    cv=5,\n",
        "    passthrough=False,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train and predict\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "```\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "**VotingClassifier/Regressor**:\n",
        "- `estimators`: List of (name, model) tuples\n",
        "- `voting`: 'hard' or 'soft' (classification only)\n",
        "- `weights`: List of weights for each model\n",
        "- `n_jobs`: Parallel processing (-1 for all cores)\n",
        "\n",
        "**StackingClassifier/Regressor**:\n",
        "- `estimators`: List of (name, base_model) tuples\n",
        "- `final_estimator`: Meta-model (default: LogisticRegression/RidgeCV)\n",
        "- `cv`: Cross-validation strategy (int or CV splitter)\n",
        "- `stack_method`: 'auto', 'predict_proba', 'decision_function', 'predict'\n",
        "- `passthrough`: Include original features in meta-model input\n",
        "- `n_jobs`: Parallel processing\n",
        "\n",
        "### Decision Tree\n",
        "\n",
        "```\n",
        "Need Ensemble?\n",
        "    |\n",
        "    \u251c\u2500 Quick baseline \u2192 Voting (Soft for classification)\n",
        "    \u2502\n",
        "    \u251c\u2500 Maximum performance needed?\n",
        "    \u2502   YES \u2192 Stacking\n",
        "    \u2502   NO \u2192 Weighted Voting\n",
        "    \u2502\n",
        "    \u251c\u2500 Large dataset + complex patterns?\n",
        "    \u2502   YES \u2192 Stacking with passthrough=True\n",
        "    \u2502   NO \u2192 Standard Stacking\n",
        "    \u2502\n",
        "    \u2514\u2500 Models have different quality?\n",
        "        YES \u2192 Weighted Voting or Stacking\n",
        "        NO \u2192 Simple Voting\n",
        "```\n",
        "\n",
        "### Performance Guidelines\n",
        "\n",
        "**Typical Improvements**:\n",
        "- Voting: 1-3% over best individual model\n",
        "- Weighted Voting: 1-4% over simple voting\n",
        "- Stacking: 2-5% over voting (more with good meta-model)\n",
        "\n",
        "**Computational Cost** (relative to single model):\n",
        "- Voting: ~N\u00d7 (N = number of models)\n",
        "- Stacking: ~N\u00d7K + meta-model (K = CV folds)\n",
        "\n",
        "### Model Selection for Diversity\n",
        "\n",
        "**Good Combinations**:\n",
        "- Linear + Tree-based + Kernel (e.g., LogReg + RF + SVM)\n",
        "- Parametric + Non-parametric (e.g., LogReg + KNN)\n",
        "- Different principles: Ensemble + Bagging + Boosting\n",
        "\n",
        "**Avoid**:\n",
        "- Multiple models of same type (e.g., 3 Random Forests)\n",
        "- Highly correlated models\n",
        "- Weak models (<60% accuracy for classification)\n",
        "\n",
        "### Best Practices Checklist\n",
        "\n",
        "\u2713 Use 3-5 diverse base models\n",
        "\u2713 Standardize features before training\n",
        "\u2713 Use cross-validation (cv=5 or cv=10)\n",
        "\u2713 Keep meta-model simple (avoid overfitting)\n",
        "\u2713 Calibrate probabilities for soft voting\n",
        "\u2713 Validate on separate test set\n",
        "\u2713 Consider computational cost for production\n",
        "\u2713 Use weights when models have different quality\n",
        "\u2713 Try passthrough for complex datasets\n",
        "\u2713 Grid search for optimal hyperparameters\n",
        "\n",
        "### When Ensembles May Not Help\n",
        "\n",
        "- Very small datasets (<100 samples)\n",
        "- All base models perform poorly\n",
        "- Models are highly correlated\n",
        "- Single model already achieves near-optimal performance\n",
        "- Strict latency requirements in production\n",
        "\n",
        "### Further Reading\n",
        "\n",
        "- **Stacking Paper**: Wolpert (1992) - \"Stacked Generalization\"\n",
        "- **Ensemble Methods Book**: Zhou (2012) - \"Ensemble Methods: Foundations and Algorithms\"\n",
        "- **Sklearn Documentation**: https://scikit-learn.org/stable/modules/ensemble.html\n",
        "- **Kaggle Ensembling Guide**: Various competition winning solutions\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Advanced stacking strategies (multi-level stacking)\n",
        "- Blending vs Stacking\n",
        "- Ensemble selection and pruning\n",
        "- Bayesian Model Averaging\n",
        "- Neural Network ensembles\n",
        "- Production deployment of ensembles"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}