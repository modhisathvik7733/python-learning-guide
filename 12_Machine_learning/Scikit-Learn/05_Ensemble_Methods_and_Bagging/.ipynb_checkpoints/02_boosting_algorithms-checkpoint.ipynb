{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Boosting Algorithms: AdaBoost and Gradient Boosting\n",
        "\n",
        "## Overview\n",
        "\n",
        "**Boosting** is an ensemble method that combines weak learners sequentially to create a strong learner. Unlike bagging (parallel training), boosting trains models iteratively, where each new model focuses on correcting errors made by previous models.\n",
        "\n",
        "## Key Difference: Bagging vs Boosting\n",
        "\n",
        "```\n",
        "Bagging:                          Boosting:\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2510 errors\u2192 \u250c\u2500\u2500\u2500\u2500\u2500\u2510 errors\u2192 \u250c\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502 M1  \u2502  \u2502 M2  \u2502  \u2502 M3  \u2502        \u2502 M1  \u2502  \u2500\u2500\u2500\u2500\u2192 \u2502 M2  \u2502  \u2500\u2500\u2500\u2500\u2192 \u2502 M3  \u2502\n",
        "\u2514\u2500\u2500\u252c\u2500\u2500\u2518  \u2514\u2500\u2500\u252c\u2500\u2500\u2518  \u2514\u2500\u2500\u252c\u2500\u2500\u2518        \u2514\u2500\u2500\u252c\u2500\u2500\u2518         \u2514\u2500\u2500\u252c\u2500\u2500\u2518         \u2514\u2500\u2500\u252c\u2500\u2500\u2518\n",
        "   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "         Average                           Weighted Sum\n",
        "\n",
        "\u2022 Parallel training            \u2022 Sequential training\n",
        "\u2022 Reduces variance             \u2022 Reduces bias\n",
        "\u2022 Works with any model         \u2022 Usually weak learners (shallow trees)\n",
        "```\n",
        "\n",
        "## Boosting Algorithms Covered\n",
        "\n",
        "1. **AdaBoost** (Adaptive Boosting)\n",
        "   - Adjusts sample weights based on errors\n",
        "   - Simple and intuitive\n",
        "   - Sensitive to outliers\n",
        "\n",
        "2. **Gradient Boosting**\n",
        "   - Fits new models to residual errors\n",
        "   - More flexible (any differentiable loss)\n",
        "   - Often superior performance\n",
        "\n",
        "## Topics Covered\n",
        "\n",
        "- AdaBoost algorithm and mathematics\n",
        "- Gradient Boosting intuition\n",
        "- sklearn implementations\n",
        "- Hyperparameter tuning\n",
        "- Learning rate and regularization\n",
        "- Comparison with bagging\n",
        "- When to use which algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Boosting algorithms\n",
        "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
        "\n",
        "# Base models\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "\n",
        "# Utilities\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, classification_report, confusion_matrix\n",
        "from sklearn.datasets import make_classification, make_regression, load_wine, load_diabetes, load_breast_cancer\n",
        "\n",
        "np.random.seed(42)\n",
        "sns.set_style('whitegrid')\n",
        "print(\"\u2713 Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adaboost-intro",
      "metadata": {},
      "source": [
        "## 1. AdaBoost (Adaptive Boosting)\n",
        "\n",
        "### Mathematical Foundation\n",
        "\n",
        "**Algorithm**:\n",
        "\n",
        "1. Initialize sample weights: \\(w_i = \\frac{1}{n}\\) for all samples\n",
        "\n",
        "2. For \\(m = 1, 2, ..., M\\):\n",
        "   - Train weak learner \\(h_m\\) on weighted samples\n",
        "   - Calculate weighted error: \\(\\epsilon_m = \\sum_{i: h_m(x_i) \\neq y_i} w_i\\)\n",
        "   - Calculate model weight: \\(\\alpha_m = \\frac{1}{2}\\ln\\left(\\frac{1-\\epsilon_m}{\\epsilon_m}\\right)\\)\n",
        "   - Update sample weights:\n",
        "     \\[\n",
        "     w_i \\leftarrow w_i \\times \\begin{cases}\n",
        "     e^{\\alpha_m} & \\text{if misclassified} \\\\\n",
        "     e^{-\\alpha_m} & \\text{if correct}\n",
        "     \\end{cases}\n",
        "     \\]\n",
        "   - Normalize weights: \\(w_i \\leftarrow \\frac{w_i}{\\sum_j w_j}\\)\n",
        "\n",
        "3. Final prediction: \\(H(x) = \\text{sign}\\left(\\sum_{m=1}^{M} \\alpha_m h_m(x)\\right)\\)\n",
        "\n",
        "**Key Insight**: \n",
        "- Misclassified samples get higher weights \u2192 next model focuses on them\n",
        "- Better models get higher \\(\\alpha_m\\) (more voting power)\n",
        "\n",
        "### 1.1 AdaBoost Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adaboost-basic",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load breast cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X_cancer = cancer.data\n",
        "y_cancer = cancer.target\n",
        "\n",
        "print(\"Breast Cancer Dataset\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_cancer.shape[0]}\")\n",
        "print(f\"Features: {X_cancer.shape[1]}\")\n",
        "print(f\"Classes: {cancer.target_names}\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adaboost-train",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nAdaBoost Training\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Single weak learner (decision stump = tree with max_depth=1)\n",
        "weak_learner = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "weak_learner.fit(X_train, y_train)\n",
        "weak_acc = weak_learner.score(X_test, y_test)\n",
        "\n",
        "print(f\"Single Weak Learner (Stump):\")\n",
        "print(f\"  Test Accuracy: {weak_acc:.4f}\")\n",
        "\n",
        "# AdaBoost with 50 stumps\n",
        "adaboost = AdaBoostClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=1),\n",
        "    n_estimators=50,\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "adaboost.fit(X_train, y_train)\n",
        "\n",
        "train_acc = adaboost.score(X_train, y_train)\n",
        "test_acc = adaboost.score(X_test, y_test)\n",
        "\n",
        "print(f\"\\nAdaBoost (50 stumps):\")\n",
        "print(f\"  Train Accuracy: {train_acc:.4f}\")\n",
        "print(f\"  Test Accuracy:  {test_acc:.4f}\")\n",
        "print(f\"  Improvement over single stump: {test_acc - weak_acc:.4f}\")\n",
        "\n",
        "# Model weights (alpha values)\n",
        "estimator_weights = adaboost.estimator_weights_\n",
        "print(f\"\\nEstimator weights (first 10): {estimator_weights[:10]}\")\n",
        "print(f\"Sum of weights: {estimator_weights.sum():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adaboost-iteration",
      "metadata": {},
      "source": [
        "### 1.2 AdaBoost Learning Progression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adaboost-staged",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"AdaBoost Staged Predictions\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Track performance as estimators are added\n",
        "n_estimators_range = list(range(1, 51))\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "for n_est in n_estimators_range:\n",
        "    ada = AdaBoostClassifier(\n",
        "        estimator=DecisionTreeClassifier(max_depth=1),\n",
        "        n_estimators=n_est,\n",
        "        random_state=42\n",
        "    )\n",
        "    ada.fit(X_train, y_train)\n",
        "    train_scores.append(ada.score(X_train, y_train))\n",
        "    test_scores.append(ada.score(X_test, y_test))\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(n_estimators_range, train_scores, label='Train Score', linewidth=2)\n",
        "plt.plot(n_estimators_range, test_scores, label='Test Score', linewidth=2)\n",
        "plt.xlabel('Number of Estimators')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('AdaBoost Learning Curve')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "best_n = n_estimators_range[np.argmax(test_scores)]\n",
        "best_score = max(test_scores)\n",
        "print(f\"\\nBest test score: {best_score:.4f} at n_estimators={best_n}\")\n",
        "print(f\"\\n\ud83d\udca1 Notice: Performance improves rapidly, then plateaus\")\n",
        "print(f\"   May eventually overfit if too many estimators\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adaboost-weights",
      "metadata": {},
      "source": [
        "### 1.3 Sample Weights Evolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sample-weights-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Sample Weights Visualization\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Train AdaBoost and access sample weights during training\n",
        "# Note: sklearn doesn't expose intermediate weights, so we'll show final distribution\n",
        "\n",
        "# Get staged predictions to find hard samples\n",
        "ada_viz = AdaBoostClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=1),\n",
        "    n_estimators=10,\n",
        "    random_state=42\n",
        ")\n",
        "ada_viz.fit(X_train, y_train)\n",
        "\n",
        "# Identify misclassified samples\n",
        "y_pred_train = ada_viz.predict(X_train)\n",
        "errors = (y_pred_train != y_train)\n",
        "\n",
        "print(f\"Training samples: {len(y_train)}\")\n",
        "print(f\"Misclassified: {errors.sum()} ({errors.sum()/len(y_train)*100:.1f}%)\")\n",
        "\n",
        "# Compare with single stump\n",
        "stump = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "stump.fit(X_train, y_train)\n",
        "stump_errors = (stump.predict(X_train) != y_train)\n",
        "\n",
        "print(f\"\\nSingle stump misclassified: {stump_errors.sum()} ({stump_errors.sum()/len(y_train)*100:.1f}%)\")\n",
        "print(f\"\\n\u2713 AdaBoost reduced training errors significantly!\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 How AdaBoost works:\")\n",
        "print(\"   1. Easy samples get lower weights (downweighted)\")\n",
        "print(\"   2. Hard samples get higher weights (focused on)\")\n",
        "print(\"   3. Each new model tries to fix previous mistakes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adaboost-regression",
      "metadata": {},
      "source": [
        "### 1.4 AdaBoost for Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adaboost-regressor",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X_diabetes = diabetes.data\n",
        "y_diabetes = diabetes.target\n",
        "\n",
        "print(\"AdaBoost Regression - Diabetes Dataset\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
        "    X_diabetes, y_diabetes, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Single tree\n",
        "single_tree = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
        "single_tree.fit(X_train_r, y_train_r)\n",
        "y_pred_single = single_tree.predict(X_test_r)\n",
        "r2_single = r2_score(y_test_r, y_pred_single)\n",
        "rmse_single = np.sqrt(mean_squared_error(y_test_r, y_pred_single))\n",
        "\n",
        "print(f\"Single Tree (depth=3):\")\n",
        "print(f\"  R\u00b2 Score: {r2_single:.4f}\")\n",
        "print(f\"  RMSE:     {rmse_single:.2f}\")\n",
        "\n",
        "# AdaBoost regressor\n",
        "ada_reg = AdaBoostRegressor(\n",
        "    estimator=DecisionTreeRegressor(max_depth=3),\n",
        "    n_estimators=100,\n",
        "    learning_rate=1.0,\n",
        "    loss='linear',  # 'linear', 'square', 'exponential'\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "ada_reg.fit(X_train_r, y_train_r)\n",
        "y_pred_ada = ada_reg.predict(X_test_r)\n",
        "r2_ada = r2_score(y_test_r, y_pred_ada)\n",
        "rmse_ada = np.sqrt(mean_squared_error(y_test_r, y_pred_ada))\n",
        "\n",
        "print(f\"\\nAdaBoost (100 trees):\")\n",
        "print(f\"  R\u00b2 Score: {r2_ada:.4f}\")\n",
        "print(f\"  RMSE:     {rmse_ada:.2f}\")\n",
        "print(f\"\\n\u2713 Improvement: R\u00b2 +{r2_ada - r2_single:.4f}, RMSE -{rmse_single - rmse_ada:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gradient-boosting",
      "metadata": {},
      "source": [
        "## 2. Gradient Boosting\n",
        "\n",
        "### Intuition and Mathematics\n",
        "\n",
        "**Key Idea**: Fit new models to the **residuals** (errors) of the ensemble.\n",
        "\n",
        "**Algorithm**:\n",
        "\n",
        "1. Initialize with constant prediction:\n",
        "   \\[\n",
        "   F_0(x) = \\arg\\min_\\gamma \\sum_{i=1}^{n} L(y_i, \\gamma)\n",
        "   \\]\n",
        "\n",
        "2. For \\(m = 1, 2, ..., M\\):\n",
        "   \n",
        "   a. Compute pseudo-residuals:\n",
        "   \\[\n",
        "   r_{im} = -\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F=F_{m-1}}\n",
        "   \\]\n",
        "   \n",
        "   b. Fit weak learner \\(h_m(x)\\) to residuals \\(r_{im}\\)\n",
        "   \n",
        "   c. Find optimal step size:\n",
        "   \\[\n",
        "   \\gamma_m = \\arg\\min_\\gamma \\sum_{i=1}^{n} L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\n",
        "   \\]\n",
        "   \n",
        "   d. Update model:\n",
        "   \\[\n",
        "   F_m(x) = F_{m-1}(x) + \\nu \\gamma_m h_m(x)\n",
        "   \\]\n",
        "   where \\(\\nu\\) is the learning rate\n",
        "\n",
        "**For Regression (squared loss)**:\n",
        "- Residuals = actual - predicted\n",
        "- Simply fit trees to errors!\n",
        "\n",
        "**For Classification (log loss)**:\n",
        "- Residuals = gradient of log loss\n",
        "- Fit trees to probability gradients\n",
        "\n",
        "### 2.1 Gradient Boosting Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gb-classification",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Gradient Boosting Classification\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Gradient Boosting Classifier\n",
        "gb_clf = GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    subsample=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "gb_clf.fit(X_train, y_train)\n",
        "\n",
        "train_acc_gb = gb_clf.score(X_train, y_train)\n",
        "test_acc_gb = gb_clf.score(X_test, y_test)\n",
        "\n",
        "print(f\"Gradient Boosting:\")\n",
        "print(f\"  Train Accuracy: {train_acc_gb:.4f}\")\n",
        "print(f\"  Test Accuracy:  {test_acc_gb:.4f}\")\n",
        "\n",
        "# Compare with AdaBoost\n",
        "print(f\"\\nComparison:\")\n",
        "print(f\"  AdaBoost Test:     {test_acc:.4f}\")\n",
        "print(f\"  Gradient Boost Test: {test_acc_gb:.4f}\")\n",
        "print(f\"  Difference:        {test_acc_gb - test_acc:+.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "learning-rate",
      "metadata": {},
      "source": [
        "### 2.2 Learning Rate Effect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "learning-rate-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Effect of Learning Rate\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "learning_rates = [0.01, 0.05, 0.1, 0.5, 1.0]\n",
        "results_lr = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    gb = GradientBoostingClassifier(\n",
        "        n_estimators=100,\n",
        "        learning_rate=lr,\n",
        "        max_depth=3,\n",
        "        random_state=42\n",
        "    )\n",
        "    gb.fit(X_train, y_train)\n",
        "    \n",
        "    train_score = gb.score(X_train, y_train)\n",
        "    test_score = gb.score(X_test, y_test)\n",
        "    \n",
        "    results_lr.append({\n",
        "        'Learning Rate': lr,\n",
        "        'Train': train_score,\n",
        "        'Test': test_score,\n",
        "        'Overfit': train_score - test_score\n",
        "    })\n",
        "    \n",
        "    print(f\"LR={lr:.2f}: Train={train_score:.4f}, Test={test_score:.4f}, Overfit={train_score-test_score:.4f}\")\n",
        "\n",
        "# Plot\n",
        "df_lr = pd.DataFrame(results_lr)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Scores\n",
        "axes[0].plot(df_lr['Learning Rate'], df_lr['Train'], 'o-', label='Train', linewidth=2)\n",
        "axes[0].plot(df_lr['Learning Rate'], df_lr['Test'], 's-', label='Test', linewidth=2)\n",
        "axes[0].set_xlabel('Learning Rate')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('Performance vs Learning Rate')\n",
        "axes[0].set_xscale('log')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Overfitting\n",
        "axes[1].plot(df_lr['Learning Rate'], df_lr['Overfit'], 'o-', linewidth=2, color='red')\n",
        "axes[1].set_xlabel('Learning Rate')\n",
        "axes[1].set_ylabel('Train - Test (Overfitting)')\n",
        "axes[1].set_title('Overfitting vs Learning Rate')\n",
        "axes[1].set_xscale('log')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Learning Rate Tradeoff:\")\n",
        "print(\"   - High LR (1.0): Fast convergence, may overfit\")\n",
        "print(\"   - Low LR (0.01): Slow convergence, better generalization\")\n",
        "print(\"   - Need to increase n_estimators with lower learning rate\")\n",
        "print(\"   - Typical: LR=0.1 with 100-1000 estimators\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gb-regression",
      "metadata": {},
      "source": [
        "### 2.3 Gradient Boosting Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gb-regressor",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Gradient Boosting Regression\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Gradient Boosting Regressor\n",
        "gb_reg = GradientBoostingRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    subsample=0.8,  # Stochastic gradient boosting\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "gb_reg.fit(X_train_r, y_train_r)\n",
        "\n",
        "y_pred_gb = gb_reg.predict(X_test_r)\n",
        "r2_gb = r2_score(y_test_r, y_pred_gb)\n",
        "rmse_gb = np.sqrt(mean_squared_error(y_test_r, y_pred_gb))\n",
        "\n",
        "print(f\"Gradient Boosting:\")\n",
        "print(f\"  R\u00b2 Score: {r2_gb:.4f}\")\n",
        "print(f\"  RMSE:     {rmse_gb:.2f}\")\n",
        "\n",
        "print(f\"\\nComparison:\")\n",
        "print(f\"  Single Tree:    R\u00b2={r2_single:.4f}, RMSE={rmse_single:.2f}\")\n",
        "print(f\"  AdaBoost:       R\u00b2={r2_ada:.4f}, RMSE={rmse_ada:.2f}\")\n",
        "print(f\"  Gradient Boost: R\u00b2={r2_gb:.4f}, RMSE={rmse_gb:.2f}\")\n",
        "\n",
        "# Visualize predictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test_r, y_pred_gb, alpha=0.6)\n",
        "plt.plot([y_test_r.min(), y_test_r.max()], \n",
        "         [y_test_r.min(), y_test_r.max()], 'r--', lw=2)\n",
        "plt.xlabel('True Values')\n",
        "plt.ylabel('Predictions')\n",
        "plt.title(f'Gradient Boosting Predictions (R\u00b2={r2_gb:.3f})')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "staged-predictions",
      "metadata": {},
      "source": [
        "### 2.4 Staged Predictions (Residual Reduction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "staged-predictions-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Gradient Boosting Staged Predictions\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Train GB with more estimators for visualization\n",
        "gb_staged = GradientBoostingRegressor(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "gb_staged.fit(X_train_r, y_train_r)\n",
        "\n",
        "# Get predictions at each stage\n",
        "train_scores_staged = []\n",
        "test_scores_staged = []\n",
        "\n",
        "for i, (train_pred, test_pred) in enumerate(zip(\n",
        "    gb_staged.staged_predict(X_train_r),\n",
        "    gb_staged.staged_predict(X_test_r)\n",
        ")):\n",
        "    train_scores_staged.append(mean_squared_error(y_train_r, train_pred))\n",
        "    test_scores_staged.append(mean_squared_error(y_test_r, test_pred))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_scores_staged, label='Train MSE', linewidth=2)\n",
        "plt.plot(test_scores_staged, label='Test MSE', linewidth=2)\n",
        "plt.xlabel('Number of Boosting Iterations')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Gradient Boosting: Error Reduction Over Iterations')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "best_iter = np.argmin(test_scores_staged) + 1\n",
        "best_mse = min(test_scores_staged)\n",
        "\n",
        "print(f\"\\nBest iteration: {best_iter}\")\n",
        "print(f\"Best test MSE: {best_mse:.2f}\")\n",
        "print(f\"\\n\ud83d\udca1 Notice:\")\n",
        "print(\"   - Train error decreases monotonically\")\n",
        "print(\"   - Test error may increase after optimal point (overfitting)\")\n",
        "print(\"   - Use early stopping or cross-validation to find best n_estimators\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feature-importance",
      "metadata": {},
      "source": [
        "### 2.5 Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feature-importance-gb",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Gradient Boosting Feature Importance\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = gb_reg.feature_importances_\n",
        "feature_names = diabetes.feature_names\n",
        "\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': feature_importance\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(importance_df.to_string(index=False))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(importance_df['Feature'], importance_df['Importance'], alpha=0.8)\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Gradient Boosting Feature Importance')\n",
        "plt.grid(alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Feature importance in boosting:\")\n",
        "print(\"   - Based on total reduction in loss from splits on that feature\")\n",
        "print(\"   - Aggregated across all trees in the ensemble\")\n",
        "print(\"   - More splits on a feature = higher importance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comparison",
      "metadata": {},
      "source": [
        "## 3. Comprehensive Comparison\n",
        "\n",
        "### 3.1 AdaBoost vs Gradient Boosting vs Bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comprehensive-comparison",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Comprehensive Ensemble Comparison\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load wine dataset for multiclass\n",
        "wine = load_wine()\n",
        "X_wine = wine.data\n",
        "y_wine = wine.target\n",
        "\n",
        "X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(\n",
        "    X_wine, y_wine, test_size=0.3, random_state=42, stratify=y_wine\n",
        ")\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Single Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
        "    'Bagging': BaggingClassifier(\n",
        "        estimator=DecisionTreeClassifier(),\n",
        "        n_estimators=100,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "    'Random Forest': RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "    'AdaBoost': AdaBoostClassifier(\n",
        "        estimator=DecisionTreeClassifier(max_depth=1),\n",
        "        n_estimators=100,\n",
        "        random_state=42\n",
        "    ),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=3,\n",
        "        random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "results_comparison = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Train\n",
        "    model.fit(X_train_w, y_train_w)\n",
        "    \n",
        "    # Evaluate\n",
        "    train_score = model.score(X_train_w, y_train_w)\n",
        "    test_score = model.score(X_test_w, y_test_w)\n",
        "    \n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(model, X_wine, y_wine, cv=5)\n",
        "    cv_mean = cv_scores.mean()\n",
        "    cv_std = cv_scores.std()\n",
        "    \n",
        "    results_comparison.append({\n",
        "        'Model': name,\n",
        "        'Train': train_score,\n",
        "        'Test': test_score,\n",
        "        'CV Mean': cv_mean,\n",
        "        'CV Std': cv_std,\n",
        "        'Overfit': train_score - test_score\n",
        "    })\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Train: {train_score:.4f}\")\n",
        "    print(f\"  Test:  {test_score:.4f}\")\n",
        "    print(f\"  CV:    {cv_mean:.4f} \u00b1 {cv_std:.4f}\")\n",
        "\n",
        "# Summary table\n",
        "df_comparison = pd.DataFrame(results_comparison)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Summary:\")\n",
        "print(df_comparison.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comparison-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Test scores\n",
        "axes[0].barh(df_comparison['Model'], df_comparison['Test'], alpha=0.8)\n",
        "axes[0].set_xlabel('Test Accuracy')\n",
        "axes[0].set_title('Test Performance Comparison')\n",
        "axes[0].grid(alpha=0.3, axis='x')\n",
        "\n",
        "# Overfitting\n",
        "colors = ['green' if x < 0.05 else 'orange' if x < 0.1 else 'red' \n",
        "          for x in df_comparison['Overfit']]\n",
        "axes[1].barh(df_comparison['Model'], df_comparison['Overfit'], color=colors, alpha=0.8)\n",
        "axes[1].set_xlabel('Train - Test (Overfitting)')\n",
        "axes[1].set_title('Overfitting Comparison')\n",
        "axes[1].axvline(x=0.05, color='orange', linestyle='--', alpha=0.5)\n",
        "axes[1].axvline(x=0.1, color='red', linestyle='--', alpha=0.5)\n",
        "axes[1].grid(alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Key Observations:\")\n",
        "print(\"   - Boosting methods (AdaBoost, GB) often achieve highest accuracy\")\n",
        "print(\"   - Bagging methods (RF, Bagging) show less overfitting\")\n",
        "print(\"   - Single tree overfits most\")\n",
        "print(\"   - Gradient Boosting typically best but requires tuning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hyperparameters",
      "metadata": {},
      "source": [
        "## 4. Hyperparameter Tuning Guide\n",
        "\n",
        "### 4.1 Key Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hyperparameter-guide",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Boosting Hyperparameters Guide\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "hyperparams = \"\"\"\n",
        "AdaBoost Parameters:\n",
        "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
        "n_estimators:     Number of weak learners\n",
        "  Default: 50\n",
        "  Range: 50-500\n",
        "  Effect: More = better, diminishing returns\n",
        "\n",
        "learning_rate:    Shrinks contribution of each classifier\n",
        "  Default: 1.0\n",
        "  Range: 0.01-1.0\n",
        "  Effect: Lower = needs more estimators, better generalization\n",
        "\n",
        "estimator:        Base weak learner\n",
        "  Default: DecisionTreeClassifier(max_depth=1)\n",
        "  Typical: Decision stumps (depth=1) or shallow trees (depth=2-3)\n",
        "\n",
        "Gradient Boosting Parameters:\n",
        "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
        "n_estimators:     Number of boosting stages\n",
        "  Default: 100\n",
        "  Range: 50-1000+\n",
        "  Effect: More = better until overfitting\n",
        "  \ud83d\udca1 Use early stopping or staged predictions\n",
        "\n",
        "learning_rate:    Shrinks contribution of each tree\n",
        "  Default: 0.1\n",
        "  Range: 0.01-0.3\n",
        "  Effect: Lower = more robust, needs more estimators\n",
        "  \ud83d\udca1 Typical: 0.1 with 100-500 estimators, or 0.01 with 1000+\n",
        "\n",
        "max_depth:        Maximum depth of trees\n",
        "  Default: 3\n",
        "  Range: 2-8\n",
        "  Effect: Controls model complexity\n",
        "  \ud83d\udca1 Shallow trees work best (3-5)\n",
        "\n",
        "subsample:        Fraction of samples for each tree\n",
        "  Default: 1.0\n",
        "  Range: 0.5-1.0\n",
        "  Effect: < 1.0 = Stochastic GB, reduces variance\n",
        "  \ud83d\udca1 0.8 often works well\n",
        "\n",
        "min_samples_split: Minimum samples to split node\n",
        "  Default: 2\n",
        "  Range: 2-20\n",
        "  Effect: Higher = more regularization\n",
        "\n",
        "min_samples_leaf: Minimum samples in leaf\n",
        "  Default: 1\n",
        "  Range: 1-20\n",
        "  Effect: Higher = smoother predictions\n",
        "\n",
        "max_features:     Features to consider for splits\n",
        "  Default: None (all features)\n",
        "  Options: 'sqrt', 'log2', int, float\n",
        "  Effect: Adds randomness, can improve generalization\n",
        "\"\"\"\n",
        "\n",
        "print(hyperparams)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "decision-guide",
      "metadata": {},
      "source": [
        "## 5. When to Use Which Algorithm\n",
        "\n",
        "### 5.1 Decision Guide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "when-to-use",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Algorithm Selection Guide\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "guide = \"\"\"\n",
        "Use ADABOOST when:\n",
        "\u2713 Simple interpretability needed (weights show sample difficulty)\n",
        "\u2713 Binary classification with well-separated classes\n",
        "\u2713 Quick baseline ensemble method\n",
        "\u2713 Data has few outliers (AdaBoost sensitive to outliers)\n",
        "\u2717 Data is noisy or has many outliers\n",
        "\u2717 Computational resources are very limited\n",
        "\n",
        "Use GRADIENT BOOSTING when:\n",
        "\u2713 Maximum accuracy is priority\n",
        "\u2713 Willing to tune hyperparameters carefully\n",
        "\u2713 Have computational resources for training\n",
        "\u2713 Need flexible loss functions\n",
        "\u2713 Can use cross-validation or early stopping\n",
        "\u2717 Need very fast predictions (deep trees = slower)\n",
        "\u2717 Cannot tune hyperparameters\n",
        "\n",
        "Use BAGGING/RANDOM FOREST when:\n",
        "\u2713 Want robust model with less tuning\n",
        "\u2713 Need fast training (parallel)\n",
        "\u2713 High variance model (deep trees)\n",
        "\u2713 Want to reduce overfitting\n",
        "\u2717 Model has high bias (underfits)\n",
        "\n",
        "General Hierarchy (for trees):\n",
        "1. Try Random Forest first (robust baseline)\n",
        "2. Try Gradient Boosting for better accuracy\n",
        "3. Try modern boosting (XGBoost, LightGBM, CatBoost)\n",
        "4. AdaBoost as alternative for binary classification\n",
        "\n",
        "Performance Ranking (typical):\n",
        "XGBoost/LightGBM > Gradient Boosting > AdaBoost \u2248 Random Forest > Bagging > Single Tree\n",
        "\n",
        "Training Speed:\n",
        "Random Forest (parallel) > Bagging > AdaBoost > Gradient Boosting\n",
        "\n",
        "Resistance to Overfitting:\n",
        "Random Forest > Bagging > Gradient Boosting (with tuning) > AdaBoost > Single Tree\n",
        "\"\"\"\n",
        "\n",
        "print(guide)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Summary and Best Practices\n",
        "\n",
        "### Quick Reference\n",
        "\n",
        "```python\n",
        "# AdaBoost\n",
        "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
        "\n",
        "ada = AdaBoostClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=1),  # Stump\n",
        "    n_estimators=50,\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Gradient Boosting\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gb = GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    subsample=0.8,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features='sqrt',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Staged predictions for early stopping\n",
        "for i, pred in enumerate(gb.staged_predict(X_test)):\n",
        "    if early_stopping_condition:\n",
        "        break\n",
        "```\n",
        "\n",
        "### Key Differences\n",
        "\n",
        "| Aspect | AdaBoost | Gradient Boosting | Bagging/RF |\n",
        "|--------|----------|-------------------|------------|\n",
        "| Training | Sequential | Sequential | Parallel |\n",
        "| Reduces | Bias | Bias | Variance |\n",
        "| Base Model | Weak (stumps) | Weak (shallow) | Strong (deep) |\n",
        "| Weights | Sample weights | Residuals | Equal |\n",
        "| Overfitting | Resistant | Prone (needs tuning) | Resistant |\n",
        "| Speed | Medium | Slow | Fast |\n",
        "| Accuracy | Good | Excellent | Good |\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "**AdaBoost:**\n",
        "1. \u2713 Start with stumps (max_depth=1)\n",
        "2. \u2713 Try 50-200 estimators\n",
        "3. \u2713 Lower learning_rate if overfitting\n",
        "4. \u2713 Remove outliers first\n",
        "\n",
        "**Gradient Boosting:**\n",
        "1. \u2713 Start with: `n_estimators=100, learning_rate=0.1, max_depth=3`\n",
        "2. \u2713 Use `subsample=0.8` for stochastic GB\n",
        "3. \u2713 Monitor staged predictions for overfitting\n",
        "4. \u2713 Lower learning_rate + more estimators = better generalization\n",
        "5. \u2713 Use cross-validation to find optimal n_estimators\n",
        "6. \u2713 Keep trees shallow (depth 3-5)\n",
        "\n",
        "### Common Mistakes\n",
        "\n",
        "1. \u274c Using deep base learners with AdaBoost\n",
        "2. \u274c Not tuning learning_rate\n",
        "3. \u274c Too many estimators without validation\n",
        "4. \u274c Using boosting when bagging would suffice\n",
        "5. \u274c Not standardizing features (for some implementations)\n",
        "6. \u274c Ignoring outliers in AdaBoost\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- XGBoost, LightGBM, CatBoost (advanced boosting)\n",
        "- Stacking and Voting ensembles\n",
        "- Hyperparameter optimization (GridSearch, RandomSearch, Bayesian)\n",
        "- Feature engineering for boosting\n",
        "- Early stopping techniques"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}