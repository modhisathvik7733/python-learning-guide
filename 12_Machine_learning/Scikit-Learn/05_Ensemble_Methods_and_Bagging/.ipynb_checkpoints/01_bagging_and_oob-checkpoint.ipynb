{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Bagging and Out-of-Bag Evaluation\n",
        "\n",
        "## Overview\n",
        "\n",
        "**Bagging** (Bootstrap AGGregating) is an ensemble method that improves model stability and accuracy by:\n",
        "1. Creating multiple bootstrap samples of the training data\n",
        "2. Training a separate model on each sample\n",
        "3. Aggregating predictions (voting for classification, averaging for regression)\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "### Bootstrap Sampling\n",
        "\n",
        "- Draw \\(n\\) samples **with replacement** from training set of size \\(n\\)\n",
        "- Each bootstrap sample contains ~63.2% unique samples\n",
        "- Remaining ~36.8% are \"out-of-bag\" (OOB) samples\n",
        "\n",
        "### Why Bagging Works\n",
        "\n",
        "**Variance Reduction**: If we have \\(B\\) independent models with variance \\(\\sigma^2\\), the ensemble variance is:\n",
        "\n",
        "\\[\n",
        "\\text{Var}(\\text{ensemble}) = \\frac{\\sigma^2}{B}\n",
        "\\]\n",
        "\n",
        "In practice, models are correlated, so:\n",
        "\n",
        "\\[\n",
        "\\text{Var}(\\text{ensemble}) = \\rho\\sigma^2 + \\frac{1-\\rho}{B}\\sigma^2\n",
        "\\]\n",
        "\n",
        "where \\(\\rho\\) is the correlation between models.\n",
        "\n",
        "**Key Insight**: Bagging reduces variance (overfitting) without increasing bias!\n",
        "\n",
        "### Out-of-Bag (OOB) Evaluation\n",
        "\n",
        "- Each sample is OOB for ~37% of models\n",
        "- Can evaluate each sample using only models that didn't see it\n",
        "- Provides unbiased error estimate without separate validation set\n",
        "- Similar to cross-validation but \"free\"\n",
        "\n",
        "## Topics Covered\n",
        "\n",
        "1. BaggingClassifier and BaggingRegressor\n",
        "2. Bootstrap sampling mechanics\n",
        "3. Out-of-Bag scoring\n",
        "4. Random Forests as specialized bagging\n",
        "5. Hyperparameter tuning\n",
        "6. When to use bagging"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensemble methods\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor\n",
        "\n",
        "# Base estimators\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Utilities\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, classification_report, confusion_matrix\n",
        "from sklearn.datasets import make_classification, make_regression, load_wine, load_diabetes, load_breast_cancer\n",
        "\n",
        "np.random.seed(42)\n",
        "sns.set_style('whitegrid')\n",
        "print(\"\u2713 Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bootstrap-demo",
      "metadata": {},
      "source": [
        "## 1. Understanding Bootstrap Sampling\n",
        "\n",
        "### 1.1 Bootstrap Mechanics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bootstrap-visual",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Bootstrap Sampling Demonstration\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Simple dataset\n",
        "original_data = np.arange(10)\n",
        "n_samples = len(original_data)\n",
        "\n",
        "print(f\"Original dataset: {original_data}\")\n",
        "print(f\"Size: {n_samples}\\n\")\n",
        "\n",
        "# Create bootstrap samples\n",
        "n_bootstrap = 5\n",
        "for i in range(n_bootstrap):\n",
        "    # Sample with replacement\n",
        "    bootstrap_sample = np.random.choice(original_data, size=n_samples, replace=True)\n",
        "    \n",
        "    # Find unique samples and OOB samples\n",
        "    unique_samples = np.unique(bootstrap_sample)\n",
        "    oob_samples = np.setdiff1d(original_data, bootstrap_sample)\n",
        "    \n",
        "    print(f\"Bootstrap {i+1}: {bootstrap_sample}\")\n",
        "    print(f\"  Unique: {unique_samples} ({len(unique_samples)}/{n_samples} = {len(unique_samples)/n_samples:.1%})\")\n",
        "    print(f\"  OOB:    {oob_samples} ({len(oob_samples)}/{n_samples} = {len(oob_samples)/n_samples:.1%})\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bootstrap-probability",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Theoretical probability of being in bootstrap sample\n",
        "print(\"Theoretical Analysis\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Probability of NOT being selected in one draw: (n-1)/n\n",
        "# Probability of NOT being selected in n draws: ((n-1)/n)^n\n",
        "# As n \u2192 \u221e, this approaches 1/e \u2248 0.368\n",
        "\n",
        "n_values = [10, 50, 100, 500, 1000]\n",
        "for n in n_values:\n",
        "    prob_not_selected = ((n-1)/n)**n\n",
        "    prob_selected = 1 - prob_not_selected\n",
        "    print(f\"n={n:4d}: P(in sample) = {prob_selected:.4f}, P(OOB) = {prob_not_selected:.4f}\")\n",
        "\n",
        "print(f\"\\nLimit (n\u2192\u221e): P(OOB) = 1/e = {1/np.e:.4f} \u2248 36.8%\")\n",
        "print(f\"             P(in sample) = 1 - 1/e = {1 - 1/np.e:.4f} \u2248 63.2%\")\n",
        "\n",
        "# Visualize\n",
        "n_range = np.arange(5, 500)\n",
        "prob_oob = ((n_range - 1) / n_range) ** n_range\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(n_range, prob_oob, linewidth=2)\n",
        "plt.axhline(y=1/np.e, color='r', linestyle='--', label=f'1/e \u2248 {1/np.e:.4f}')\n",
        "plt.xlabel('Dataset Size (n)')\n",
        "plt.ylabel('P(sample is Out-of-Bag)')\n",
        "plt.title('Probability of Being Out-of-Bag vs Dataset Size')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Key Takeaway: ~37% of samples are OOB for each bootstrap sample\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bagging-classifier",
      "metadata": {},
      "source": [
        "## 2. BaggingClassifier\n",
        "\n",
        "### 2.1 Basic Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bagging-classifier-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load wine dataset\n",
        "wine = load_wine()\n",
        "X_wine = wine.data\n",
        "y_wine = wine.target\n",
        "\n",
        "print(\"Wine Classification Dataset\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_wine.shape[0]}\")\n",
        "print(f\"Features: {X_wine.shape[1]}\")\n",
        "print(f\"Classes: {len(wine.target_names)} - {wine.target_names}\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_wine, y_wine, test_size=0.3, random_state=42, stratify=y_wine\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "single-vs-bagging",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nSingle Decision Tree vs Bagging\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Single decision tree (prone to overfitting)\n",
        "single_tree = DecisionTreeClassifier(random_state=42)\n",
        "single_tree.fit(X_train, y_train)\n",
        "\n",
        "train_acc_single = single_tree.score(X_train, y_train)\n",
        "test_acc_single = single_tree.score(X_test, y_test)\n",
        "\n",
        "print(f\"Single Decision Tree:\")\n",
        "print(f\"  Train Accuracy: {train_acc_single:.4f}\")\n",
        "print(f\"  Test Accuracy:  {test_acc_single:.4f}\")\n",
        "print(f\"  Overfitting:    {train_acc_single - test_acc_single:.4f}\")\n",
        "\n",
        "# Bagging ensemble\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    max_samples=1.0,  # Use 100% of training data for each bootstrap\n",
        "    max_features=1.0,  # Use 100% of features\n",
        "    bootstrap=True,\n",
        "    oob_score=True,  # Enable OOB scoring\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "train_acc_bagging = bagging.score(X_train, y_train)\n",
        "test_acc_bagging = bagging.score(X_test, y_test)\n",
        "oob_acc_bagging = bagging.oob_score_\n",
        "\n",
        "print(f\"\\nBagging (100 trees):\")\n",
        "print(f\"  Train Accuracy: {train_acc_bagging:.4f}\")\n",
        "print(f\"  Test Accuracy:  {test_acc_bagging:.4f}\")\n",
        "print(f\"  OOB Accuracy:   {oob_acc_bagging:.4f}\")\n",
        "print(f\"  Overfitting:    {train_acc_bagging - test_acc_bagging:.4f}\")\n",
        "\n",
        "print(f\"\\n\u2713 Improvement: {test_acc_bagging - test_acc_single:.4f} ({(test_acc_bagging/test_acc_single - 1)*100:.1f}%)\")\n",
        "print(f\"\u2713 Reduced overfitting: {(train_acc_single - test_acc_single) - (train_acc_bagging - test_acc_bagging):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oob-evaluation",
      "metadata": {},
      "source": [
        "### 2.2 Out-of-Bag (OOB) Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oob-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Out-of-Bag Evaluation\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nOOB Score: {oob_acc_bagging:.4f}\")\n",
        "print(f\"Test Score: {test_acc_bagging:.4f}\")\n",
        "print(f\"Difference: {abs(oob_acc_bagging - test_acc_bagging):.4f}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udca1 OOB score approximates test performance without using test set!\")\n",
        "print(f\"   Each training sample is evaluated only by models that didn't see it.\")\n",
        "print(f\"   This provides unbiased error estimate for model selection.\")\n",
        "\n",
        "# OOB decision function (probability estimates)\n",
        "oob_decision = bagging.oob_decision_function_\n",
        "print(f\"\\nOOB decision function shape: {oob_decision.shape}\")\n",
        "print(f\"  (samples \u00d7 classes) = ({oob_decision.shape[0]} \u00d7 {oob_decision.shape[1]})\")\n",
        "\n",
        "# Show example OOB predictions\n",
        "print(f\"\\nExample OOB predictions (first 5 samples):\")\n",
        "for i in range(5):\n",
        "    true_class = y_train[i]\n",
        "    pred_class = np.argmax(oob_decision[i])\n",
        "    probs = oob_decision[i]\n",
        "    print(f\"  Sample {i}: True={true_class}, Pred={pred_class}, Probs={probs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n-estimators",
      "metadata": {},
      "source": [
        "### 2.3 Effect of Number of Estimators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n-estimators-analysis",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Effect of Number of Estimators\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "n_estimators_range = [1, 5, 10, 20, 50, 100, 200, 500]\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "oob_scores = []\n",
        "\n",
        "for n_est in n_estimators_range:\n",
        "    bag = BaggingClassifier(\n",
        "        estimator=DecisionTreeClassifier(),\n",
        "        n_estimators=n_est,\n",
        "        oob_score=True,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    bag.fit(X_train, y_train)\n",
        "    \n",
        "    train_scores.append(bag.score(X_train, y_train))\n",
        "    test_scores.append(bag.score(X_test, y_test))\n",
        "    oob_scores.append(bag.oob_score_)\n",
        "    \n",
        "    print(f\"n={n_est:3d}: Train={train_scores[-1]:.4f}, Test={test_scores[-1]:.4f}, OOB={oob_scores[-1]:.4f}\")\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(n_estimators_range, train_scores, 'o-', label='Train Score', linewidth=2)\n",
        "plt.plot(n_estimators_range, test_scores, 's-', label='Test Score', linewidth=2)\n",
        "plt.plot(n_estimators_range, oob_scores, '^-', label='OOB Score', linewidth=2)\n",
        "plt.xlabel('Number of Estimators')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Bagging Performance vs Number of Estimators')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.xscale('log')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Observations:\")\n",
        "print(\"   - Performance improves quickly, then plateaus\")\n",
        "print(\"   - More estimators = more stable but diminishing returns\")\n",
        "print(\"   - OOB score closely tracks test score\")\n",
        "print(\"   - Typically 50-200 estimators is sufficient\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bagging-regressor",
      "metadata": {},
      "source": [
        "## 3. BaggingRegressor\n",
        "\n",
        "### 3.1 Regression Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bagging-regressor-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X_diabetes = diabetes.data\n",
        "y_diabetes = diabetes.target\n",
        "\n",
        "print(\"Diabetes Regression Dataset\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_diabetes.shape[0]}\")\n",
        "print(f\"Features: {X_diabetes.shape[1]}\")\n",
        "\n",
        "# Split data\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_diabetes, y_diabetes, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Single tree\n",
        "single_tree_reg = DecisionTreeRegressor(random_state=42)\n",
        "single_tree_reg.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "y_pred_single = single_tree_reg.predict(X_test_reg)\n",
        "r2_single = r2_score(y_test_reg, y_pred_single)\n",
        "rmse_single = np.sqrt(mean_squared_error(y_test_reg, y_pred_single))\n",
        "\n",
        "print(f\"\\nSingle Decision Tree:\")\n",
        "print(f\"  R\u00b2 Score: {r2_single:.4f}\")\n",
        "print(f\"  RMSE:     {rmse_single:.2f}\")\n",
        "\n",
        "# Bagging regressor\n",
        "bagging_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=100,\n",
        "    oob_score=True,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "bagging_reg.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "y_pred_bagging = bagging_reg.predict(X_test_reg)\n",
        "r2_bagging = r2_score(y_test_reg, y_pred_bagging)\n",
        "rmse_bagging = np.sqrt(mean_squared_error(y_test_reg, y_pred_bagging))\n",
        "\n",
        "print(f\"\\nBagging (100 trees):\")\n",
        "print(f\"  R\u00b2 Score: {r2_bagging:.4f}\")\n",
        "print(f\"  RMSE:     {rmse_bagging:.2f}\")\n",
        "print(f\"  OOB R\u00b2:   {bagging_reg.oob_score_:.4f}\")\n",
        "\n",
        "print(f\"\\n\u2713 Improvement:\")\n",
        "print(f\"  R\u00b2 increase: {r2_bagging - r2_single:.4f}\")\n",
        "print(f\"  RMSE decrease: {rmse_single - rmse_bagging:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bagging-regressor-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Single tree\n",
        "axes[0].scatter(y_test_reg, y_pred_single, alpha=0.6)\n",
        "axes[0].plot([y_test_reg.min(), y_test_reg.max()], \n",
        "             [y_test_reg.min(), y_test_reg.max()], 'r--', lw=2)\n",
        "axes[0].set_xlabel('True Values')\n",
        "axes[0].set_ylabel('Predictions')\n",
        "axes[0].set_title(f'Single Tree (R\u00b2={r2_single:.3f})')\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Bagging\n",
        "axes[1].scatter(y_test_reg, y_pred_bagging, alpha=0.6)\n",
        "axes[1].plot([y_test_reg.min(), y_test_reg.max()], \n",
        "             [y_test_reg.min(), y_test_reg.max()], 'r--', lw=2)\n",
        "axes[1].set_xlabel('True Values')\n",
        "axes[1].set_ylabel('Predictions')\n",
        "axes[1].set_title(f'Bagging (R\u00b2={r2_bagging:.3f})')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\ud83d\udca1 Notice: Bagging produces smoother, more stable predictions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "base-estimator",
      "metadata": {},
      "source": [
        "## 4. Different Base Estimators\n",
        "\n",
        "### 4.1 Bagging with Various Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "different-base-estimators",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Bagging with Different Base Estimators\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load breast cancer for binary classification\n",
        "cancer = load_breast_cancer()\n",
        "X_cancer = cancer.data\n",
        "y_cancer = cancer.target\n",
        "\n",
        "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
        "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
        ")\n",
        "\n",
        "# Standardize for non-tree models\n",
        "scaler = StandardScaler()\n",
        "X_train_c_scaled = scaler.fit_transform(X_train_c)\n",
        "X_test_c_scaled = scaler.transform(X_test_c)\n",
        "\n",
        "base_estimators = {\n",
        "    'Decision Tree': DecisionTreeClassifier(max_depth=5),\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "    'KNN (k=5)': KNeighborsClassifier(n_neighbors=5),\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, base_est in base_estimators.items():\n",
        "    # Use scaled data for non-tree models\n",
        "    X_tr = X_train_c if 'Tree' in name else X_train_c_scaled\n",
        "    X_te = X_test_c if 'Tree' in name else X_test_c_scaled\n",
        "    \n",
        "    # Single model\n",
        "    base_est.fit(X_tr, y_train_c)\n",
        "    single_score = base_est.score(X_te, y_test_c)\n",
        "    \n",
        "    # Bagging\n",
        "    bagging = BaggingClassifier(\n",
        "        estimator=base_est,\n",
        "        n_estimators=50,\n",
        "        oob_score=True,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    bagging.fit(X_tr, y_train_c)\n",
        "    bagging_score = bagging.score(X_te, y_test_c)\n",
        "    oob_score = bagging.oob_score_\n",
        "    \n",
        "    improvement = bagging_score - single_score\n",
        "    \n",
        "    results.append({\n",
        "        'Base Estimator': name,\n",
        "        'Single': single_score,\n",
        "        'Bagging': bagging_score,\n",
        "        'OOB': oob_score,\n",
        "        'Improvement': improvement\n",
        "    })\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Single:      {single_score:.4f}\")\n",
        "    print(f\"  Bagging:     {bagging_score:.4f}\")\n",
        "    print(f\"  OOB:         {oob_score:.4f}\")\n",
        "    print(f\"  Improvement: {improvement:+.4f}\")\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Summary:\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Observations:\")\n",
        "print(\"   - Bagging helps most with unstable models (trees, KNN)\")\n",
        "print(\"   - Stable models (logistic regression) see less benefit\")\n",
        "print(\"   - Always test if bagging improves your specific case!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hyperparameters",
      "metadata": {},
      "source": [
        "## 5. Hyperparameter Tuning\n",
        "\n",
        "### 5.1 Key Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hyperparameter-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Bagging Hyperparameters\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "Key Parameters:\n",
        "\n",
        "1. n_estimators: Number of base models\n",
        "   - More is usually better (diminishing returns)\n",
        "   - Typical: 50-500\n",
        "\n",
        "2. max_samples: Samples per bootstrap (int or float)\n",
        "   - Default: 1.0 (100% of training data)\n",
        "   - Lower values: More diversity, faster training\n",
        "   - Range: 0.5-1.0\n",
        "\n",
        "3. max_features: Features per model (int or float)\n",
        "   - Default: 1.0 (all features)\n",
        "   - Lower values: More diversity (like Random Forest)\n",
        "   - Range: 0.5-1.0 or sqrt(n_features)\n",
        "\n",
        "4. bootstrap: Sample with replacement\n",
        "   - Default: True\n",
        "   - False = Pasting (without replacement)\n",
        "\n",
        "5. oob_score: Use out-of-bag samples for evaluation\n",
        "   - Only works with bootstrap=True\n",
        "   - Provides free validation estimate\n",
        "\n",
        "6. bootstrap_features: Sample features with replacement\n",
        "   - Default: False\n",
        "   - True: Random subspace method\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "max-samples-analysis",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nEffect of max_samples\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "max_samples_values = [0.3, 0.5, 0.7, 0.9, 1.0]\n",
        "scores_by_samples = []\n",
        "\n",
        "for max_samp in max_samples_values:\n",
        "    bag = BaggingClassifier(\n",
        "        estimator=DecisionTreeClassifier(),\n",
        "        n_estimators=100,\n",
        "        max_samples=max_samp,\n",
        "        oob_score=True,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    bag.fit(X_train, y_train)\n",
        "    \n",
        "    test_score = bag.score(X_test, y_test)\n",
        "    oob_score = bag.oob_score_\n",
        "    \n",
        "    scores_by_samples.append({\n",
        "        'max_samples': max_samp,\n",
        "        'test_score': test_score,\n",
        "        'oob_score': oob_score\n",
        "    })\n",
        "    \n",
        "    print(f\"max_samples={max_samp:.1f}: Test={test_score:.4f}, OOB={oob_score:.4f}\")\n",
        "\n",
        "# Plot\n",
        "df_samples = pd.DataFrame(scores_by_samples)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(df_samples['max_samples'], df_samples['test_score'], 'o-', label='Test Score', linewidth=2)\n",
        "plt.plot(df_samples['max_samples'], df_samples['oob_score'], 's-', label='OOB Score', linewidth=2)\n",
        "plt.xlabel('max_samples (fraction of training data)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Effect of max_samples on Performance')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rf-comparison",
      "metadata": {},
      "source": [
        "## 6. Random Forest as Specialized Bagging\n",
        "\n",
        "### 6.1 Bagging vs Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bagging-vs-rf",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Bagging vs Random Forest\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Standard Bagging\n",
        "bagging_standard = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    max_features=1.0,  # All features\n",
        "    oob_score=True,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Bagging with feature subsampling (like RF)\n",
        "bagging_feature_sampling = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    max_features=int(np.sqrt(X_train.shape[1])),  # sqrt(n_features)\n",
        "    oob_score=True,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_features='sqrt',\n",
        "    oob_score=True,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "models = {\n",
        "    'Bagging (all features)': bagging_standard,\n",
        "    'Bagging (sqrt features)': bagging_feature_sampling,\n",
        "    'Random Forest': rf\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    test_score = model.score(X_test, y_test)\n",
        "    oob_score = model.oob_score_\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Test Score: {test_score:.4f}\")\n",
        "    print(f\"  OOB Score:  {oob_score:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Key Differences:\")\n",
        "print(\"\"\"\n",
        "Bagging:\n",
        "  - Uses all features for each split\n",
        "  - Bootstrap samples only\n",
        "  - Can use any base estimator\n",
        "\n",
        "Random Forest:\n",
        "  - Samples features at each split (random subspace)\n",
        "  - Bootstrap samples + feature sampling\n",
        "  - Optimized for decision trees\n",
        "  - Often better than standard bagging\n",
        "  - Additional optimizations (faster, better splitting)\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "when-to-use",
      "metadata": {},
      "source": [
        "## 7. When to Use Bagging\n",
        "\n",
        "### 7.1 Decision Guide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decision-guide",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"When to Use Bagging\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "guide = \"\"\"\n",
        "\u2713 USE BAGGING when:\n",
        "  1. Base model has high variance (overfits)\n",
        "     - Deep decision trees\n",
        "     - Neural networks\n",
        "     - KNN with small k\n",
        "  \n",
        "  2. You want to reduce overfitting\n",
        "     - Single model: high train, low test accuracy\n",
        "  \n",
        "  3. You can train models in parallel\n",
        "     - Bagging is embarrassingly parallel\n",
        "  \n",
        "  4. You need probability estimates\n",
        "     - Averaging produces smooth probabilities\n",
        "  \n",
        "  5. You want \"free\" validation (OOB)\n",
        "     - No need for separate validation set\n",
        "\n",
        "\u2717 DON'T USE BAGGING when:\n",
        "  1. Base model is already stable\n",
        "     - Linear regression\n",
        "     - Logistic regression with regularization\n",
        "     - Small decision trees with max_depth\n",
        "  \n",
        "  2. Model has high bias (underfits)\n",
        "     - Bagging reduces variance, not bias\n",
        "     - Use boosting instead\n",
        "  \n",
        "  3. Interpretability is critical\n",
        "     - Ensemble is harder to interpret than single model\n",
        "  \n",
        "  4. Training time is severely limited\n",
        "     - Must train n_estimators models\n",
        "  \n",
        "  5. Memory is constrained\n",
        "     - Must store n_estimators models\n",
        "\n",
        "\u26a1 ALTERNATIVES:\n",
        "  - Boosting (AdaBoost, GradientBoosting): High bias problems\n",
        "  - Random Forest: Usually better than plain bagging for trees\n",
        "  - Stacking: Combine different model types\n",
        "  - Regularization: Single model with better generalization\n",
        "\"\"\"\n",
        "\n",
        "print(guide)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Summary and Best Practices\n",
        "\n",
        "### Quick Reference\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Classification\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),  # Base model\n",
        "    n_estimators=100,                     # Number of models\n",
        "    max_samples=1.0,                      # Samples per bootstrap\n",
        "    max_features=1.0,                     # Features per model\n",
        "    bootstrap=True,                       # Sample with replacement\n",
        "    oob_score=True,                       # Enable OOB scoring\n",
        "    random_state=42,\n",
        "    n_jobs=-1                             # Parallel training\n",
        ")\n",
        "\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = bagging.predict(X_test)\n",
        "y_proba = bagging.predict_proba(X_test)\n",
        "\n",
        "# OOB evaluation (free validation!)\n",
        "oob_score = bagging.oob_score_\n",
        "oob_decision = bagging.oob_decision_function_\n",
        "```\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "1. **Bootstrap Sampling**: ~63% in-bag, ~37% out-of-bag per sample\n",
        "\n",
        "2. **Variance Reduction**: \\(\\text{Var}(\\text{ensemble}) \\approx \\frac{\\sigma^2}{B}\\)\n",
        "\n",
        "3. **OOB Scoring**: Unbiased error estimate without validation set\n",
        "\n",
        "4. **Aggregation**: \n",
        "   - Classification: Majority vote\n",
        "   - Regression: Average prediction\n",
        "\n",
        "### Hyperparameter Guidelines\n",
        "\n",
        "| Parameter | Typical Range | Effect |\n",
        "|-----------|---------------|--------|\n",
        "| `n_estimators` | 50-500 | More = better, diminishing returns |\n",
        "| `max_samples` | 0.7-1.0 | Lower = more diversity, faster |\n",
        "| `max_features` | 0.5-1.0 | Lower = more diversity (RF style) |\n",
        "| `oob_score` | True | Enable for free validation |\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. \u2713 **Start with unstable models** (deep trees, high-variance models)\n",
        "2. \u2713 **Use OOB scoring** for model selection and tuning\n",
        "3. \u2713 **Try 100 estimators** as starting point\n",
        "4. \u2713 **Parallelize training** with `n_jobs=-1`\n",
        "5. \u2713 **Compare with Random Forest** for tree-based models\n",
        "6. \u2713 **Monitor train/test gap** to verify variance reduction\n",
        "\n",
        "### Common Mistakes\n",
        "\n",
        "1. \u274c Using bagging with already-stable models\n",
        "2. \u274c Not enabling `oob_score=True`\n",
        "3. \u274c Using too few estimators (< 20)\n",
        "4. \u274c Expecting bias reduction (bagging only reduces variance)\n",
        "5. \u274c Not comparing with Random Forest for trees\n",
        "\n",
        "### Bagging Family\n",
        "\n",
        "```\n",
        "Bagging Family:\n",
        "\u2502\n",
        "\u251c\u2500 BaggingClassifier/Regressor\n",
        "\u2502   \u2514\u2500 General purpose, any base estimator\n",
        "\u2502\n",
        "\u251c\u2500 Random Forest\n",
        "\u2502   \u2514\u2500 Specialized bagging with feature sampling\n",
        "\u2502\n",
        "\u251c\u2500 Extra Trees\n",
        "\u2502   \u2514\u2500 Random Forest with random thresholds\n",
        "\u2502\n",
        "\u2514\u2500 Pasting\n",
        "    \u2514\u2500 Bagging without replacement (bootstrap=False)\n",
        "```\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Random Forest deep dive\n",
        "- Boosting methods (AdaBoost, Gradient Boosting)\n",
        "- Stacking and Voting ensembles\n",
        "- Advanced ensemble techniques\n",
        "- XGBoost and LightGBM"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}