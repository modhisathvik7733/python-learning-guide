{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Multi-Layer Perceptron (MLP) in sklearn\n",
        "\n",
        "## Overview\n",
        "\n",
        "**Multi-Layer Perceptron (MLP)** is a feedforward artificial neural network with one or more hidden layers. It's sklearn's implementation of neural networks for classification and regression.\n",
        "\n",
        "### Core Concept\n",
        "\n",
        "*\"Learn complex non-linear patterns through layers of interconnected neurons\"*\n",
        "\n",
        "### Key Ideas\n",
        "\n",
        "1. **Layers**: Input \u2192 Hidden(s) \u2192 Output\n",
        "2. **Non-linearity**: Activation functions enable complex patterns\n",
        "3. **Backpropagation**: Gradient descent to learn weights\n",
        "4. **Universal Approximation**: Can approximate any continuous function\n",
        "\n",
        "## Mathematical Foundation\n",
        "\n",
        "### Single Neuron (Perceptron)\n",
        "\n",
        "\\[\n",
        "y = \\sigma\\left(\\sum_{i=1}^{n} w_i x_i + b\\right) = \\sigma(w^T x + b)\n",
        "\\]\n",
        "\n",
        "where:\n",
        "- \\(x\\) = input features\n",
        "- \\(w\\) = weights\n",
        "- \\(b\\) = bias\n",
        "- \\(\\sigma\\) = activation function\n",
        "\n",
        "### Multi-Layer Network\n",
        "\n",
        "**Forward Pass** (2 hidden layers):\n",
        "\n",
        "\\[\n",
        "h^{(1)} = \\sigma_1(W^{(1)} x + b^{(1)})\n",
        "\\]\n",
        "\\[\n",
        "h^{(2)} = \\sigma_2(W^{(2)} h^{(1)} + b^{(2)})\n",
        "\\]\n",
        "\\[\n",
        "\\hat{y} = \\sigma_3(W^{(3)} h^{(2)} + b^{(3)})\n",
        "\\]\n",
        "\n",
        "### Activation Functions\n",
        "\n",
        "**ReLU (Rectified Linear Unit)**:\n",
        "\\[\n",
        "\\text{ReLU}(x) = \\max(0, x)\n",
        "\\]\n",
        "\n",
        "**Logistic (Sigmoid)**:\n",
        "\\[\n",
        "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
        "\\]\n",
        "\n",
        "**Tanh (Hyperbolic Tangent)**:\n",
        "\\[\n",
        "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
        "\\]\n",
        "\n",
        "**Identity (Linear)**:\n",
        "\\[\n",
        "f(x) = x\n",
        "\\]\n",
        "\n",
        "### Loss Functions\n",
        "\n",
        "**Classification (Cross-Entropy)**:\n",
        "\\[\n",
        "L = -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{k=1}^{K} y_{ik} \\log(\\hat{y}_{ik})\n",
        "\\]\n",
        "\n",
        "**Regression (Mean Squared Error)**:\n",
        "\\[\n",
        "L = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\n",
        "\\]\n",
        "\n",
        "## Topics Covered\n",
        "\n",
        "1. MLP architecture and components\n",
        "2. MLPClassifier for classification\n",
        "3. MLPRegressor for regression\n",
        "4. Activation functions comparison\n",
        "5. Hidden layer sizes and network depth\n",
        "6. Regularization (alpha parameter)\n",
        "7. Learning rate and optimization\n",
        "8. Early stopping and validation\n",
        "9. Feature scaling importance\n",
        "10. Hyperparameter tuning\n",
        "11. Learning curves and convergence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from time import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# MLP models\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "\n",
        "# Other models for comparison\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Utilities\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, cross_val_score, GridSearchCV,\n",
        "    learning_curve, validation_curve\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, confusion_matrix,\n",
        "    mean_squared_error, r2_score, mean_absolute_error\n",
        ")\n",
        "from sklearn.datasets import (\n",
        "    make_classification, make_regression, make_moons, make_circles,\n",
        "    load_breast_cancer, load_diabetes, load_digits\n",
        ")\n",
        "\n",
        "np.random.seed(42)\n",
        "sns.set_style('whitegrid')\n",
        "print(\"\u2713 Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mlp-architecture",
      "metadata": {},
      "source": [
        "## 1. MLP Architecture and Components\n",
        "\n",
        "### 1.1 Understanding Network Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "architecture-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Multi-Layer Perceptron Architecture\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nStructure: Input Layer \u2192 Hidden Layer(s) \u2192 Output Layer\\n\")\n",
        "\n",
        "# Example architecture\n",
        "print(\"Example: Binary Classification with 4 features\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "n_features = 4\n",
        "hidden_layer_sizes = (10, 5)\n",
        "n_classes = 2\n",
        "\n",
        "print(f\"\\nInput Layer:    {n_features} neurons (features)\")\n",
        "print(f\"Hidden Layer 1: {hidden_layer_sizes[0]} neurons\")\n",
        "print(f\"Hidden Layer 2: {hidden_layer_sizes[1]} neurons\")\n",
        "print(f\"Output Layer:   {n_classes} neurons (classes)\")\n",
        "\n",
        "# Calculate parameters\n",
        "weights_1 = n_features * hidden_layer_sizes[0]\n",
        "bias_1 = hidden_layer_sizes[0]\n",
        "weights_2 = hidden_layer_sizes[0] * hidden_layer_sizes[1]\n",
        "bias_2 = hidden_layer_sizes[1]\n",
        "weights_3 = hidden_layer_sizes[1] * n_classes\n",
        "bias_3 = n_classes\n",
        "\n",
        "total_params = weights_1 + bias_1 + weights_2 + bias_2 + weights_3 + bias_3\n",
        "\n",
        "print(f\"\\nParameter Count:\")\n",
        "print(f\"  Layer 1: W{n_features}x{hidden_layer_sizes[0]} + b{hidden_layer_sizes[0]} = {weights_1 + bias_1}\")\n",
        "print(f\"  Layer 2: W{hidden_layer_sizes[0]}x{hidden_layer_sizes[1]} + b{hidden_layer_sizes[1]} = {weights_2 + bias_2}\")\n",
        "print(f\"  Layer 3: W{hidden_layer_sizes[1]}x{n_classes} + b{n_classes} = {weights_3 + bias_3}\")\n",
        "print(f\"  Total Parameters: {total_params}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\\n\ud83d\udca1 Key Concepts:\")\n",
        "print(\"   - More layers = deeper network (learns hierarchical features)\")\n",
        "print(\"   - More neurons per layer = wider network (more capacity)\")\n",
        "print(\"   - More parameters = more capacity but risk of overfitting\")\n",
        "print(\"   - Each connection has a learnable weight\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "activation-functions",
      "metadata": {},
      "source": [
        "### 1.2 Activation Functions Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "activation-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Activation Functions\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Define activation functions\n",
        "x = np.linspace(-5, 5, 100)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "activations = [\n",
        "    (relu, 'ReLU', 'max(0, x)', 'Most popular, prevents vanishing gradient'),\n",
        "    (sigmoid, 'Logistic', '1 / (1 + e^-x)', 'Output layer for binary classification'),\n",
        "    (tanh, 'Tanh', '(e^x - e^-x) / (e^x + e^-x)', 'Zero-centered, range [-1, 1]'),\n",
        "    (identity, 'Identity', 'x', 'Used in regression output layer')\n",
        "]\n",
        "\n",
        "for idx, (func, name, formula, desc) in enumerate(activations):\n",
        "    y = func(x)\n",
        "    axes[idx].plot(x, y, linewidth=3)\n",
        "    axes[idx].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
        "    axes[idx].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
        "    axes[idx].grid(alpha=0.3)\n",
        "    axes[idx].set_xlabel('Input')\n",
        "    axes[idx].set_ylabel('Output')\n",
        "    axes[idx].set_title(f'{name}\\n{formula}\\n{desc}', fontsize=10)\n",
        "    \n",
        "    # Add range annotation\n",
        "    if name == 'ReLU':\n",
        "        axes[idx].text(0.05, 0.95, 'Range: [0, \u221e)', transform=axes[idx].transAxes,\n",
        "                      verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "    elif name == 'Logistic':\n",
        "        axes[idx].text(0.05, 0.95, 'Range: (0, 1)', transform=axes[idx].transAxes,\n",
        "                      verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "    elif name == 'Tanh':\n",
        "        axes[idx].text(0.05, 0.95, 'Range: (-1, 1)', transform=axes[idx].transAxes,\n",
        "                      verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "    else:\n",
        "        axes[idx].text(0.05, 0.95, 'Range: (-\u221e, \u221e)', transform=axes[idx].transAxes,\n",
        "                      verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Choosing Activation Function:\")\n",
        "print(\"   Hidden Layers: ReLU (default, works well)\")\n",
        "print(\"   Binary Classification Output: logistic\")\n",
        "print(\"   Multi-class Output: softmax (automatic)\")\n",
        "print(\"   Regression Output: identity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mlp-classifier",
      "metadata": {},
      "source": [
        "## 2. MLPClassifier - Binary Classification\n",
        "\n",
        "### 2.1 Simple Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mlp-classifier-simple",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate non-linear dataset (moons)\n",
        "X_moons, y_moons = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
        "\n",
        "print(\"MLPClassifier - Binary Classification\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Dataset: Half-moons (non-linear)\")\n",
        "print(f\"Samples: {X_moons.shape[0]}\")\n",
        "print(f\"Features: {X_moons.shape[1]}\\n\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_moons, y_moons, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# CRITICAL: Scale features for MLP!\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train MLP\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(10, 5),  # 2 hidden layers: 10 and 5 neurons\n",
        "    activation='relu',           # ReLU activation\n",
        "    solver='adam',               # Adam optimizer\n",
        "    max_iter=1000,               # Maximum iterations\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training MLP with architecture (2 \u2192 10 \u2192 5 \u2192 2)...\")\n",
        "start = time()\n",
        "mlp.fit(X_train_scaled, y_train)\n",
        "train_time = time() - start\n",
        "\n",
        "# Predictions\n",
        "y_pred = mlp.predict(X_test_scaled)\n",
        "y_pred_proba = mlp.predict_proba(X_test_scaled)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nTraining time: {train_time:.3f}s\")\n",
        "print(f\"Iterations: {mlp.n_iter_}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"\\nNetwork Structure:\")\n",
        "print(f\"  Input: {X_train.shape[1]} features\")\n",
        "print(f\"  Hidden layers: {mlp.hidden_layer_sizes}\")\n",
        "print(f\"  Output: {mlp.n_outputs_} classes\")\n",
        "print(f\"  Total layers: {mlp.n_layers_}\")\n",
        "\n",
        "# Show loss curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(mlp.loss_curve_, linewidth=2)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Curve')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Loss curve shows convergence - loss decreases over iterations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mlp-decision-boundary",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot decision boundary\n",
        "def plot_decision_boundary(model, X, y, title):\n",
        "    h = 0.02\n",
        "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='k', s=50)\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(mlp, X_test_scaled, y_test, \n",
        "                      'MLP Decision Boundary (Half-Moons Dataset)')\n",
        "\n",
        "print(\"\\n\ud83d\udca1 MLP learned complex non-linear decision boundary!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "real-world-classification",
      "metadata": {},
      "source": [
        "### 2.2 Real-World Classification: Breast Cancer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "breast-cancer-mlp",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X_cancer = cancer.data\n",
        "y_cancer = cancer.target\n",
        "\n",
        "print(\"MLPClassifier - Breast Cancer Detection\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_cancer.shape[0]}\")\n",
        "print(f\"Features: {X_cancer.shape[1]}\")\n",
        "print(f\"Classes: {cancer.target_names}\\n\")\n",
        "\n",
        "# Split and scale\n",
        "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
        "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
        ")\n",
        "\n",
        "scaler_c = StandardScaler()\n",
        "X_train_c_scaled = scaler_c.fit_transform(X_train_c)\n",
        "X_test_c_scaled = scaler_c.transform(X_test_c)\n",
        "\n",
        "# Train MLP\n",
        "mlp_cancer = MLPClassifier(\n",
        "    hidden_layer_sizes=(100, 50),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    alpha=0.0001,  # L2 regularization\n",
        "    max_iter=500,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training MLP...\")\n",
        "mlp_cancer.fit(X_train_c_scaled, y_train_c)\n",
        "\n",
        "# Evaluate\n",
        "y_pred_c = mlp_cancer.predict(X_test_c_scaled)\n",
        "accuracy_c = accuracy_score(y_test_c, y_pred_c)\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy_c:.4f}\")\n",
        "print(f\"Iterations: {mlp_cancer.n_iter_}\")\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_c, y_pred_c, target_names=cancer.target_names))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test_c, y_pred_c)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "           xticklabels=cancer.target_names,\n",
        "           yticklabels=cancer.target_names)\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.title('MLP Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mlp-regressor",
      "metadata": {},
      "source": [
        "## 3. MLPRegressor - Regression\n",
        "\n",
        "### 3.1 Diabetes Progression Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mlp-regressor-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X_diab = diabetes.data\n",
        "y_diab = diabetes.target\n",
        "\n",
        "print(\"MLPRegressor - Diabetes Progression\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_diab.shape[0]}\")\n",
        "print(f\"Features: {X_diab.shape[1]}\\n\")\n",
        "\n",
        "# Split and scale\n",
        "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(\n",
        "    X_diab, y_diab, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "scaler_d = StandardScaler()\n",
        "X_train_d_scaled = scaler_d.fit_transform(X_train_d)\n",
        "X_test_d_scaled = scaler_d.transform(X_test_d)\n",
        "\n",
        "# Train MLPRegressor\n",
        "mlp_reg = MLPRegressor(\n",
        "    hidden_layer_sizes=(100, 50),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    alpha=0.001,\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training MLPRegressor...\")\n",
        "mlp_reg.fit(X_train_d_scaled, y_train_d)\n",
        "\n",
        "# Predictions\n",
        "y_pred_d = mlp_reg.predict(X_test_d_scaled)\n",
        "\n",
        "# Evaluate\n",
        "mse = mean_squared_error(y_test_d, y_pred_d)\n",
        "mae = mean_absolute_error(y_test_d, y_pred_d)\n",
        "r2 = r2_score(y_test_d, y_pred_d)\n",
        "\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"  R\u00b2 Score: {r2:.4f}\")\n",
        "print(f\"  RMSE: {np.sqrt(mse):.2f}\")\n",
        "print(f\"  MAE: {mae:.2f}\")\n",
        "print(f\"  Iterations: {mlp_reg.n_iter_}\")\n",
        "\n",
        "# Plot predictions vs actual\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Predicted vs Actual\n",
        "axes[0].scatter(y_test_d, y_pred_d, alpha=0.6)\n",
        "axes[0].plot([y_test_d.min(), y_test_d.max()], \n",
        "            [y_test_d.min(), y_test_d.max()], 'r--', linewidth=2)\n",
        "axes[0].set_xlabel('Actual')\n",
        "axes[0].set_ylabel('Predicted')\n",
        "axes[0].set_title(f'Predicted vs Actual (R\u00b2={r2:.3f})')\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Loss curve\n",
        "axes[1].plot(mlp_reg.loss_curve_, linewidth=2)\n",
        "axes[1].set_xlabel('Iteration')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].set_title('Training Loss Curve')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 MLPRegressor can capture non-linear relationships in regression tasks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hidden-layer-sizes",
      "metadata": {},
      "source": [
        "## 4. Effect of Hidden Layer Sizes\n",
        "\n",
        "### 4.1 Comparing Different Architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hidden-layer-comparison",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Effect of Hidden Layer Sizes\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Test different architectures\n",
        "architectures = [\n",
        "    (10,),           # Single layer, 10 neurons\n",
        "    (50,),           # Single layer, 50 neurons\n",
        "    (100,),          # Single layer, 100 neurons\n",
        "    (50, 25),        # Two layers\n",
        "    (100, 50),       # Two layers (wider)\n",
        "    (100, 50, 25),   # Three layers\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for arch in architectures:\n",
        "    mlp_test = MLPClassifier(\n",
        "        hidden_layer_sizes=arch,\n",
        "        activation='relu',\n",
        "        solver='adam',\n",
        "        max_iter=500,\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    # Train\n",
        "    start = time()\n",
        "    mlp_test.fit(X_train_c_scaled, y_train_c)\n",
        "    train_time = time() - start\n",
        "    \n",
        "    # Evaluate\n",
        "    train_acc = mlp_test.score(X_train_c_scaled, y_train_c)\n",
        "    test_acc = mlp_test.score(X_test_c_scaled, y_test_c)\n",
        "    \n",
        "    # Count parameters\n",
        "    n_params = sum(w.size for w in mlp_test.coefs_) + sum(b.size for b in mlp_test.intercepts_)\n",
        "    \n",
        "    results.append({\n",
        "        'Architecture': str(arch),\n",
        "        'Layers': len(arch),\n",
        "        'Parameters': n_params,\n",
        "        'Train Acc': train_acc,\n",
        "        'Test Acc': test_acc,\n",
        "        'Train Time (s)': train_time\n",
        "    })\n",
        "    \n",
        "    print(f\"{str(arch):20} - Test: {test_acc:.4f}, Train: {train_acc:.4f}, \"\n",
        "          f\"Params: {n_params:6d}, Time: {train_time:.2f}s\")\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy comparison\n",
        "x = np.arange(len(results_df))\n",
        "width = 0.35\n",
        "axes[0].bar(x - width/2, results_df['Train Acc'], width, label='Train', alpha=0.8)\n",
        "axes[0].bar(x + width/2, results_df['Test Acc'], width, label='Test', alpha=0.8)\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_xlabel('Architecture')\n",
        "axes[0].set_title('Accuracy by Architecture')\n",
        "axes[0].set_xticks(x)\n",
        "axes[0].set_xticklabels(results_df['Architecture'], rotation=45, ha='right')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3, axis='y')\n",
        "\n",
        "# Parameters vs Test Accuracy\n",
        "axes[1].scatter(results_df['Parameters'], results_df['Test Acc'], s=100, alpha=0.7)\n",
        "for idx, row in results_df.iterrows():\n",
        "    axes[1].annotate(row['Architecture'], \n",
        "                    (row['Parameters'], row['Test Acc']),\n",
        "                    fontsize=8, ha='right')\n",
        "axes[1].set_xlabel('Number of Parameters')\n",
        "axes[1].set_ylabel('Test Accuracy')\n",
        "axes[1].set_title('Model Complexity vs Accuracy')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Observations:\")\n",
        "print(\"   - Deeper/wider networks can learn more complex patterns\")\n",
        "print(\"   - Too large = overfitting (high train, low test accuracy)\")\n",
        "print(\"   - Too small = underfitting (low train and test accuracy)\")\n",
        "print(\"   - Start with (100,) or (100, 50) as reasonable defaults\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "activation-comparison",
      "metadata": {},
      "source": [
        "## 5. Activation Function Comparison\n",
        "\n",
        "### 5.1 Testing Different Activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "activation-comparison-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Activation Function Comparison\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "activations = ['relu', 'logistic', 'tanh', 'identity']\n",
        "activation_results = []\n",
        "\n",
        "for activation in activations:\n",
        "    mlp_act = MLPClassifier(\n",
        "        hidden_layer_sizes=(100, 50),\n",
        "        activation=activation,\n",
        "        solver='adam',\n",
        "        max_iter=500,\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    # Train\n",
        "    start = time()\n",
        "    mlp_act.fit(X_train_c_scaled, y_train_c)\n",
        "    train_time = time() - start\n",
        "    \n",
        "    # Evaluate\n",
        "    test_acc = mlp_act.score(X_test_c_scaled, y_test_c)\n",
        "    n_iter = mlp_act.n_iter_\n",
        "    \n",
        "    activation_results.append({\n",
        "        'Activation': activation,\n",
        "        'Test Accuracy': test_acc,\n",
        "        'Iterations': n_iter,\n",
        "        'Train Time (s)': train_time\n",
        "    })\n",
        "    \n",
        "    print(f\"{activation:10} - Test Acc: {test_acc:.4f}, Iterations: {n_iter:3d}, Time: {train_time:.2f}s\")\n",
        "\n",
        "act_df = pd.DataFrame(activation_results)\n",
        "act_df = act_df.sort_values('Test Accuracy', ascending=False)\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "bars = ax.barh(act_df['Activation'], act_df['Test Accuracy'], alpha=0.7)\n",
        "ax.set_xlabel('Test Accuracy')\n",
        "ax.set_title('Activation Function Performance')\n",
        "ax.set_xlim([0.9, 1.0])\n",
        "ax.grid(alpha=0.3, axis='x')\n",
        "\n",
        "# Color the best one\n",
        "bars[0].set_color('green')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + act_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\ud83d\udca1 ReLU is usually the best choice for hidden layers!\")\n",
        "print(\"   - Avoids vanishing gradient problem\")\n",
        "print(\"   - Computationally efficient\")\n",
        "print(\"   - Works well in practice\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "regularization",
      "metadata": {},
      "source": [
        "## 6. Regularization (Alpha Parameter)\n",
        "\n",
        "### 6.1 Effect of L2 Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "regularization-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Regularization (Alpha Parameter)\")\n",
        "print(\"=\"*70)\n",
        "print(\"Alpha = L2 regularization penalty (prevents overfitting)\")\n",
        "print(\"  - Small alpha: Less regularization (may overfit)\")\n",
        "print(\"  - Large alpha: More regularization (may underfit)\\n\")\n",
        "\n",
        "# Test different alpha values\n",
        "alpha_values = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0]\n",
        "alpha_results = []\n",
        "\n",
        "for alpha in alpha_values:\n",
        "    mlp_alpha = MLPClassifier(\n",
        "        hidden_layer_sizes=(100, 50),\n",
        "        activation='relu',\n",
        "        solver='adam',\n",
        "        alpha=alpha,\n",
        "        max_iter=500,\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    mlp_alpha.fit(X_train_c_scaled, y_train_c)\n",
        "    \n",
        "    train_acc = mlp_alpha.score(X_train_c_scaled, y_train_c)\n",
        "    test_acc = mlp_alpha.score(X_test_c_scaled, y_test_c)\n",
        "    \n",
        "    alpha_results.append({\n",
        "        'Alpha': alpha,\n",
        "        'Train Acc': train_acc,\n",
        "        'Test Acc': test_acc\n",
        "    })\n",
        "    \n",
        "    print(f\"Alpha={alpha:7.5f} - Train: {train_acc:.4f}, Test: {test_acc:.4f}\")\n",
        "\n",
        "alpha_df = pd.DataFrame(alpha_results)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(alpha_df['Alpha'], alpha_df['Train Acc'], 'o-', label='Train Accuracy', linewidth=2)\n",
        "plt.semilogx(alpha_df['Alpha'], alpha_df['Test Acc'], 's-', label='Test Accuracy', linewidth=2)\n",
        "plt.xlabel('Alpha (Regularization Strength)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Effect of Regularization')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "best_alpha = alpha_df.loc[alpha_df['Test Acc'].idxmax(), 'Alpha']\n",
        "print(f\"\\nBest alpha: {best_alpha}\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Regularization helps prevent overfitting\")\n",
        "print(\"   Default alpha=0.0001 works well for most cases\")\n",
        "print(\"   Increase if overfitting (high train, low test accuracy)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "learning-rate",
      "metadata": {},
      "source": [
        "## 7. Learning Rate and Optimization\n",
        "\n",
        "### 7.1 Comparing Solvers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "solver-comparison",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Solver Comparison\")\n",
        "print(\"=\"*70)\n",
        "print(\"Solvers = Optimization algorithms for weight updates\\n\")\n",
        "\n",
        "# Test different solvers\n",
        "solvers = ['sgd', 'adam', 'lbfgs']\n",
        "solver_results = []\n",
        "\n",
        "for solver in solvers:\n",
        "    # Note: lbfgs doesn't support partial_fit, needs more memory\n",
        "    if solver == 'sgd':\n",
        "        mlp_solver = MLPClassifier(\n",
        "            hidden_layer_sizes=(100, 50),\n",
        "            activation='relu',\n",
        "            solver=solver,\n",
        "            learning_rate_init=0.001,  # Initial learning rate\n",
        "            max_iter=500,\n",
        "            random_state=42\n",
        "        )\n",
        "    else:\n",
        "        mlp_solver = MLPClassifier(\n",
        "            hidden_layer_sizes=(100, 50),\n",
        "            activation='relu',\n",
        "            solver=solver,\n",
        "            max_iter=500,\n",
        "            random_state=42\n",
        "        )\n",
        "    \n",
        "    # Train\n",
        "    start = time()\n",
        "    mlp_solver.fit(X_train_c_scaled, y_train_c)\n",
        "    train_time = time() - start\n",
        "    \n",
        "    # Evaluate\n",
        "    test_acc = mlp_solver.score(X_test_c_scaled, y_test_c)\n",
        "    n_iter = mlp_solver.n_iter_\n",
        "    \n",
        "    solver_results.append({\n",
        "        'Solver': solver,\n",
        "        'Test Accuracy': test_acc,\n",
        "        'Iterations': n_iter,\n",
        "        'Train Time (s)': train_time\n",
        "    })\n",
        "    \n",
        "    print(f\"{solver:8} - Test Acc: {test_acc:.4f}, Iterations: {n_iter:3d}, Time: {train_time:.2f}s\")\n",
        "\n",
        "solver_df = pd.DataFrame(solver_results)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy\n",
        "axes[0].barh(solver_df['Solver'], solver_df['Test Accuracy'], alpha=0.7)\n",
        "axes[0].set_xlabel('Test Accuracy')\n",
        "axes[0].set_title('Solver Performance')\n",
        "axes[0].set_xlim([0.9, 1.0])\n",
        "axes[0].grid(alpha=0.3, axis='x')\n",
        "\n",
        "# Training time\n",
        "axes[1].barh(solver_df['Solver'], solver_df['Train Time (s)'], alpha=0.7, color='orange')\n",
        "axes[1].set_xlabel('Training Time (seconds)')\n",
        "axes[1].set_title('Training Time Comparison')\n",
        "axes[1].grid(alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Solver Selection:\")\n",
        "print(\"   adam: Best default choice (adaptive learning rate)\")\n",
        "print(\"   sgd: Good for large datasets, needs tuning\")\n",
        "print(\"   lbfgs: Good for small datasets, memory intensive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "early-stopping",
      "metadata": {},
      "source": [
        "## 8. Early Stopping\n",
        "\n",
        "### 8.1 Preventing Overfitting with Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "early-stopping-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Early Stopping\")\n",
        "print(\"=\"*70)\n",
        "print(\"Stop training when validation score stops improving\")\n",
        "print(\"Prevents overfitting and saves training time\\n\")\n",
        "\n",
        "# Train with early stopping\n",
        "mlp_early = MLPClassifier(\n",
        "    hidden_layer_sizes=(100, 50),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=1000,\n",
        "    early_stopping=True,        # Enable early stopping\n",
        "    validation_fraction=0.1,    # Use 10% for validation\n",
        "    n_iter_no_change=10,        # Stop if no improvement for 10 iterations\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training with early stopping enabled...\")\n",
        "mlp_early.fit(X_train_c_scaled, y_train_c)\n",
        "\n",
        "print(f\"\\nStopped at iteration: {mlp_early.n_iter_}\")\n",
        "print(f\"Best validation score: {mlp_early.best_validation_score_:.4f}\")\n",
        "print(f\"Test accuracy: {mlp_early.score(X_test_c_scaled, y_test_c):.4f}\")\n",
        "\n",
        "# Plot validation curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(mlp_early.validation_scores_, linewidth=2, label='Validation Score')\n",
        "plt.axvline(x=mlp_early.n_iter_, color='red', linestyle='--', \n",
        "           label=f'Stopped at iteration {mlp_early.n_iter_}')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.title('Early Stopping - Validation Curve')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Early stopping:\")\n",
        "print(\"   - Automatically finds optimal number of iterations\")\n",
        "print(\"   - Prevents overfitting\")\n",
        "print(\"   - Saves computation time\")\n",
        "print(\"   - Recommended for most applications\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "scaling-importance",
      "metadata": {},
      "source": [
        "## 9. Feature Scaling - CRITICAL!\n",
        "\n",
        "### 9.1 Impact of Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "scaling-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Feature Scaling Impact on MLP\")\n",
        "print(\"=\"*70)\n",
        "print(\"MLP is VERY sensitive to feature scales!\\n\")\n",
        "\n",
        "# Train without scaling\n",
        "print(\"Training WITHOUT scaling...\")\n",
        "mlp_unscaled = MLPClassifier(\n",
        "    hidden_layer_sizes=(100, 50),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=500,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "start = time()\n",
        "mlp_unscaled.fit(X_train_c, y_train_c)  # No scaling!\n",
        "time_unscaled = time() - start\n",
        "acc_unscaled = mlp_unscaled.score(X_test_c, y_test_c)\n",
        "iter_unscaled = mlp_unscaled.n_iter_\n",
        "\n",
        "# Train with scaling\n",
        "print(\"Training WITH scaling...\")\n",
        "mlp_scaled = MLPClassifier(\n",
        "    hidden_layer_sizes=(100, 50),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=500,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "start = time()\n",
        "mlp_scaled.fit(X_train_c_scaled, y_train_c)  # Scaled!\n",
        "time_scaled = time() - start\n",
        "acc_scaled = mlp_scaled.score(X_test_c_scaled, y_test_c)\n",
        "iter_scaled = mlp_scaled.n_iter_\n",
        "\n",
        "# Compare\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\\nComparison:\")\n",
        "print(f\"{'Metric':20} {'Without Scaling':>20} {'With Scaling':>20}\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Test Accuracy':20} {acc_unscaled:>20.4f} {acc_scaled:>20.4f}\")\n",
        "print(f\"{'Iterations':20} {iter_unscaled:>20d} {iter_scaled:>20d}\")\n",
        "print(f\"{'Training Time (s)':20} {time_unscaled:>20.2f} {time_scaled:>20.2f}\")\n",
        "\n",
        "# Visualize loss curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].plot(mlp_unscaled.loss_curve_, linewidth=2)\n",
        "axes[0].set_xlabel('Iteration')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title(f'Without Scaling\\nFinal Accuracy: {acc_unscaled:.4f}')\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "axes[1].plot(mlp_scaled.loss_curve_, linewidth=2, color='green')\n",
        "axes[1].set_xlabel('Iteration')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].set_title(f'With Scaling\\nFinal Accuracy: {acc_scaled:.4f}')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 ALWAYS SCALE FEATURES FOR MLP!\")\n",
        "print(\"   Without scaling:\")\n",
        "print(\"   - Poor convergence\")\n",
        "print(\"   - Longer training time\")\n",
        "print(\"   - Worse performance\")\n",
        "print(\"   \\n   Use StandardScaler before training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hyperparameter-tuning",
      "metadata": {},
      "source": [
        "## 10. Hyperparameter Tuning\n",
        "\n",
        "### 10.1 Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "grid-search-mlp",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Hyperparameter Tuning for MLP\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (100, 50)],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'alpha': [0.0001, 0.001, 0.01],\n",
        "    'learning_rate': ['constant', 'adaptive']\n",
        "}\n",
        "\n",
        "print(\"Parameter Grid:\")\n",
        "for param, values in param_grid.items():\n",
        "    print(f\"  {param}: {values}\")\n",
        "\n",
        "print(f\"\\nTotal combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
        "print(\"\\nPerforming Grid Search with 3-fold CV...\\n\")\n",
        "\n",
        "# Grid search\n",
        "grid_search = GridSearchCV(\n",
        "    MLPClassifier(max_iter=500, random_state=42),\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "start = time()\n",
        "grid_search.fit(X_train_c_scaled, y_train_c)\n",
        "grid_time = time() - start\n",
        "\n",
        "print(f\"\\nGrid Search completed in {grid_time:.2f}s\")\n",
        "print(f\"\\nBest Parameters:\")\n",
        "for param, value in grid_search.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "print(f\"\\nBest CV Score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "best_mlp = grid_search.best_estimator_\n",
        "test_acc_best = best_mlp.score(X_test_c_scaled, y_test_c)\n",
        "\n",
        "print(f\"Test Set Accuracy: {test_acc_best:.4f}\")\n",
        "\n",
        "# Show top 5 configurations\n",
        "results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "top5 = results_df.nlargest(5, 'mean_test_score')[[\n",
        "    'param_hidden_layer_sizes', 'param_activation', 'param_alpha', \n",
        "    'param_learning_rate', 'mean_test_score', 'std_test_score'\n",
        "]]\n",
        "\n",
        "print(\"\\nTop 5 Configurations:\")\n",
        "print(top5.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Summary and Quick Reference\n",
        "\n",
        "### Quick Reference Code\n",
        "\n",
        "```python\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# CRITICAL: Always scale features!\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ===== CLASSIFICATION =====\n",
        "\n",
        "mlp_clf = MLPClassifier(\n",
        "    hidden_layer_sizes=(100, 50),  # Tuple: (layer1, layer2, ...)\n",
        "    activation='relu',             # 'relu', 'tanh', 'logistic'\n",
        "    solver='adam',                 # 'adam', 'sgd', 'lbfgs'\n",
        "    alpha=0.0001,                  # L2 regularization\n",
        "    batch_size='auto',             # Mini-batch size\n",
        "    learning_rate='constant',      # 'constant', 'adaptive'\n",
        "    learning_rate_init=0.001,      # Initial learning rate\n",
        "    max_iter=500,                  # Maximum epochs\n",
        "    early_stopping=True,           # Enable validation-based stopping\n",
        "    validation_fraction=0.1,       # Validation set size\n",
        "    n_iter_no_change=10,           # Patience for early stopping\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ===== REGRESSION =====\n",
        "\n",
        "mlp_reg = MLPRegressor(\n",
        "    hidden_layer_sizes=(100, 50),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    alpha=0.001,\n",
        "    max_iter=1000,\n",
        "    early_stopping=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train\n",
        "mlp.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict\n",
        "predictions = mlp.predict(X_test_scaled)\n",
        "probabilities = mlp.predict_proba(X_test_scaled)  # Classification only\n",
        "\n",
        "# Monitoring\n",
        "print(f\"Iterations: {mlp.n_iter_}\")\n",
        "print(f\"Loss curve: {mlp.loss_curve_}\")\n",
        "```\n",
        "\n",
        "### Key Hyperparameters\n",
        "\n",
        "**hidden_layer_sizes**:\n",
        "- Tuple defining network architecture\n",
        "- (100,) = 1 hidden layer with 100 neurons\n",
        "- (100, 50) = 2 layers: 100 then 50 neurons\n",
        "- (100, 50, 25) = 3 layers\n",
        "- Start with (100,) or (100, 50)\n",
        "\n",
        "**activation**:\n",
        "- 'relu': Default, works best for most tasks\n",
        "- 'tanh': Can work better than ReLU sometimes\n",
        "- 'logistic': Sigmoid, rarely used in hidden layers\n",
        "- 'identity': Linear, only for special cases\n",
        "\n",
        "**solver**:\n",
        "- 'adam': Best default (adaptive learning)\n",
        "- 'sgd': Stochastic gradient descent, needs tuning\n",
        "- 'lbfgs': Good for small datasets\n",
        "\n",
        "**alpha**:\n",
        "- L2 regularization strength\n",
        "- Default: 0.0001\n",
        "- Increase if overfitting: try 0.001, 0.01\n",
        "- Decrease if underfitting: try 0.00001\n",
        "\n",
        "**max_iter**:\n",
        "- Maximum training epochs\n",
        "- 200-1000 usually sufficient\n",
        "- Use early_stopping to avoid setting manually\n",
        "\n",
        "### Default Configuration (Good Starting Point)\n",
        "\n",
        "```python\n",
        "MLPClassifier(\n",
        "    hidden_layer_sizes=(100,),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    alpha=0.0001,\n",
        "    max_iter=500,\n",
        "    early_stopping=True,\n",
        "    random_state=42\n",
        ")\n",
        "```\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. **Always scale features**: Use StandardScaler (CRITICAL!)\n",
        "2. **Start simple**: Begin with (100,) hidden layer\n",
        "3. **Use early stopping**: Prevents overfitting automatically\n",
        "4. **Monitor loss curve**: Check convergence\n",
        "5. **Use adam solver**: Best default optimizer\n",
        "6. **Set random_state**: For reproducibility\n",
        "7. **Cross-validate**: Don't rely on single train/test split\n",
        "8. **Increase max_iter**: If not converging\n",
        "\n",
        "### Common Issues and Solutions\n",
        "\n",
        "| Issue | Solution |\n",
        "|-------|----------|\n",
        "| Poor performance | Scale features with StandardScaler |\n",
        "| Convergence warning | Increase max_iter or enable early_stopping |\n",
        "| Overfitting | Increase alpha, reduce hidden_layer_sizes |\n",
        "| Underfitting | Increase hidden_layer_sizes, decrease alpha |\n",
        "| Slow training | Reduce hidden_layer_sizes, use smaller dataset |\n",
        "| Unstable training | Decrease learning_rate_init |\n",
        "\n",
        "### When to Use MLP\n",
        "\n",
        "\u2713 **Good for:**\n",
        "- Complex non-linear patterns\n",
        "- Image data (after flattening or CNN features)\n",
        "- Large datasets\n",
        "- High-dimensional data\n",
        "- When interpretability not required\n",
        "\n",
        "\u2717 **Avoid when:**\n",
        "- Small datasets (<1000 samples)\n",
        "- Need interpretability\n",
        "- Simple linear patterns\n",
        "- Limited computational resources\n",
        "- Quick prototyping needed\n",
        "\n",
        "### MLP vs Other Models\n",
        "\n",
        "| Aspect | MLP | Logistic Reg | Random Forest | SVM |\n",
        "|--------|-----|--------------|---------------|-----|\n",
        "| Non-linear | Excellent | Poor | Excellent | Good |\n",
        "| Training Speed | Slow | Fast | Medium | Slow |\n",
        "| Prediction Speed | Fast | Very Fast | Medium | Medium |\n",
        "| Interpretability | Poor | Excellent | Good | Poor |\n",
        "| Hyperparameters | Many | Few | Few | Few |\n",
        "| Feature Scaling | Required | Optional | Not Required | Required |\n",
        "\n",
        "### Computational Complexity\n",
        "\n",
        "**Training**: O(n \u00d7 d \u00d7 h \u00d7 t)\n",
        "- n = samples\n",
        "- d = features  \n",
        "- h = hidden neurons\n",
        "- t = iterations\n",
        "\n",
        "**Prediction**: O(d \u00d7 h) per sample\n",
        "\n",
        "### Further Reading\n",
        "\n",
        "- **Book**: \"Deep Learning\" - Goodfellow, Bengio, Courville\n",
        "- **sklearn Docs**: https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
        "- **Paper**: \"Understanding the difficulty of training deep feedforward neural networks\" - Glorot & Bengio\n",
        "- **For production**: Consider TensorFlow/PyTorch for larger networks"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}