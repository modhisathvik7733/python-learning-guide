{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Probability Calibration\n",
        "\n",
        "## Overview\n",
        "\n",
        "**Probability calibration** ensures that predicted probabilities match the true likelihood of the outcome. A well-calibrated classifier's predicted probability of 0.8 means the event occurs 80% of the time.\n",
        "\n",
        "### Core Concept\n",
        "\n",
        "*\"When a model predicts 70% probability, it should be correct 70% of the time\"*\n",
        "\n",
        "### Key Ideas\n",
        "\n",
        "1. **Well-calibrated**: Predicted probabilities match true frequencies\n",
        "2. **Poorly-calibrated**: Probabilities are too confident or not confident enough\n",
        "3. **Calibration methods**: Transform probabilities without changing rankings\n",
        "4. **When it matters**: Decision-making, risk assessment, probability-based systems\n",
        "\n",
        "## Mathematical Foundation\n",
        "\n",
        "### Perfect Calibration\n",
        "\n",
        "For a perfectly calibrated classifier:\n",
        "\n",
        "\\[\n",
        "P(y=1 | \\hat{p} = p) = p\n",
        "\\]\n",
        "\n",
        "**Example**: Among all predictions where model predicts 0.6, exactly 60% should be positive class.\n",
        "\n",
        "### Calibration Curve (Reliability Diagram)\n",
        "\n",
        "Plot predicted probabilities vs empirical frequencies:\n",
        "1. Bin predictions by probability (e.g., [0-0.1, 0.1-0.2, ...])\n",
        "2. Calculate true frequency in each bin\n",
        "3. Plot: x-axis = mean predicted probability, y-axis = fraction of positives\n",
        "\n",
        "**Perfect calibration**: Points lie on diagonal (y = x)\n",
        "\n",
        "### Brier Score\n",
        "\n",
        "**Measures accuracy and calibration** of probabilistic predictions:\n",
        "\n",
        "\\[\n",
        "\\text{Brier Score} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{p}_i - y_i)^2\n",
        "\\]\n",
        "\n",
        "- Lower is better (0 = perfect)\n",
        "- Range: [0, 1]\n",
        "- Combines calibration and resolution\n",
        "\n",
        "### Calibration Methods\n",
        "\n",
        "#### 1. Platt Scaling (Sigmoid)\n",
        "\n",
        "Fit logistic regression on classifier outputs:\n",
        "\n",
        "\\[\n",
        "P_{\\text{calibrated}} = \\frac{1}{1 + \\exp(A \\cdot f(x) + B)}\n",
        "\\]\n",
        "\n",
        "where \\(f(x)\\) is the classifier's raw output.\n",
        "\n",
        "**Properties**:\n",
        "- Parametric (assumes sigmoid relationship)\n",
        "- Works well for small datasets\n",
        "- Good for SVM, boosted trees\n",
        "\n",
        "#### 2. Isotonic Regression\n",
        "\n",
        "Fits non-parametric, piecewise-constant, monotonic function:\n",
        "\n",
        "\\[\n",
        "P_{\\text{calibrated}} = \\arg\\min_z \\sum_i (z_i - y_i)^2 \\quad \\text{subject to} \\quad z_1 \\leq z_2 \\leq ... \\leq z_n\n",
        "\\]\n",
        "\n",
        "**Properties**:\n",
        "- Non-parametric (more flexible)\n",
        "- Needs more data (prone to overfitting)\n",
        "- Better for tree-based models\n",
        "\n",
        "## Topics Covered\n",
        "\n",
        "1. Understanding calibration and why it matters\n",
        "2. Calibration curves (reliability diagrams)\n",
        "3. Brier score and calibration metrics\n",
        "4. Which models need calibration\n",
        "5. Platt scaling (sigmoid method)\n",
        "6. Isotonic regression\n",
        "7. CalibratedClassifierCV in sklearn\n",
        "8. Before/after calibration comparison\n",
        "9. Best practices and guidelines"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from time import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Calibration tools\n",
        "from sklearn.calibration import (\n",
        "    CalibratedClassifierCV, calibration_curve\n",
        ")\n",
        "\n",
        "# Models\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Utilities\n",
        "from sklearn.model_selection import train_test_split, cross_val_predict\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, brier_score_loss, log_loss,\n",
        "    roc_auc_score, classification_report\n",
        ")\n",
        "from sklearn.datasets import make_classification, load_breast_cancer\n",
        "\n",
        "np.random.seed(42)\n",
        "sns.set_style('whitegrid')\n",
        "print(\"\u2713 Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "calibration-intuition",
      "metadata": {},
      "source": [
        "## 1. Understanding Calibration\n",
        "\n",
        "### 1.1 What is a Well-Calibrated Model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "calibration-intro",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"What is Calibration?\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nScenario: Medical diagnosis model\\n\")\n",
        "\n",
        "# Simulate predictions\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# Well-calibrated model\n",
        "print(\"WELL-CALIBRATED MODEL:\")\n",
        "print(\"-\" * 70)\n",
        "predicted_probs_good = np.array([0.1, 0.3, 0.5, 0.7, 0.9])\n",
        "for prob in predicted_probs_good:\n",
        "    # Generate outcomes based on true probability\n",
        "    n_pred = 200\n",
        "    outcomes = np.random.binomial(1, prob, n_pred)\n",
        "    empirical_prob = outcomes.mean()\n",
        "    \n",
        "    print(f\"  Predicted: {prob:.1f} \u2192 Actual: {empirical_prob:.2f} \u2713\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "# Poorly-calibrated model (overconfident)\n",
        "print(\"\\nPOORLY-CALIBRATED MODEL (Overconfident):\")\n",
        "print(\"-\" * 70)\n",
        "predicted_probs_bad = np.array([0.0, 0.2, 0.5, 0.8, 1.0])\n",
        "true_probs_bad = np.array([0.1, 0.3, 0.5, 0.7, 0.9])  # Actual probabilities\n",
        "for pred_prob, true_prob in zip(predicted_probs_bad, true_probs_bad):\n",
        "    n_pred = 200\n",
        "    outcomes = np.random.binomial(1, true_prob, n_pred)\n",
        "    empirical_prob = outcomes.mean()\n",
        "    \n",
        "    print(f\"  Predicted: {pred_prob:.1f} \u2192 Actual: {empirical_prob:.2f} \u2717\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\\n\ud83d\udca1 Key Insight:\")\n",
        "print(\"   Well-calibrated: Predicted probabilities match empirical frequencies\")\n",
        "print(\"   Overconfident: Predicts extreme probabilities (0 or 1) too often\")\n",
        "print(\"   Underconfident: Predictions too close to 0.5\")\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Well-calibrated\n",
        "axes[0].plot([0, 1], [0, 1], 'k--', label='Perfect calibration', linewidth=2)\n",
        "axes[0].scatter(predicted_probs_good, predicted_probs_good, s=100, alpha=0.7, label='Model predictions')\n",
        "axes[0].set_xlabel('Predicted Probability')\n",
        "axes[0].set_ylabel('Empirical Probability')\n",
        "axes[0].set_title('Well-Calibrated Model')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "axes[0].set_xlim([0, 1])\n",
        "axes[0].set_ylim([0, 1])\n",
        "\n",
        "# Overconfident\n",
        "axes[1].plot([0, 1], [0, 1], 'k--', label='Perfect calibration', linewidth=2)\n",
        "axes[1].scatter(predicted_probs_bad, true_probs_bad, s=100, alpha=0.7, \n",
        "               color='red', label='Model predictions')\n",
        "axes[1].set_xlabel('Predicted Probability')\n",
        "axes[1].set_ylabel('Empirical Probability')\n",
        "axes[1].set_title('Poorly-Calibrated Model (Overconfident)')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "axes[1].set_xlim([0, 1])\n",
        "axes[1].set_ylim([0, 1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "calibration-curves",
      "metadata": {},
      "source": [
        "## 2. Calibration Curves (Reliability Diagrams)\n",
        "\n",
        "### 2.1 Comparing Different Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "calibration-curves-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "print(\"Calibration Curves - Breast Cancer Dataset\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X.shape[0]}\")\n",
        "print(f\"Features: {X.shape[1]}\\n\")\n",
        "\n",
        "# Split and scale\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train multiple models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'SVM': SVC(probability=True, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "print(\"Training models...\\n\")\n",
        "predictions = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Get probability predictions\n",
        "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    brier = brier_score_loss(y_test, y_pred_proba)\n",
        "    \n",
        "    predictions[name] = {\n",
        "        'proba': y_pred_proba,\n",
        "        'accuracy': accuracy,\n",
        "        'brier': brier\n",
        "    }\n",
        "    \n",
        "    print(f\"  Accuracy: {accuracy:.4f}, Brier Score: {brier:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "plot-calibration-curves",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot calibration curves\n",
        "print(\"\\nGenerating Calibration Curves...\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "# Perfect calibration line\n",
        "ax.plot([0, 1], [0, 1], 'k--', label='Perfect calibration', linewidth=2)\n",
        "\n",
        "# Plot each model\n",
        "for name, pred_dict in predictions.items():\n",
        "    y_pred_proba = pred_dict['proba']\n",
        "    \n",
        "    # Calculate calibration curve\n",
        "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
        "        y_test, y_pred_proba, n_bins=10, strategy='uniform'\n",
        "    )\n",
        "    \n",
        "    # Plot\n",
        "    ax.plot(mean_predicted_value, fraction_of_positives, 'o-', \n",
        "           label=f\"{name} (Brier: {pred_dict['brier']:.3f})\", linewidth=2, markersize=8)\n",
        "\n",
        "ax.set_xlabel('Mean Predicted Probability', fontsize=12)\n",
        "ax.set_ylabel('Fraction of Positives', fontsize=12)\n",
        "ax.set_title('Calibration Curves (Reliability Diagrams)', fontsize=14)\n",
        "ax.legend(loc='upper left')\n",
        "ax.grid(alpha=0.3)\n",
        "ax.set_xlim([0, 1])\n",
        "ax.set_ylim([0, 1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Interpreting Calibration Curves:\")\n",
        "print(\"   On diagonal (y=x): Perfectly calibrated\")\n",
        "print(\"   Above diagonal: Underconfident (predicts lower than actual)\")\n",
        "print(\"   Below diagonal: Overconfident (predicts higher than actual)\")\n",
        "print(\"   \\n   Observations:\")\n",
        "print(\"   - Logistic Regression: Usually well-calibrated\")\n",
        "print(\"   - Naive Bayes: Often pushes probabilities to extremes\")\n",
        "print(\"   - Random Forest: Tends to be underconfident\")\n",
        "print(\"   - SVM: Often overconfident\")\n",
        "print(\"   - Gradient Boosting: Can be overconfident\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "brier-score",
      "metadata": {},
      "source": [
        "## 3. Brier Score\n",
        "\n",
        "### 3.1 Understanding and Decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "brier-score-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Brier Score Breakdown\")\n",
        "print(\"=\"*70)\n",
        "print(\"Brier Score = mean((predicted_prob - actual_outcome)^2)\")\n",
        "print(\"  - Range: [0, 1]\")\n",
        "print(\"  - 0 = Perfect prediction\")\n",
        "print(\"  - Lower is better\\n\")\n",
        "\n",
        "# Create comparison table\n",
        "metrics_data = []\n",
        "for name, pred_dict in predictions.items():\n",
        "    metrics_data.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': pred_dict['accuracy'],\n",
        "        'Brier Score': pred_dict['brier'],\n",
        "        'Log Loss': log_loss(y_test, pred_dict['proba'])\n",
        "    })\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "metrics_df = metrics_df.sort_values('Brier Score')\n",
        "\n",
        "print(\"Model Performance Comparison:\")\n",
        "print(\"=\"*70)\n",
        "print(metrics_df.to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Brier Score\n",
        "axes[0].barh(metrics_df['Model'], metrics_df['Brier Score'], alpha=0.7)\n",
        "axes[0].set_xlabel('Brier Score (lower is better)')\n",
        "axes[0].set_title('Brier Score Comparison')\n",
        "axes[0].invert_yaxis()\n",
        "axes[0].grid(alpha=0.3, axis='x')\n",
        "\n",
        "# Accuracy\n",
        "axes[1].barh(metrics_df['Model'], metrics_df['Accuracy'], alpha=0.7, color='green')\n",
        "axes[1].set_xlabel('Accuracy (higher is better)')\n",
        "axes[1].set_title('Accuracy Comparison')\n",
        "axes[1].invert_yaxis()\n",
        "axes[1].set_xlim([0.9, 1.0])\n",
        "axes[1].grid(alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Note: High accuracy doesn't guarantee good calibration!\")\n",
        "print(\"   A model can be very accurate but have poorly calibrated probabilities\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "calibration-methods",
      "metadata": {},
      "source": [
        "## 4. Calibration Methods\n",
        "\n",
        "### 4.1 Platt Scaling (Sigmoid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "platt-scaling",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Platt Scaling (Sigmoid Calibration)\")\n",
        "print(\"=\"*70)\n",
        "print(\"Method: Fit sigmoid function to map uncalibrated scores to probabilities\")\n",
        "print(\"  P_calibrated = 1 / (1 + exp(A*f(x) + B))\")\n",
        "print(\"  \\nBest for: SVM, Naive Bayes, Boosted trees\")\n",
        "print(\"  Requirements: Works with smaller datasets\\n\")\n",
        "\n",
        "# Calibrate Random Forest (tends to be underconfident)\n",
        "print(\"Calibrating Random Forest with Platt Scaling...\\n\")\n",
        "\n",
        "rf_original = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_original.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Calibrate using Platt scaling (sigmoid)\n",
        "rf_calibrated_platt = CalibratedClassifierCV(\n",
        "    rf_original, \n",
        "    method='sigmoid',  # Platt scaling\n",
        "    cv='prefit'  # Use already fitted model\n",
        ")\n",
        "\n",
        "# Need separate calibration set\n",
        "# Split training data further\n",
        "X_train_sub, X_calib, y_train_sub, y_calib = train_test_split(\n",
        "    X_train_scaled, y_train, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Retrain on subset\n",
        "rf_original = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_original.fit(X_train_sub, y_train_sub)\n",
        "\n",
        "# Calibrate\n",
        "rf_calibrated_platt = CalibratedClassifierCV(\n",
        "    rf_original,\n",
        "    method='sigmoid',\n",
        "    cv='prefit'\n",
        ")\n",
        "rf_calibrated_platt.fit(X_calib, y_calib)\n",
        "\n",
        "# Get predictions\n",
        "y_pred_proba_original = rf_original.predict_proba(X_test_scaled)[:, 1]\n",
        "y_pred_proba_calibrated = rf_calibrated_platt.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculate Brier scores\n",
        "brier_original = brier_score_loss(y_test, y_pred_proba_original)\n",
        "brier_calibrated = brier_score_loss(y_test, y_pred_proba_calibrated)\n",
        "\n",
        "print(f\"Random Forest (Original):\")\n",
        "print(f\"  Brier Score: {brier_original:.4f}\")\n",
        "print(f\"\\nRandom Forest (Platt Scaling):\")\n",
        "print(f\"  Brier Score: {brier_calibrated:.4f}\")\n",
        "print(f\"\\nImprovement: {(brier_original - brier_calibrated):.4f} ({((brier_original - brier_calibrated)/brier_original*100):+.1f}%)\")\n",
        "\n",
        "# Plot calibration curves\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "ax.plot([0, 1], [0, 1], 'k--', label='Perfect calibration', linewidth=2)\n",
        "\n",
        "# Original\n",
        "fraction_pos_orig, mean_pred_orig = calibration_curve(y_test, y_pred_proba_original, n_bins=10)\n",
        "ax.plot(mean_pred_orig, fraction_pos_orig, 'o-', label=f'Original RF (Brier: {brier_original:.3f})',\n",
        "       linewidth=2, markersize=8)\n",
        "\n",
        "# Calibrated\n",
        "fraction_pos_cal, mean_pred_cal = calibration_curve(y_test, y_pred_proba_calibrated, n_bins=10)\n",
        "ax.plot(mean_pred_cal, fraction_pos_cal, 's-', label=f'Platt Scaling (Brier: {brier_calibrated:.3f})',\n",
        "       linewidth=2, markersize=8)\n",
        "\n",
        "ax.set_xlabel('Mean Predicted Probability')\n",
        "ax.set_ylabel('Fraction of Positives')\n",
        "ax.set_title('Calibration: Before vs After Platt Scaling')\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Platt Scaling moved predictions closer to diagonal!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "isotonic-regression",
      "metadata": {},
      "source": [
        "### 4.2 Isotonic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "isotonic-regression-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Isotonic Regression Calibration\")\n",
        "print(\"=\"*70)\n",
        "print(\"Method: Fit non-parametric, monotonic, piecewise-constant function\")\n",
        "print(\"  More flexible than sigmoid\")\n",
        "print(\"  \\nBest for: Tree-based models, non-linear calibration errors\")\n",
        "print(\"  Requirements: Needs larger datasets (prone to overfitting)\\n\")\n",
        "\n",
        "# Calibrate with isotonic regression\n",
        "rf_calibrated_isotonic = CalibratedClassifierCV(\n",
        "    rf_original,\n",
        "    method='isotonic',  # Isotonic regression\n",
        "    cv='prefit'\n",
        ")\n",
        "rf_calibrated_isotonic.fit(X_calib, y_calib)\n",
        "\n",
        "# Get predictions\n",
        "y_pred_proba_isotonic = rf_calibrated_isotonic.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculate Brier score\n",
        "brier_isotonic = brier_score_loss(y_test, y_pred_proba_isotonic)\n",
        "\n",
        "print(f\"Random Forest (Isotonic Regression):\")\n",
        "print(f\"  Brier Score: {brier_isotonic:.4f}\")\n",
        "print(f\"\\nComparison:\")\n",
        "print(f\"  Original:          {brier_original:.4f}\")\n",
        "print(f\"  Platt Scaling:     {brier_calibrated:.4f}\")\n",
        "print(f\"  Isotonic:          {brier_isotonic:.4f}\")\n",
        "\n",
        "# Plot all three\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "ax.plot([0, 1], [0, 1], 'k--', label='Perfect calibration', linewidth=2)\n",
        "\n",
        "# Original\n",
        "ax.plot(mean_pred_orig, fraction_pos_orig, 'o-', \n",
        "       label=f'Original RF (Brier: {brier_original:.3f})', linewidth=2, markersize=8)\n",
        "\n",
        "# Platt\n",
        "ax.plot(mean_pred_cal, fraction_pos_cal, 's-', \n",
        "       label=f'Platt Scaling (Brier: {brier_calibrated:.3f})', linewidth=2, markersize=8)\n",
        "\n",
        "# Isotonic\n",
        "fraction_pos_iso, mean_pred_iso = calibration_curve(y_test, y_pred_proba_isotonic, n_bins=10)\n",
        "ax.plot(mean_pred_iso, fraction_pos_iso, '^-', \n",
        "       label=f'Isotonic Regression (Brier: {brier_isotonic:.3f})', linewidth=2, markersize=8)\n",
        "\n",
        "ax.set_xlabel('Mean Predicted Probability')\n",
        "ax.set_ylabel('Fraction of Positives')\n",
        "ax.set_title('Calibration Methods Comparison')\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Isotonic Regression:\")\n",
        "print(\"   - More flexible (can capture non-linear miscalibration)\")\n",
        "print(\"   - Needs more data\")\n",
        "print(\"   - Usually better for tree-based models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "calibrated-cv",
      "metadata": {},
      "source": [
        "## 5. CalibratedClassifierCV - Cross-Validation Approach\n",
        "\n",
        "### 5.1 Training with Built-in Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "calibrated-cv-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"CalibratedClassifierCV with Cross-Validation\")\n",
        "print(\"=\"*70)\n",
        "print(\"When cv='prefit': Uses already fitted model + separate calibration set\")\n",
        "print(\"When cv=k: Trains k models using k-fold CV (better use of data)\\n\")\n",
        "\n",
        "# Method 1: Using cv=5 (recommended)\n",
        "print(\"Method 1: Using cv=5 (k-fold cross-validation)\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Train base model\n",
        "rf_base = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Calibrate with CV\n",
        "rf_calibrated_cv = CalibratedClassifierCV(\n",
        "    rf_base,\n",
        "    method='sigmoid',\n",
        "    cv=5  # 5-fold cross-validation\n",
        ")\n",
        "\n",
        "print(\"Training with 5-fold CV calibration...\")\n",
        "start = time()\n",
        "rf_calibrated_cv.fit(X_train_scaled, y_train)\n",
        "train_time = time() - start\n",
        "\n",
        "print(f\"Training time: {train_time:.2f}s\")\n",
        "print(f\"Number of base estimators: {len(rf_calibrated_cv.calibrated_classifiers_)}\")\n",
        "\n",
        "# Predictions\n",
        "y_pred_proba_cv = rf_calibrated_cv.predict_proba(X_test_scaled)[:, 1]\n",
        "y_pred_cv = rf_calibrated_cv.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate\n",
        "accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
        "brier_cv = brier_score_loss(y_test, y_pred_proba_cv)\n",
        "\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"  Accuracy: {accuracy_cv:.4f}\")\n",
        "print(f\"  Brier Score: {brier_cv:.4f}\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 cv=5 approach:\")\n",
        "print(\"   - Uses all training data efficiently\")\n",
        "print(\"   - Trains 5 base models + 5 calibrators\")\n",
        "print(\"   - Averages predictions from all 5\")\n",
        "print(\"   - More robust than single calibration\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "model-comparison",
      "metadata": {},
      "source": [
        "## 6. Which Models Need Calibration?\n",
        "\n",
        "### 6.1 Testing Different Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "models-need-calibration",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Which Models Need Calibration?\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Test various models\n",
        "test_models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'Decision Tree': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "    'SVM (RBF)': SVC(probability=True, random_state=42)\n",
        "}\n",
        "\n",
        "calibration_results = []\n",
        "\n",
        "for name, model in test_models.items():\n",
        "    print(f\"\\nTesting {name}...\")\n",
        "    \n",
        "    # Train original\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred_orig = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    brier_orig = brier_score_loss(y_test, y_pred_orig)\n",
        "    \n",
        "    # Calibrate with sigmoid\n",
        "    model_calibrated = CalibratedClassifierCV(model, method='sigmoid', cv=5)\n",
        "    model_calibrated.fit(X_train_scaled, y_train)\n",
        "    y_pred_cal = model_calibrated.predict_proba(X_test_scaled)[:, 1]\n",
        "    brier_cal = brier_score_loss(y_test, y_pred_cal)\n",
        "    \n",
        "    improvement = ((brier_orig - brier_cal) / brier_orig) * 100\n",
        "    \n",
        "    calibration_results.append({\n",
        "        'Model': name,\n",
        "        'Brier (Original)': brier_orig,\n",
        "        'Brier (Calibrated)': brier_cal,\n",
        "        'Improvement (%)': improvement\n",
        "    })\n",
        "    \n",
        "    print(f\"  Original Brier:    {brier_orig:.4f}\")\n",
        "    print(f\"  Calibrated Brier:  {brier_cal:.4f}\")\n",
        "    print(f\"  Improvement:       {improvement:+.1f}%\")\n",
        "\n",
        "cal_results_df = pd.DataFrame(calibration_results)\n",
        "cal_results_df = cal_results_df.sort_values('Improvement (%)', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\\nCalibration Impact Summary:\")\n",
        "print(cal_results_df.to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "x = np.arange(len(cal_results_df))\n",
        "width = 0.35\n",
        "\n",
        "ax.barh(x - width/2, cal_results_df['Brier (Original)'], width, \n",
        "       label='Original', alpha=0.8)\n",
        "ax.barh(x + width/2, cal_results_df['Brier (Calibrated)'], width, \n",
        "       label='Calibrated', alpha=0.8)\n",
        "\n",
        "ax.set_yticks(x)\n",
        "ax.set_yticklabels(cal_results_df['Model'])\n",
        "ax.set_xlabel('Brier Score (lower is better)')\n",
        "ax.set_title('Calibration Impact on Different Models')\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Models that NEED calibration:\")\n",
        "print(\"   \u2713 Naive Bayes (pushes to extremes)\")\n",
        "print(\"   \u2713 SVM (decision function not probabilistic)\")\n",
        "print(\"   \u2713 Boosted Trees (can be overconfident)\")\n",
        "print(\"   \u2713 Random Forest (tends to be underconfident)\")\n",
        "print(\"   \\n   Models already well-calibrated:\")\n",
        "print(\"   \u2713 Logistic Regression (often well-calibrated)\")\n",
        "print(\"   \u2713 Decision Trees (decent, but can improve)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "best-practices",
      "metadata": {},
      "source": [
        "## 7. Best Practices and Guidelines\n",
        "\n",
        "### 7.1 When to Use Calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "best-practices-guide",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Calibration Best Practices\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n\u2713 WHEN CALIBRATION IS IMPORTANT:\")\n",
        "use_cases = [\n",
        "    \"Medical diagnosis (probability represents risk)\",\n",
        "    \"Credit scoring (probability determines action)\",\n",
        "    \"Weather forecasting (probabilities must be accurate)\",\n",
        "    \"Fraud detection (threshold decisions based on probability)\",\n",
        "    \"Ranking/recommendation systems\",\n",
        "    \"Multi-class classification with probability-based decisions\",\n",
        "    \"Cost-sensitive learning\",\n",
        "    \"Ensemble methods combining probability outputs\"\n",
        "]\n",
        "for i, case in enumerate(use_cases, 1):\n",
        "    print(f\"  {i}. {case}\")\n",
        "\n",
        "print(\"\\n\u2717 WHEN CALIBRATION LESS IMPORTANT:\")\n",
        "not_important = [\n",
        "    \"Only care about final class predictions (not probabilities)\",\n",
        "    \"Using fixed threshold (e.g., 0.5) for all decisions\",\n",
        "    \"Ranking tasks where relative order matters, not absolute values\",\n",
        "    \"Already using well-calibrated model (Logistic Regression)\"\n",
        "]\n",
        "for i, case in enumerate(not_important, 1):\n",
        "    print(f\"  {i}. {case}\")\n",
        "\n",
        "print(\"\\n\\n\u2699\ufe0f METHOD SELECTION GUIDE:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "method_guide = [\n",
        "    {\n",
        "        'Scenario': 'Small dataset (<1000 samples)',\n",
        "        'Method': 'Platt Scaling (sigmoid)',\n",
        "        'Reason': 'More stable with less data'\n",
        "    },\n",
        "    {\n",
        "        'Scenario': 'Large dataset (>10k samples)',\n",
        "        'Method': 'Isotonic Regression',\n",
        "        'Reason': 'More flexible, needs more data'\n",
        "    },\n",
        "    {\n",
        "        'Scenario': 'SVM or Naive Bayes',\n",
        "        'Method': 'Platt Scaling',\n",
        "        'Reason': 'Standard choice for these models'\n",
        "    },\n",
        "    {\n",
        "        'Scenario': 'Tree-based models',\n",
        "        'Method': 'Isotonic Regression',\n",
        "        'Reason': 'Better for non-linear miscalibration'\n",
        "    },\n",
        "    {\n",
        "        'Scenario': 'Uncertain which to use',\n",
        "        'Method': 'Try both, compare on validation',\n",
        "        'Reason': 'Empirical comparison'\n",
        "    },\n",
        "]\n",
        "\n",
        "method_df = pd.DataFrame(method_guide)\n",
        "print(method_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\\n\ud83d\udccb CALIBRATION WORKFLOW:\")\n",
        "print(\"-\" * 70)\n",
        "workflow = [\n",
        "    \"1. Split data: Train / Calibration / Test (or use CV)\",\n",
        "    \"2. Train model on training set\",\n",
        "    \"3. Check calibration curve on validation set\",\n",
        "    \"4. If poorly calibrated: Apply CalibratedClassifierCV\",\n",
        "    \"5. Choose method: sigmoid (small data) or isotonic (large data)\",\n",
        "    \"6. Use cv=5 for better data efficiency\",\n",
        "    \"7. Evaluate on test set using Brier score\",\n",
        "    \"8. Generate final calibration curve\"\n",
        "]\n",
        "for step in workflow:\n",
        "    print(f\"  {step}\")\n",
        "\n",
        "print(\"\\n\\n\u26a0\ufe0f COMMON PITFALLS:\")\n",
        "print(\"-\" * 70)\n",
        "pitfalls = [\n",
        "    (\"Calibrating on training data\", \"Use separate calibration set or CV\"),\n",
        "    (\"Using isotonic with small data\", \"Use sigmoid for <1000 samples\"),\n",
        "    (\"Ignoring calibration for medical/financial\", \"Always check calibration\"),\n",
        "    (\"Not checking calibration curve\", \"Visualize before and after\"),\n",
        "    (\"Over-calibrating\", \"Don't calibrate multiple times\"),\n",
        "    (\"Using wrong metric\", \"Use Brier score, not just accuracy\"),\n",
        "]\n",
        "for pitfall, solution in pitfalls:\n",
        "    print(f\"  \u274c {pitfall}\")\n",
        "    print(f\"     \u2713 {solution}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Summary and Quick Reference\n",
        "\n",
        "### Quick Reference Code\n",
        "\n",
        "```python\n",
        "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
        "from sklearn.metrics import brier_score_loss\n",
        "\n",
        "# ===== METHOD 1: Separate Calibration Set =====\n",
        "\n",
        "# Split data\n",
        "X_train, X_calib, y_train, y_calib = train_test_split(\n",
        "    X_train_full, y_train_full, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train base model\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Calibrate\n",
        "calibrated_model = CalibratedClassifierCV(\n",
        "    model,\n",
        "    method='sigmoid',  # or 'isotonic'\n",
        "    cv='prefit'  # Use already fitted model\n",
        ")\n",
        "calibrated_model.fit(X_calib, y_calib)\n",
        "\n",
        "# ===== METHOD 2: Cross-Validation (Recommended) =====\n",
        "\n",
        "# Train and calibrate in one step\n",
        "calibrated_model_cv = CalibratedClassifierCV(\n",
        "    RandomForestClassifier(),\n",
        "    method='sigmoid',\n",
        "    cv=5  # 5-fold cross-validation\n",
        ")\n",
        "calibrated_model_cv.fit(X_train_full, y_train_full)\n",
        "\n",
        "# ===== EVALUATION =====\n",
        "\n",
        "# Predictions\n",
        "y_pred_proba = calibrated_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Brier score (lower is better)\n",
        "brier = brier_score_loss(y_test, y_pred_proba)\n",
        "\n",
        "# Calibration curve\n",
        "fraction_of_positives, mean_predicted_value = calibration_curve(\n",
        "    y_test, y_pred_proba, n_bins=10\n",
        ")\n",
        "\n",
        "# Plot\n",
        "plt.plot(mean_predicted_value, fraction_of_positives, 'o-')\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Perfect calibration\n",
        "```\n",
        "\n",
        "### Key Metrics\n",
        "\n",
        "**Brier Score**:\n",
        "```python\n",
        "brier = (1/n) * sum((predicted_prob - actual)^2)\n",
        "```\n",
        "- Range: [0, 1]\n",
        "- 0 = Perfect\n",
        "- Lower is better\n",
        "\n",
        "**Log Loss (Cross-Entropy)**:\n",
        "```python\n",
        "log_loss = -(1/n) * sum(y*log(p) + (1-y)*log(1-p))\n",
        "```\n",
        "- Penalizes confident wrong predictions heavily\n",
        "\n",
        "### Method Comparison\n",
        "\n",
        "| Method | Type | Best For | Dataset Size | Flexibility |\n",
        "|--------|------|----------|--------------|-------------|\n",
        "| Platt Scaling | Parametric | SVM, NB, Boosting | Small-Medium | Low (sigmoid) |\n",
        "| Isotonic | Non-parametric | Trees, large data | Large | High (arbitrary monotonic) |\n",
        "\n",
        "### Models Calibration Needs\n",
        "\n",
        "| Model | Natural Calibration | Need Calibration? |\n",
        "|-------|---------------------|-------------------|\n",
        "| Logistic Regression | Excellent | Usually No |\n",
        "| Naive Bayes | Poor (extreme probs) | Yes |\n",
        "| SVM | Poor | Yes |\n",
        "| Decision Tree | Moderate | Sometimes |\n",
        "| Random Forest | Underconfident | Yes |\n",
        "| Gradient Boosting | Overconfident | Yes |\n",
        "| Neural Networks | Variable | Often Yes |\n",
        "\n",
        "### Calibration Curves Interpretation\n",
        "\n",
        "**On diagonal (y=x)**: Perfectly calibrated\n",
        "- Predicted 0.7 \u2192 Actually 0.7\n",
        "\n",
        "**Above diagonal**: Underconfident\n",
        "- Predicted 0.6 \u2192 Actually 0.8\n",
        "- Model is too conservative\n",
        "\n",
        "**Below diagonal**: Overconfident\n",
        "- Predicted 0.8 \u2192 Actually 0.6\n",
        "- Model is too aggressive\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. **Always use separate data**: Never calibrate on training data\n",
        "2. **Prefer cv approach**: Use cv=5 instead of single calibration set\n",
        "3. **Check visually**: Plot calibration curves before and after\n",
        "4. **Use Brier score**: Don't rely only on accuracy\n",
        "5. **Method selection**: Sigmoid for small data, isotonic for large\n",
        "6. **Don't over-calibrate**: Calibrate once, not multiple times\n",
        "7. **Consider cost**: Calibration adds computational overhead\n",
        "8. **Validate properly**: Use test set never seen during calibration\n",
        "\n",
        "### When Calibration Matters Most\n",
        "\n",
        "\u2713 **Critical Applications**:\n",
        "- Medical diagnosis (risk assessment)\n",
        "- Financial modeling (default probability)\n",
        "- Weather forecasting\n",
        "- Fraud detection with thresholds\n",
        "- Cost-sensitive decisions\n",
        "\n",
        "\u2717 **Less Critical**:\n",
        "- Binary classification with fixed 0.5 threshold\n",
        "- Ranking tasks (only order matters)\n",
        "- Already using Logistic Regression\n",
        "\n",
        "### Common Mistakes\n",
        "\n",
        "| Mistake | Consequence | Solution |\n",
        "|---------|-------------|----------|\n",
        "| Calibrate on training data | Overfitting | Use holdout or CV |\n",
        "| Use isotonic with <1k samples | Overfitting | Use sigmoid instead |\n",
        "| Ignore calibration curves | Miss miscalibration | Always visualize |\n",
        "| Multiple calibrations | Can hurt performance | Calibrate once |\n",
        "| Trust accuracy only | Miss probability errors | Use Brier score |\n",
        "\n",
        "### Computational Considerations\n",
        "\n",
        "**Training Time**:\n",
        "- Sigmoid: Fast (fits simple logistic)\n",
        "- Isotonic: Moderate (sorts and fits)\n",
        "- cv=5: 5\u00d7 slower than prefit\n",
        "\n",
        "**Prediction Time**:\n",
        "- Sigmoid: Minimal overhead\n",
        "- Isotonic: Minimal overhead\n",
        "- cv=5: Averages 5 predictions\n",
        "\n",
        "**Memory**:\n",
        "- Sigmoid: Stores 2 parameters (A, B)\n",
        "- Isotonic: Stores calibration mapping\n",
        "- cv=5: Stores 5 calibrators\n",
        "\n",
        "### Further Reading\n",
        "\n",
        "- **Paper**: \"Predicting Good Probabilities with Supervised Learning\" - Niculescu-Mizil & Caruana\n",
        "- **Paper**: \"Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers\" - Zadrozny & Elkan\n",
        "- **sklearn Docs**: https://scikit-learn.org/stable/modules/calibration.html\n",
        "- **Platt Scaling**: \"Probabilistic Outputs for Support Vector Machines\" - Platt (1999)\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Temperature scaling for neural networks\n",
        "- Multi-class calibration\n",
        "- Calibration in online learning\n",
        "- Beta calibration\n",
        "- Venn-ABERS predictors"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}