{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Naive Bayes Classifiers\n",
        "\n",
        "## Overview\n",
        "\n",
        "**Naive Bayes** is a family of probabilistic classifiers based on Bayes' theorem with the \"naive\" assumption of conditional independence between features.\n",
        "\n",
        "### Core Concept\n",
        "\n",
        "*\"Predict the class with highest posterior probability\"*\n",
        "\n",
        "### Key Ideas\n",
        "\n",
        "1. **Probabilistic**: Models probability distribution of features\n",
        "2. **Naive Independence**: Assumes features are conditionally independent given class\n",
        "3. **Fast**: Training and prediction are very efficient\n",
        "4. **Simple**: Minimal hyperparameters\n",
        "\n",
        "## Mathematical Foundation\n",
        "\n",
        "### Bayes' Theorem\n",
        "\n",
        "\\[\n",
        "P(y|x) = \\frac{P(x|y) \\cdot P(y)}{P(x)}\n",
        "\\]\n",
        "\n",
        "where:\n",
        "- \\(P(y|x)\\) = **Posterior**: Probability of class \\(y\\) given features \\(x\\)\n",
        "- \\(P(x|y)\\) = **Likelihood**: Probability of features \\(x\\) given class \\(y\\)\n",
        "- \\(P(y)\\) = **Prior**: Probability of class \\(y\\)\n",
        "- \\(P(x)\\) = **Evidence**: Probability of features \\(x\\) (constant for all classes)\n",
        "\n",
        "### Naive Bayes Classifier\n",
        "\n",
        "**Naive assumption**: Features are conditionally independent given class\n",
        "\n",
        "\\[\n",
        "P(x|y) = P(x_1, x_2, ..., x_d | y) = \\prod_{i=1}^{d} P(x_i | y)\n",
        "\\]\n",
        "\n",
        "**Classification**: Choose class with highest posterior probability\n",
        "\n",
        "\\[\n",
        "\\hat{y} = \\arg\\max_{y} P(y) \\prod_{i=1}^{d} P(x_i | y)\n",
        "\\]\n",
        "\n",
        "or in log-space (numerical stability):\n",
        "\n",
        "\\[\n",
        "\\hat{y} = \\arg\\max_{y} \\left[ \\log P(y) + \\sum_{i=1}^{d} \\log P(x_i | y) \\right]\n",
        "\\]\n",
        "\n",
        "## Types of Naive Bayes\n",
        "\n",
        "### 1. Gaussian Naive Bayes\n",
        "\n",
        "**Assumption**: Features follow Gaussian (normal) distribution\n",
        "\n",
        "\\[\n",
        "P(x_i | y) = \\frac{1}{\\sqrt{2\\pi\\sigma_y^2}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma_y^2}\\right)\n",
        "\\]\n",
        "\n",
        "**Use for**: Continuous features\n",
        "\n",
        "### 2. Multinomial Naive Bayes\n",
        "\n",
        "**Assumption**: Features represent counts or frequencies\n",
        "\n",
        "\\[\n",
        "P(x|y) = \\frac{(\\sum_i x_i)!}{\\prod_i x_i!} \\prod_i p_{yi}^{x_i}\n",
        "\\]\n",
        "\n",
        "**Use for**: Text classification (word counts), document categorization\n",
        "\n",
        "### 3. Bernoulli Naive Bayes\n",
        "\n",
        "**Assumption**: Features are binary (0/1)\n",
        "\n",
        "\\[\n",
        "P(x_i | y) = P(i|y) \\cdot x_i + (1 - P(i|y)) \\cdot (1 - x_i)\n",
        "\\]\n",
        "\n",
        "**Use for**: Binary features, presence/absence of features\n",
        "\n",
        "## Topics Covered\n",
        "\n",
        "1. Bayes' theorem intuition\n",
        "2. Gaussian Naive Bayes for continuous data\n",
        "3. Multinomial Naive Bayes for text classification\n",
        "4. Bernoulli Naive Bayes for binary features\n",
        "5. Comparing all three variants\n",
        "6. Laplace smoothing (alpha parameter)\n",
        "7. Real-world text classification\n",
        "8. Strengths, limitations, and best practices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from time import time\n",
        "\n",
        "# Naive Bayes models\n",
        "from sklearn.naive_bayes import (\n",
        "    GaussianNB, MultinomialNB, BernoulliNB,\n",
        "    ComplementNB, CategoricalNB\n",
        ")\n",
        "\n",
        "# Text processing\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Other models for comparison\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Utilities\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, cross_val_score, GridSearchCV\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, confusion_matrix,\n",
        "    roc_auc_score, log_loss\n",
        ")\n",
        "from sklearn.datasets import (\n",
        "    load_iris, load_wine, load_breast_cancer,\n",
        "    make_classification, fetch_20newsgroups\n",
        ")\n",
        "\n",
        "np.random.seed(42)\n",
        "sns.set_style('whitegrid')\n",
        "print(\"\u2713 Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bayes-intuition",
      "metadata": {},
      "source": [
        "## 1. Bayes' Theorem Intuition\n",
        "\n",
        "### 1.1 Simple Example: Medical Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bayes-example",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Bayes' Theorem: Medical Test Example\")\n",
        "print(\"=\"*70)\n",
        "print(\"Scenario: Testing for a rare disease\\n\")\n",
        "\n",
        "# Define probabilities\n",
        "P_disease = 0.01  # 1% of population has disease (Prior)\n",
        "P_positive_given_disease = 0.95  # 95% sensitivity (True Positive Rate)\n",
        "P_positive_given_healthy = 0.05  # 5% false positive rate\n",
        "\n",
        "# Calculate P(positive) using law of total probability\n",
        "P_positive = (P_positive_given_disease * P_disease + \n",
        "              P_positive_given_healthy * (1 - P_disease))\n",
        "\n",
        "# Apply Bayes' Theorem: P(disease | positive)\n",
        "P_disease_given_positive = (P_positive_given_disease * P_disease) / P_positive\n",
        "\n",
        "print(f\"Prior Probability:\")\n",
        "print(f\"  P(disease) = {P_disease:.3f} = {P_disease*100:.1f}%\\n\")\n",
        "\n",
        "print(f\"Likelihood:\")\n",
        "print(f\"  P(positive | disease) = {P_positive_given_disease:.3f} = {P_positive_given_disease*100:.0f}%\")\n",
        "print(f\"  P(positive | healthy) = {P_positive_given_healthy:.3f} = {P_positive_given_healthy*100:.0f}%\\n\")\n",
        "\n",
        "print(f\"Evidence:\")\n",
        "print(f\"  P(positive) = {P_positive:.4f} = {P_positive*100:.2f}%\\n\")\n",
        "\n",
        "print(f\"Posterior Probability (using Bayes' Theorem):\")\n",
        "print(f\"  P(disease | positive) = {P_disease_given_positive:.4f} = {P_disease_given_positive*100:.1f}%\\n\")\n",
        "\n",
        "print(\"\ud83d\udca1 Key Insight:\")\n",
        "print(f\"   Even with 95% test accuracy, only {P_disease_given_positive*100:.1f}% chance of having disease!\")\n",
        "print(\"   Why? The disease is rare (low prior probability)\")\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Before test (Prior)\n",
        "axes[0].bar(['Healthy', 'Disease'], [1-P_disease, P_disease], color=['green', 'red'], alpha=0.7)\n",
        "axes[0].set_ylabel('Probability')\n",
        "axes[0].set_title('Before Test (Prior)\\nP(disease) = 1%')\n",
        "axes[0].set_ylim([0, 1])\n",
        "\n",
        "# After positive test (Posterior)\n",
        "axes[1].bar(['Healthy', 'Disease'], \n",
        "           [1-P_disease_given_positive, P_disease_given_positive], \n",
        "           color=['green', 'red'], alpha=0.7)\n",
        "axes[1].set_ylabel('Probability')\n",
        "axes[1].set_title(f'After Positive Test (Posterior)\\nP(disease|positive) = {P_disease_given_positive*100:.1f}%')\n",
        "axes[1].set_ylim([0, 1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nThis is the foundation of Naive Bayes classification!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gaussian-nb",
      "metadata": {},
      "source": [
        "## 2. Gaussian Naive Bayes\n",
        "\n",
        "### 2.1 For Continuous Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gaussian-nb-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target\n",
        "\n",
        "print(\"Gaussian Naive Bayes - Iris Dataset\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_iris.shape[0]}\")\n",
        "print(f\"Features: {X_iris.shape[1]} (continuous)\")\n",
        "print(f\"Classes: {iris.target_names}\\n\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
        ")\n",
        "\n",
        "# Train Gaussian NB\n",
        "gnb = GaussianNB()\n",
        "\n",
        "start_time = time()\n",
        "gnb.fit(X_train, y_train)\n",
        "train_time = time() - start_time\n",
        "\n",
        "# Predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "y_pred_proba = gnb.predict_proba(X_test)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Training time: {train_time:.6f}s\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "# Show learned parameters\n",
        "print(\"\\nLearned Parameters (Mean and Variance per class):\")\n",
        "print(\"=\"*70)\n",
        "for idx, class_name in enumerate(iris.target_names):\n",
        "    print(f\"\\n{class_name}:\")\n",
        "    print(f\"  Prior: P({class_name}) = {gnb.class_prior_[idx]:.3f}\")\n",
        "    print(f\"  Means: {gnb.theta_[idx]}\")\n",
        "    print(f\"  Variances: {gnb.var_[idx]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gaussian-nb-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Gaussian distributions for first feature\n",
        "print(\"\\nVisualizing Feature Distributions\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for feature_idx in range(4):\n",
        "    ax = axes[feature_idx]\n",
        "    \n",
        "    # Plot histograms for each class\n",
        "    for class_idx, class_name in enumerate(iris.target_names):\n",
        "        class_data = X_train[y_train == class_idx, feature_idx]\n",
        "        ax.hist(class_data, bins=15, alpha=0.5, label=class_name)\n",
        "        \n",
        "        # Overlay Gaussian\n",
        "        mu = gnb.theta_[class_idx, feature_idx]\n",
        "        sigma = np.sqrt(gnb.var_[class_idx, feature_idx])\n",
        "        x_range = np.linspace(X_train[:, feature_idx].min(), \n",
        "                             X_train[:, feature_idx].max(), 100)\n",
        "        gaussian = (1/(sigma * np.sqrt(2*np.pi))) * np.exp(-0.5*((x_range-mu)/sigma)**2)\n",
        "        # Scale to match histogram\n",
        "        gaussian_scaled = gaussian * len(class_data) * (x_range[1] - x_range[0]) * 15\n",
        "        ax.plot(x_range, gaussian_scaled, linewidth=2)\n",
        "    \n",
        "    ax.set_xlabel(iris.feature_names[feature_idx])\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.set_title(f'Feature {feature_idx+1}: {iris.feature_names[feature_idx]}')\n",
        "    ax.legend()\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Gaussian NB fits a Gaussian distribution for each feature per class\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "multinomial-nb",
      "metadata": {},
      "source": [
        "## 3. Multinomial Naive Bayes\n",
        "\n",
        "### 3.1 For Count Data (Text Classification)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "multinomial-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Multinomial Naive Bayes - Text Classification\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Sample text data\n",
        "texts = [\n",
        "    \"I love this movie\",\n",
        "    \"This film is great\",\n",
        "    \"Best movie ever\",\n",
        "    \"Amazing film\",\n",
        "    \"I hate this movie\",\n",
        "    \"Worst film ever\",\n",
        "    \"Terrible movie\",\n",
        "    \"Bad film\"\n",
        "]\n",
        "\n",
        "labels = [1, 1, 1, 1, 0, 0, 0, 0]  # 1=positive, 0=negative\n",
        "\n",
        "print(\"Training Data:\")\n",
        "for text, label in zip(texts, labels):\n",
        "    sentiment = \"Positive\" if label == 1 else \"Negative\"\n",
        "    print(f\"  [{sentiment}] {text}\")\n",
        "\n",
        "# Convert to count vectors\n",
        "vectorizer = CountVectorizer()\n",
        "X_counts = vectorizer.fit_transform(texts)\n",
        "\n",
        "print(f\"\\nVocabulary: {vectorizer.get_feature_names_out()}\")\n",
        "print(f\"Vocabulary size: {len(vectorizer.get_feature_names_out())}\")\n",
        "\n",
        "# Show count matrix\n",
        "print(\"\\nCount Matrix:\")\n",
        "count_df = pd.DataFrame(\n",
        "    X_counts.toarray(),\n",
        "    columns=vectorizer.get_feature_names_out(),\n",
        "    index=[f\"Doc{i+1}\" for i in range(len(texts))]\n",
        ")\n",
        "print(count_df)\n",
        "\n",
        "# Train Multinomial NB\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_counts, labels)\n",
        "\n",
        "print(f\"\\nTrained Multinomial Naive Bayes\")\n",
        "print(f\"Class priors: {mnb.class_prior_}\")\n",
        "\n",
        "# Test predictions\n",
        "test_texts = [\n",
        "    \"This movie is great\",\n",
        "    \"I hate this film\",\n",
        "    \"Amazing and best\"\n",
        "]\n",
        "\n",
        "X_test_counts = vectorizer.transform(test_texts)\n",
        "predictions = mnb.predict(X_test_counts)\n",
        "probabilities = mnb.predict_proba(X_test_counts)\n",
        "\n",
        "print(\"\\nTest Predictions:\")\n",
        "for text, pred, proba in zip(test_texts, predictions, probabilities):\n",
        "    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
        "    print(f\"  '{text}'\")\n",
        "    print(f\"    \u2192 {sentiment} (confidence: {proba[pred]:.3f})\")\n",
        "    print(f\"    Probabilities: [Neg={proba[0]:.3f}, Pos={proba[1]:.3f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "real-text-classification",
      "metadata": {},
      "source": [
        "### 3.2 Real-World Text Classification: 20 Newsgroups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "newsgroups-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"20 Newsgroups Dataset - Multinomial Naive Bayes\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load subset of categories\n",
        "categories = ['sci.space', 'rec.sport.baseball', 'comp.graphics']\n",
        "\n",
        "print(f\"Loading {len(categories)} categories: {categories}\\n\")\n",
        "\n",
        "# Load train and test data\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories,\n",
        "                                      remove=('headers', 'footers', 'quotes'),\n",
        "                                      random_state=42)\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories,\n",
        "                                     remove=('headers', 'footers', 'quotes'),\n",
        "                                     random_state=42)\n",
        "\n",
        "print(f\"Training samples: {len(newsgroups_train.data)}\")\n",
        "print(f\"Test samples: {len(newsgroups_test.data)}\\n\")\n",
        "\n",
        "# Show sample documents\n",
        "print(\"Sample Documents:\")\n",
        "print(\"=\"*70)\n",
        "for i in range(3):\n",
        "    print(f\"\\nDocument {i+1} - Category: {newsgroups_train.target_names[newsgroups_train.target[i]]}\")\n",
        "    print(newsgroups_train.data[i][:200] + \"...\\n\")\n",
        "\n",
        "# Convert to count vectors\n",
        "print(\"Converting to count vectors...\")\n",
        "vectorizer_news = CountVectorizer(max_features=5000, stop_words='english')\n",
        "X_train_counts = vectorizer_news.fit_transform(newsgroups_train.data)\n",
        "X_test_counts = vectorizer_news.transform(newsgroups_test.data)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vectorizer_news.get_feature_names_out())}\")\n",
        "print(f\"Train matrix shape: {X_train_counts.shape}\")\n",
        "print(f\"Test matrix shape: {X_test_counts.shape}\")\n",
        "\n",
        "# Train Multinomial NB\n",
        "print(\"\\nTraining Multinomial Naive Bayes...\")\n",
        "start = time()\n",
        "mnb_news = MultinomialNB(alpha=1.0)\n",
        "mnb_news.fit(X_train_counts, newsgroups_train.target)\n",
        "train_time = time() - start\n",
        "\n",
        "# Predict\n",
        "start = time()\n",
        "y_pred = mnb_news.predict(X_test_counts)\n",
        "predict_time = time() - start\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(newsgroups_test.target, y_pred)\n",
        "\n",
        "print(f\"\\nTraining time: {train_time:.3f}s\")\n",
        "print(f\"Prediction time: {predict_time:.3f}s\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(newsgroups_test.target, y_pred, \n",
        "                           target_names=newsgroups_train.target_names))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(newsgroups_test.target, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "           xticklabels=newsgroups_train.target_names,\n",
        "           yticklabels=newsgroups_train.target_names)\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.title('Multinomial NB - Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Multinomial NB is very fast and effective for text classification!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bernoulli-nb",
      "metadata": {},
      "source": [
        "## 4. Bernoulli Naive Bayes\n",
        "\n",
        "### 4.1 For Binary Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bernoulli-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Bernoulli Naive Bayes - Binary Features\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Use same text data but convert to binary (word presence/absence)\n",
        "texts_binary = [\n",
        "    \"I love this movie\",\n",
        "    \"This film is great\",\n",
        "    \"Best movie ever\",\n",
        "    \"Amazing film\",\n",
        "    \"I hate this movie\",\n",
        "    \"Worst film ever\",\n",
        "    \"Terrible movie\",\n",
        "    \"Bad film\"\n",
        "]\n",
        "\n",
        "labels_binary = [1, 1, 1, 1, 0, 0, 0, 0]\n",
        "\n",
        "# Convert to binary features (presence/absence)\n",
        "vectorizer_binary = CountVectorizer(binary=True)  # binary=True!\n",
        "X_binary = vectorizer_binary.fit_transform(texts_binary)\n",
        "\n",
        "print(\"Binary Feature Matrix (1 = word present, 0 = word absent):\")\n",
        "binary_df = pd.DataFrame(\n",
        "    X_binary.toarray(),\n",
        "    columns=vectorizer_binary.get_feature_names_out(),\n",
        "    index=[f\"Doc{i+1}\" for i in range(len(texts_binary))]\n",
        ")\n",
        "print(binary_df)\n",
        "\n",
        "# Train Bernoulli NB\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_binary, labels_binary)\n",
        "\n",
        "print(f\"\\nTrained Bernoulli Naive Bayes\")\n",
        "print(f\"Class priors: {bnb.class_prior_}\")\n",
        "\n",
        "# Feature log probabilities\n",
        "print(\"\\nFeature probabilities for each class:\")\n",
        "feature_probs = np.exp(bnb.feature_log_prob_)\n",
        "prob_df = pd.DataFrame(\n",
        "    feature_probs.T,\n",
        "    columns=['P(word|Negative)', 'P(word|Positive)'],\n",
        "    index=vectorizer_binary.get_feature_names_out()\n",
        ").round(3)\n",
        "print(prob_df)\n",
        "\n",
        "# Test\n",
        "test_binary = [\"This is an amazing movie\"]\n",
        "X_test_binary = vectorizer_binary.transform(test_binary)\n",
        "pred = bnb.predict(X_test_binary)\n",
        "proba = bnb.predict_proba(X_test_binary)\n",
        "\n",
        "print(f\"\\nTest: '{test_binary[0]}'\")\n",
        "print(f\"Prediction: {'Positive' if pred[0] == 1 else 'Negative'}\")\n",
        "print(f\"Probabilities: [Neg={proba[0][0]:.3f}, Pos={proba[0][1]:.3f}]\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Bernoulli NB: Each feature is binary (present/absent)\")\n",
        "print(\"   vs Multinomial NB: Uses word counts\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comparison",
      "metadata": {},
      "source": [
        "## 5. Comparing All Three Variants\n",
        "\n",
        "### 5.1 Same Dataset, Different NB Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nb-comparison",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Comparing Gaussian, Multinomial, and Bernoulli Naive Bayes\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Use breast cancer dataset (binary classification)\n",
        "cancer = load_breast_cancer()\n",
        "X_cancer = cancer.data\n",
        "y_cancer = cancer.target\n",
        "\n",
        "print(f\"Dataset: {cancer.DESCR.split(chr(10))[0]}\")\n",
        "print(f\"Samples: {X_cancer.shape[0]}\")\n",
        "print(f\"Features: {X_cancer.shape[1]} (continuous)\\n\")\n",
        "\n",
        "# Split\n",
        "X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = train_test_split(\n",
        "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
        ")\n",
        "\n",
        "# For Multinomial and Bernoulli: need non-negative features\n",
        "# Scale to [0, 1] range\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_cancer)\n",
        "X_test_scaled = scaler.transform(X_test_cancer)\n",
        "\n",
        "# Train all three\n",
        "models = {\n",
        "    'Gaussian NB': GaussianNB(),\n",
        "    'Multinomial NB': MultinomialNB(),\n",
        "    'Bernoulli NB': BernoulliNB()\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Use scaled data for Multinomial and Bernoulli\n",
        "    if 'Gaussian' in name:\n",
        "        X_tr, X_te = X_train_cancer, X_test_cancer\n",
        "    else:\n",
        "        X_tr, X_te = X_train_scaled, X_test_scaled\n",
        "    \n",
        "    # Train\n",
        "    start = time()\n",
        "    model.fit(X_tr, y_train_cancer)\n",
        "    train_time = time() - start\n",
        "    \n",
        "    # Predict\n",
        "    start = time()\n",
        "    y_pred = model.predict(X_te)\n",
        "    predict_time = time() - start\n",
        "    \n",
        "    # Evaluate\n",
        "    accuracy = accuracy_score(y_test_cancer, y_pred)\n",
        "    \n",
        "    # Cross-validation\n",
        "    cv_score = cross_val_score(model, X_tr, y_train_cancer, cv=5).mean()\n",
        "    \n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Train Time (s)': train_time,\n",
        "        'Predict Time (s)': predict_time,\n",
        "        'CV Score': cv_score,\n",
        "        'Test Accuracy': accuracy\n",
        "    })\n",
        "    \n",
        "    print(f\"{name:20} - Test Acc: {accuracy:.4f}, CV: {cv_score:.4f}, \"\n",
        "          f\"Train: {train_time:.5f}s\")\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy comparison\n",
        "x = np.arange(len(results_df))\n",
        "width = 0.35\n",
        "axes[0].bar(x - width/2, results_df['CV Score'], width, label='CV Score', alpha=0.8)\n",
        "axes[0].bar(x + width/2, results_df['Test Accuracy'], width, label='Test Accuracy', alpha=0.8)\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('Naive Bayes Variants - Accuracy Comparison')\n",
        "axes[0].set_xticks(x)\n",
        "axes[0].set_xticklabels(results_df['Model'])\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3, axis='y')\n",
        "\n",
        "# Training time comparison\n",
        "axes[1].bar(results_df['Model'], results_df['Train Time (s)'], alpha=0.8, color='orange')\n",
        "axes[1].set_ylabel('Training Time (seconds)')\n",
        "axes[1].set_title('Training Time Comparison')\n",
        "axes[1].grid(alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "alpha-smoothing",
      "metadata": {},
      "source": [
        "## 6. Laplace Smoothing (Alpha Parameter)\n",
        "\n",
        "### 6.1 Handling Zero Probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "alpha-smoothing-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Laplace Smoothing (Alpha Parameter)\")\n",
        "print(\"=\"*70)\n",
        "print(\"Problem: What if a word never appears in training data?\")\n",
        "print(\"  \u2192 P(word|class) = 0 \u2192 Entire probability becomes 0!\\n\")\n",
        "print(\"Solution: Add-alpha (Laplace) smoothing\")\n",
        "print(\"  \u2192 Add small constant \u03b1 to all counts\\n\")\n",
        "\n",
        "# Test different alpha values\n",
        "alpha_values = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
        "\n",
        "# Use newsgroups data\n",
        "alpha_results = []\n",
        "\n",
        "for alpha in alpha_values:\n",
        "    mnb_alpha = MultinomialNB(alpha=alpha)\n",
        "    mnb_alpha.fit(X_train_counts, newsgroups_train.target)\n",
        "    \n",
        "    y_pred_alpha = mnb_alpha.predict(X_test_counts)\n",
        "    accuracy_alpha = accuracy_score(newsgroups_test.target, y_pred_alpha)\n",
        "    \n",
        "    alpha_results.append({\n",
        "        'Alpha': alpha,\n",
        "        'Accuracy': accuracy_alpha\n",
        "    })\n",
        "    \n",
        "    print(f\"Alpha = {alpha:4.1f} \u2192 Accuracy: {accuracy_alpha:.4f}\")\n",
        "\n",
        "alpha_df = pd.DataFrame(alpha_results)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(alpha_df['Alpha'], alpha_df['Accuracy'], 'o-', linewidth=2, markersize=8)\n",
        "plt.xlabel('Alpha (Smoothing Parameter)')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.title('Effect of Laplace Smoothing on Multinomial NB')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.axvline(x=1.0, color='red', linestyle='--', alpha=0.5, label='\u03b1=1 (standard Laplace)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "best_alpha = alpha_df.loc[alpha_df['Accuracy'].idxmax(), 'Alpha']\n",
        "print(f\"\\nBest alpha: {best_alpha}\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Alpha Parameter:\")\n",
        "print(\"   \u03b1 = 0: No smoothing (can cause zero probabilities)\")\n",
        "print(\"   \u03b1 = 1: Laplace smoothing (standard choice)\")\n",
        "print(\"   \u03b1 > 1: More smoothing (more uniform distribution)\")\n",
        "print(\"   \\n   Tip: \u03b1=1 is usually a good default\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comparison-other-models",
      "metadata": {},
      "source": [
        "## 7. Naive Bayes vs Other Classifiers\n",
        "\n",
        "### 7.1 Performance and Speed Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "model-comparison",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Naive Bayes vs Other Classifiers - Text Classification\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Models to compare\n",
        "classifiers = {\n",
        "    'Multinomial NB': MultinomialNB(),\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'SVM (Linear)': SVC(kernel='linear', random_state=42)\n",
        "}\n",
        "\n",
        "comparison_results = []\n",
        "\n",
        "for name, clf in classifiers.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    \n",
        "    # Train\n",
        "    start = time()\n",
        "    clf.fit(X_train_counts, newsgroups_train.target)\n",
        "    train_time = time() - start\n",
        "    \n",
        "    # Predict\n",
        "    start = time()\n",
        "    y_pred_comp = clf.predict(X_test_counts)\n",
        "    predict_time = time() - start\n",
        "    \n",
        "    # Evaluate\n",
        "    accuracy_comp = accuracy_score(newsgroups_test.target, y_pred_comp)\n",
        "    \n",
        "    comparison_results.append({\n",
        "        'Model': name,\n",
        "        'Train Time (s)': train_time,\n",
        "        'Predict Time (s)': predict_time,\n",
        "        'Accuracy': accuracy_comp\n",
        "    })\n",
        "    \n",
        "    print(f\"  Accuracy: {accuracy_comp:.4f}, Train: {train_time:.3f}s, Predict: {predict_time:.4f}s\")\n",
        "\n",
        "comp_df = pd.DataFrame(comparison_results)\n",
        "\n",
        "print(\"\\n\" + comp_df.to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Training time\n",
        "axes[0].barh(comp_df['Model'], comp_df['Train Time (s)'], alpha=0.8)\n",
        "axes[0].set_xlabel('Training Time (seconds)')\n",
        "axes[0].set_title('Training Time Comparison')\n",
        "axes[0].grid(alpha=0.3, axis='x')\n",
        "\n",
        "# Prediction time\n",
        "axes[1].barh(comp_df['Model'], comp_df['Predict Time (s)'], alpha=0.8, color='orange')\n",
        "axes[1].set_xlabel('Prediction Time (seconds)')\n",
        "axes[1].set_title('Prediction Time Comparison')\n",
        "axes[1].grid(alpha=0.3, axis='x')\n",
        "\n",
        "# Accuracy\n",
        "axes[2].barh(comp_df['Model'], comp_df['Accuracy'], alpha=0.8, color='green')\n",
        "axes[2].set_xlabel('Accuracy')\n",
        "axes[2].set_title('Accuracy Comparison')\n",
        "axes[2].set_xlim([0.8, 1.0])\n",
        "axes[2].grid(alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Naive Bayes is extremely fast and competitive in accuracy!\")\n",
        "print(\"   Especially good for:\")\n",
        "print(\"   - Text classification\")\n",
        "print(\"   - Real-time prediction\")\n",
        "print(\"   - Large datasets\")\n",
        "print(\"   - Baseline models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "best-practices",
      "metadata": {},
      "source": [
        "## 8. Decision Guide and Best Practices\n",
        "\n",
        "### 8.1 Which Naive Bayes to Use?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decision-guide",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Naive Bayes Decision Guide\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "guide = [\n",
        "    {\n",
        "        'Data Type': 'Continuous features (real-valued)',\n",
        "        'Use': 'GaussianNB',\n",
        "        'Example': 'Medical measurements, sensor data'\n",
        "    },\n",
        "    {\n",
        "        'Data Type': 'Count data (word counts, frequencies)',\n",
        "        'Use': 'MultinomialNB',\n",
        "        'Example': 'Text classification, document categorization'\n",
        "    },\n",
        "    {\n",
        "        'Data Type': 'Binary features (yes/no, present/absent)',\n",
        "        'Use': 'BernoulliNB',\n",
        "        'Example': 'Feature presence, binary attributes'\n",
        "    },\n",
        "    {\n",
        "        'Data Type': 'Imbalanced text data',\n",
        "        'Use': 'ComplementNB',\n",
        "        'Example': 'Skewed text categories'\n",
        "    },\n",
        "    {\n",
        "        'Data Type': 'Categorical features',\n",
        "        'Use': 'CategoricalNB',\n",
        "        'Example': 'Nominal categories (color, size, etc.)'\n",
        "    },\n",
        "]\n",
        "\n",
        "guide_df = pd.DataFrame(guide)\n",
        "print(guide_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "strengths-limitations",
      "metadata": {},
      "source": [
        "### 8.2 Strengths and Limitations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "strengths-limitations",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nNaive Bayes Strengths and Limitations\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n\u2713 STRENGTHS:\")\n",
        "strengths = [\n",
        "    \"Very fast training and prediction\",\n",
        "    \"Works well with small training data\",\n",
        "    \"Handles high-dimensional data well\",\n",
        "    \"Not sensitive to irrelevant features\",\n",
        "    \"Provides probability estimates\",\n",
        "    \"Simple to understand and implement\",\n",
        "    \"Performs well on text classification\",\n",
        "    \"Good baseline model\",\n",
        "    \"Minimal hyperparameter tuning needed\",\n",
        "    \"Handles missing values naturally (Gaussian)\"\n",
        "]\n",
        "for i, s in enumerate(strengths, 1):\n",
        "    print(f\"  {i:2}. {s}\")\n",
        "\n",
        "print(\"\\n\u2717 LIMITATIONS:\")\n",
        "limitations = [\n",
        "    \"Strong independence assumption (often violated)\",\n",
        "    \"Cannot capture feature interactions\",\n",
        "    \"Probability estimates can be poorly calibrated\",\n",
        "    \"Zero-frequency problem (needs smoothing)\",\n",
        "    \"Assumes specific distribution (Gaussian for continuous)\",\n",
        "    \"Less accurate than discriminative models on complex tasks\",\n",
        "    \"Sensitive to feature scale (Gaussian)\",\n",
        "    \"Cannot handle continuous features (Multinomial/Bernoulli)\"\n",
        "]\n",
        "for i, l in enumerate(limitations, 1):\n",
        "    print(f\"  {i}. {l}\")\n",
        "\n",
        "print(\"\\n\\n\ud83d\udca1 WHEN TO USE NAIVE BAYES:\")\n",
        "use_cases = [\n",
        "    \"Text classification (spam detection, sentiment analysis)\",\n",
        "    \"Document categorization\",\n",
        "    \"Need for real-time prediction\",\n",
        "    \"Small training dataset\",\n",
        "    \"High-dimensional data\",\n",
        "    \"Need for simple baseline\",\n",
        "    \"Probabilistic predictions required\",\n",
        "    \"Multi-class classification\"\n",
        "]\n",
        "for i, u in enumerate(use_cases, 1):\n",
        "    print(f\"  {i}. {u}\")\n",
        "\n",
        "print(\"\\n\\n\u26a0\ufe0f WHEN TO AVOID NAIVE BAYES:\")\n",
        "avoid_cases = [\n",
        "    \"Features are highly correlated\",\n",
        "    \"Need to capture feature interactions\",\n",
        "    \"Require well-calibrated probabilities\",\n",
        "    \"Maximum accuracy is critical\",\n",
        "    \"Data doesn't fit distribution assumptions\",\n",
        "    \"Complex non-linear relationships\"\n",
        "]\n",
        "for i, a in enumerate(avoid_cases, 1):\n",
        "    print(f\"  {i}. {a}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Summary and Quick Reference\n",
        "\n",
        "### Quick Reference Code\n",
        "\n",
        "```python\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "\n",
        "# ===== GAUSSIAN NB (Continuous Features) =====\n",
        "gnb = GaussianNB(\n",
        "    priors=None,      # Class priors (None = use training data)\n",
        "    var_smoothing=1e-9  # Portion of largest variance added to all\n",
        ")\n",
        "\n",
        "# ===== MULTINOMIAL NB (Count Data) =====\n",
        "mnb = MultinomialNB(\n",
        "    alpha=1.0,        # Laplace smoothing (0=none, 1=standard)\n",
        "    fit_prior=True    # Learn class priors from data\n",
        ")\n",
        "\n",
        "# ===== BERNOULLI NB (Binary Features) =====\n",
        "bnb = BernoulliNB(\n",
        "    alpha=1.0,        # Laplace smoothing\n",
        "    binarize=0.0,     # Threshold for binarizing (None = already binary)\n",
        "    fit_prior=True\n",
        ")\n",
        "\n",
        "# Train and predict\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "probabilities = model.predict_proba(X_test)\n",
        "log_probs = model.predict_log_proba(X_test)  # For numerical stability\n",
        "```\n",
        "\n",
        "### Text Classification Pipeline\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Option 1: Count Vectors\n",
        "text_clf = Pipeline([\n",
        "    ('vect', CountVectorizer(max_features=5000, stop_words='english')),\n",
        "    ('clf', MultinomialNB(alpha=1.0))\n",
        "])\n",
        "\n",
        "# Option 2: TF-IDF\n",
        "text_clf_tfidf = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english')),\n",
        "    ('clf', MultinomialNB(alpha=1.0))\n",
        "])\n",
        "\n",
        "# Train\n",
        "text_clf.fit(texts_train, labels_train)\n",
        "predictions = text_clf.predict(texts_test)\n",
        "```\n",
        "\n",
        "### Key Hyperparameters\n",
        "\n",
        "**alpha (Multinomial/Bernoulli)**:\n",
        "- Laplace/Lidstone smoothing parameter\n",
        "- \u03b1 = 0: No smoothing\n",
        "- \u03b1 = 1: Laplace smoothing (standard)\n",
        "- \u03b1 > 1: More smoothing\n",
        "- Typical range: [0.1, 0.5, 1.0, 2.0]\n",
        "\n",
        "**var_smoothing (Gaussian)**:\n",
        "- Portion of largest variance added to all for stability\n",
        "- Default: 1e-9\n",
        "- Increase if underflow errors occur\n",
        "\n",
        "**binarize (Bernoulli)**:\n",
        "- Threshold for binarizing features\n",
        "- None: Assume features already binary\n",
        "- float: Binarize at this threshold\n",
        "\n",
        "### Comparison Table\n",
        "\n",
        "| Variant | Feature Type | Distribution | Use Case |\n",
        "|---------|--------------|--------------|----------|\n",
        "| GaussianNB | Continuous | Gaussian | Sensor data, measurements |\n",
        "| MultinomialNB | Counts/Frequencies | Multinomial | Text classification, word counts |\n",
        "| BernoulliNB | Binary (0/1) | Bernoulli | Feature presence/absence |\n",
        "| ComplementNB | Counts (imbalanced) | Complement | Imbalanced text data |\n",
        "| CategoricalNB | Categorical | Categorical | Nominal categories |\n",
        "\n",
        "### Computational Complexity\n",
        "\n",
        "| Phase | Complexity | Notes |\n",
        "|-------|------------|-------|\n",
        "| Training | O(n \u00d7 d) | Very fast |\n",
        "| Prediction | O(c \u00d7 d) | c = number of classes |\n",
        "| Memory | O(c \u00d7 d) | Stores parameters only |\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. **Choose right variant**: Match to your data type\n",
        "2. **Use smoothing**: \u03b1=1 is good default for Multinomial/Bernoulli\n",
        "3. **Text preprocessing**: Remove stopwords, use max_features\n",
        "4. **Try both CountVectorizer and TfidfVectorizer**: Test both\n",
        "5. **Check distribution**: Verify Gaussian assumption for GaussianNB\n",
        "6. **Consider calibration**: Use CalibratedClassifierCV if probabilities matter\n",
        "7. **Use as baseline**: Always try NB first for text\n",
        "8. **Log probabilities**: Use predict_log_proba for numerical stability\n",
        "\n",
        "### Common Pitfalls\n",
        "\n",
        "| Pitfall | Solution |\n",
        "|---------|----------|\n",
        "| Negative values in Multinomial | Use MinMaxScaler or absolute values |\n",
        "| Zero probabilities | Use alpha smoothing (\u03b1 \u2265 1) |\n",
        "| Poor probability calibration | Use CalibratedClassifierCV |\n",
        "| Wrong variant for data type | Match variant to feature type |\n",
        "| Correlated features | Consider feature selection or other models |\n",
        "| Underflow in probabilities | Use predict_log_proba |\n",
        "\n",
        "### Typical Performance\n",
        "\n",
        "**Text Classification**:\n",
        "- Training: <1s for 10k documents\n",
        "- Prediction: <0.1s for 1k documents\n",
        "- Accuracy: 80-95% depending on task\n",
        "\n",
        "**Continuous Data**:\n",
        "- Often 70-85% accuracy\n",
        "- Outperformed by SVM/Random Forest on complex tasks\n",
        "- Excellent baseline\n",
        "\n",
        "### Real-World Applications\n",
        "\n",
        "1. **Spam Detection**: Multinomial NB on email text\n",
        "2. **Sentiment Analysis**: Binary/Multinomial on reviews\n",
        "3. **Document Classification**: Multinomial NB on article text\n",
        "4. **Medical Diagnosis**: Gaussian NB on symptoms/measurements\n",
        "5. **Recommender Systems**: Collaborative filtering\n",
        "6. **Real-time Classification**: Fast prediction needed\n",
        "\n",
        "### Further Reading\n",
        "\n",
        "- **Paper**: \"Naive Bayes at Forty\" - Lewis (1998)\n",
        "- **Book**: \"Machine Learning: A Probabilistic Perspective\" - Murphy\n",
        "- **sklearn Docs**: https://scikit-learn.org/stable/modules/naive_bayes.html\n",
        "- **Text Classification**: \"Text Classification from Labeled and Unlabeled Documents\" - Nigam et al.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Probability calibration techniques\n",
        "- Semi-supervised Naive Bayes\n",
        "- Online learning with partial_fit()\n",
        "- Feature engineering for NB\n",
        "- Ensemble methods with NB"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}