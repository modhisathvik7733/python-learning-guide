{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Hyperparameter Tuning: GridSearchCV and RandomizedSearchCV\n",
        "\n",
        "## Overview\n",
        "\n",
        "**Why Hyperparameter Tuning?**\n",
        "- Default parameters rarely give best performance\n",
        "- Finding optimal parameters manually is time-consuming\n",
        "- Need systematic approach to explore parameter space\n",
        "\n",
        "## Tuning Strategies\n",
        "\n",
        "### 1. GridSearchCV\n",
        "- **Exhaustive search**: Tests all combinations\n",
        "- \u2713 Guaranteed to find best combination in grid\n",
        "- \u2717 Computationally expensive (exponential with parameters)\n",
        "- **Use when**: Small parameter space, computational resources available\n",
        "\n",
        "### 2. RandomizedSearchCV\n",
        "- **Random sampling**: Tests random combinations\n",
        "- \u2713 Much faster than grid search\n",
        "- \u2713 Can explore wider parameter ranges\n",
        "- \u2717 May miss optimal combination\n",
        "- **Use when**: Large parameter space, limited time/resources\n",
        "\n",
        "### 3. Nested Cross-Validation\n",
        "- **Unbiased evaluation**: Separates tuning from evaluation\n",
        "- \u2713 Prevents overfitting to validation set\n",
        "- \u2713 More reliable performance estimates\n",
        "- \u2717 Computationally expensive\n",
        "- **Use when**: Need unbiased final model assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from time import time\n",
        "from sklearn.datasets import load_breast_cancer, load_digits, load_wine\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    GridSearchCV,\n",
        "    RandomizedSearchCV,\n",
        "    cross_val_score,\n",
        "    KFold,\n",
        "    StratifiedKFold\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, make_scorer\n",
        "from scipy.stats import randint, uniform\n",
        "\n",
        "np.random.seed(42)\n",
        "print(\"\u2713 Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gridsearch-basic",
      "metadata": {},
      "source": [
        "## 1. GridSearchCV - Exhaustive Search\n",
        "\n",
        "### 1.1 Basic GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "grid-basic",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "print(\"Breast Cancer Dataset:\")\n",
        "print(f\"Samples: {len(X)}, Features: {X.shape[1]}\")\n",
        "print(f\"Classes: {cancer.target_names}\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
        "    'kernel': ['rbf', 'linear']\n",
        "}\n",
        "\n",
        "print(f\"\\nParameter Grid:\")\n",
        "for param, values in param_grid.items():\n",
        "    print(f\"  {param}: {values}\")\n",
        "\n",
        "total_combinations = np.prod([len(v) for v in param_grid.values()])\n",
        "print(f\"\\nTotal combinations: {total_combinations}\")\n",
        "print(f\"With 5-fold CV: {total_combinations * 5} model fits\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "grid-run",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    SVC(random_state=42),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,         # Use all CPU cores\n",
        "    verbose=2,         # Show progress\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Fit (this searches through all combinations)\n",
        "print(\"\\nStarting Grid Search...\")\n",
        "start_time = time()\n",
        "grid_search.fit(X_train, y_train)\n",
        "elapsed_time = time() - start_time\n",
        "\n",
        "print(f\"\\n\u2713 Grid Search completed in {elapsed_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "grid-results",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Best parameters and score\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"GRID SEARCH RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best CV Score: {grid_search.best_score_:.4f}\")\n",
        "print(f\"Best Estimator: {grid_search.best_estimator_}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = grid_search.predict(X_test)\n",
        "test_score = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nTest Set Performance:\")\n",
        "print(f\"  Accuracy: {test_score:.4f}\")\n",
        "\n",
        "# Compare with default parameters\n",
        "default_model = SVC(random_state=42)\n",
        "default_model.fit(X_train, y_train)\n",
        "default_score = default_model.score(X_test, y_test)\n",
        "\n",
        "print(f\"\\nDefault Parameters:\")\n",
        "print(f\"  Accuracy: {default_score:.4f}\")\n",
        "print(f\"\\nImprovement: {(test_score - default_score):.4f} ({(test_score/default_score - 1)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "grid-analysis",
      "metadata": {},
      "source": [
        "### 1.2 Analyzing Grid Search Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "grid-detailed",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get detailed results\n",
        "results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "\n",
        "# Show top 10 configurations\n",
        "print(\"\\nTop 10 Parameter Combinations:\")\n",
        "print(\"=\"*70)\n",
        "top_results = results_df[\n",
        "    ['params', 'mean_test_score', 'std_test_score', 'rank_test_score']\n",
        "].sort_values('rank_test_score').head(10)\n",
        "\n",
        "for idx, row in top_results.iterrows():\n",
        "    print(f\"\\nRank {row['rank_test_score']}: {row['mean_test_score']:.4f} (\u00b1{row['std_test_score']:.4f})\")\n",
        "    print(f\"  Params: {row['params']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "grid-visual",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize parameter effects (for numeric parameters)\n",
        "# Extract C and gamma values for rbf kernel\n",
        "rbf_results = results_df[results_df['param_kernel'] == 'rbf'].copy()\n",
        "\n",
        "# Create pivot table for heatmap\n",
        "if len(rbf_results) > 0:\n",
        "    # Convert to numeric\n",
        "    rbf_results['C_val'] = rbf_results['param_C']\n",
        "    rbf_results['gamma_val'] = rbf_results['param_gamma'].apply(\n",
        "        lambda x: 0.0001 if x == 'scale' else (0.0002 if x == 'auto' else x)\n",
        "    )\n",
        "    \n",
        "    pivot = rbf_results.pivot_table(\n",
        "        values='mean_test_score',\n",
        "        index='gamma_val',\n",
        "        columns='C_val'\n",
        "    )\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(pivot, annot=True, fmt='.3f', cmap='YlOrRd', cbar_kws={'label': 'Accuracy'})\n",
        "    plt.title('Grid Search Results: C vs Gamma (RBF Kernel)')\n",
        "    plt.xlabel('C (Regularization)')\n",
        "    plt.ylabel('Gamma')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "randomized-search",
      "metadata": {},
      "source": [
        "## 2. RandomizedSearchCV - Faster Alternative\n",
        "\n",
        "### 2.1 Basic RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "random-basic",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define parameter distributions\n",
        "param_distributions = {\n",
        "    'C': uniform(0.1, 100),              # Continuous uniform distribution\n",
        "    'gamma': uniform(0.0001, 0.1),       # Continuous uniform distribution\n",
        "    'kernel': ['rbf', 'linear', 'poly']  # Discrete choices\n",
        "}\n",
        "\n",
        "print(\"Parameter Distributions:\")\n",
        "print(\"  C: uniform(0.1, 100) - continuous\")\n",
        "print(\"  gamma: uniform(0.0001, 0.1) - continuous\")\n",
        "print(\"  kernel: ['rbf', 'linear', 'poly'] - discrete\")\n",
        "\n",
        "# Create RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    SVC(random_state=42),\n",
        "    param_distributions,\n",
        "    n_iter=50,         # Number of random combinations to try\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "print(f\"\\nTotal evaluations: {50} random combinations \u00d7 5 folds = {50*5} fits\")\n",
        "print(\"(Much less than GridSearch's {})\\n\".format(total_combinations * 5))\n",
        "\n",
        "# Fit\n",
        "print(\"Starting Randomized Search...\")\n",
        "start_time = time()\n",
        "random_search.fit(X_train, y_train)\n",
        "elapsed_time = time() - start_time\n",
        "\n",
        "print(f\"\\n\u2713 Randomized Search completed in {elapsed_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "random-results",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Results\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RANDOMIZED SEARCH RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nBest Parameters: {random_search.best_params_}\")\n",
        "print(f\"Best CV Score: {random_search.best_score_:.4f}\")\n",
        "\n",
        "# Test set performance\n",
        "y_pred_random = random_search.predict(X_test)\n",
        "test_score_random = accuracy_score(y_test, y_pred_random)\n",
        "print(f\"\\nTest Set Accuracy: {test_score_random:.4f}\")\n",
        "\n",
        "# Compare with GridSearch\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(\"COMPARISON: GridSearch vs RandomizedSearch\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nGridSearch:\")\n",
        "print(f\"  Best CV Score: {grid_search.best_score_:.4f}\")\n",
        "print(f\"  Test Accuracy: {test_score:.4f}\")\n",
        "print(f\"\\nRandomizedSearch:\")\n",
        "print(f\"  Best CV Score: {random_search.best_score_:.4f}\")\n",
        "print(f\"  Test Accuracy: {test_score_random:.4f}\")\n",
        "print(f\"\\n\ud83d\udca1 RandomizedSearch achieved similar performance with fewer evaluations!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "random-forest-example",
      "metadata": {},
      "source": [
        "### 2.2 RandomizedSearch for Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rf-random",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load larger dataset\n",
        "digits = load_digits()\n",
        "X_digits, y_digits = digits.data, digits.target\n",
        "\n",
        "print(f\"Digits Dataset: {X_digits.shape[0]} samples, {X_digits.shape[1]} features, {len(np.unique(y_digits))} classes\")\n",
        "\n",
        "# Split\n",
        "X_tr_dig, X_te_dig, y_tr_dig, y_te_dig = train_test_split(\n",
        "    X_digits, y_digits, test_size=0.2, random_state=42, stratify=y_digits\n",
        ")\n",
        "\n",
        "# Define extensive parameter space for Random Forest\n",
        "rf_param_dist = {\n",
        "    'n_estimators': randint(50, 500),           # Discrete uniform\n",
        "    'max_depth': randint(5, 50),                # Discrete uniform\n",
        "    'min_samples_split': randint(2, 20),       # Discrete uniform\n",
        "    'min_samples_leaf': randint(1, 10),        # Discrete uniform\n",
        "    'max_features': ['sqrt', 'log2', None],    # Discrete choices\n",
        "    'bootstrap': [True, False]                  # Boolean\n",
        "}\n",
        "\n",
        "print(\"\\nRandom Forest Parameter Space:\")\n",
        "for param, dist in rf_param_dist.items():\n",
        "    print(f\"  {param}: {dist}\")\n",
        "\n",
        "# Randomized search\n",
        "rf_random = RandomizedSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    rf_param_dist,\n",
        "    n_iter=100,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\nSearching for best parameters...\")\n",
        "rf_random.fit(X_tr_dig, y_tr_dig)\n",
        "\n",
        "print(f\"\\nBest Parameters:\")\n",
        "for param, value in rf_random.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "print(f\"\\nBest CV Accuracy: {rf_random.best_score_:.4f}\")\n",
        "print(f\"Test Accuracy: {rf_random.score(X_te_dig, y_te_dig):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pipeline-tuning",
      "metadata": {},
      "source": [
        "## 3. Tuning Pipelines\n",
        "\n",
        "### 3.1 GridSearch with Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pipeline-grid",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', SVC(random_state=42))\n",
        "])\n",
        "\n",
        "# Parameter grid for pipeline (note the naming: step__parameter)\n",
        "pipeline_params = {\n",
        "    'classifier__C': [0.1, 1, 10],\n",
        "    'classifier__gamma': [0.001, 0.01, 0.1],\n",
        "    'classifier__kernel': ['rbf', 'linear']\n",
        "}\n",
        "\n",
        "print(\"Pipeline:\")\n",
        "print(pipeline)\n",
        "print(\"\\nParameter Grid (pipeline syntax):\")\n",
        "for param, values in pipeline_params.items():\n",
        "    print(f\"  {param}: {values}\")\n",
        "\n",
        "# Grid search on pipeline\n",
        "pipeline_grid = GridSearchCV(\n",
        "    pipeline,\n",
        "    pipeline_params,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "pipeline_grid.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\nBest Pipeline Parameters: {pipeline_grid.best_params_}\")\n",
        "print(f\"Best CV Score: {pipeline_grid.best_score_:.4f}\")\n",
        "print(f\"Test Accuracy: {pipeline_grid.score(X_test, y_test):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "multiple-models",
      "metadata": {},
      "source": [
        "### 3.2 Comparing Multiple Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "multi-model",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define models and their parameter spaces\n",
        "models = {\n",
        "    'Logistic Regression': {\n",
        "        'model': LogisticRegression(max_iter=1000, random_state=42),\n",
        "        'params': {\n",
        "            'C': [0.01, 0.1, 1, 10],\n",
        "            'penalty': ['l1', 'l2'],\n",
        "            'solver': ['liblinear']\n",
        "        }\n",
        "    },\n",
        "    'SVM': {\n",
        "        'model': SVC(random_state=42),\n",
        "        'params': {\n",
        "            'C': [0.1, 1, 10],\n",
        "            'gamma': [0.001, 0.01, 0.1],\n",
        "            'kernel': ['rbf']\n",
        "        }\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'model': RandomForestClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [10, 20, None],\n",
        "            'min_samples_split': [2, 5]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Comparing Multiple Models with Hyperparameter Tuning\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "results = []\n",
        "for name, config in models.items():\n",
        "    print(f\"\\nTuning {name}...\")\n",
        "    \n",
        "    grid = GridSearchCV(\n",
        "        config['model'],\n",
        "        config['params'],\n",
        "        cv=5,\n",
        "        scoring='accuracy',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    grid.fit(X_train, y_train)\n",
        "    \n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Best CV Score': grid.best_score_,\n",
        "        'Test Score': grid.score(X_test, y_test),\n",
        "        'Best Params': grid.best_params_\n",
        "    })\n",
        "    \n",
        "    print(f\"  Best CV Score: {grid.best_score_:.4f}\")\n",
        "    print(f\"  Test Score: {grid.score(X_test, y_test):.4f}\")\n",
        "\n",
        "# Summary\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(results_df[['Model', 'Best CV Score', 'Test Score']].to_string(index=False))\n",
        "\n",
        "best_model = results_df.loc[results_df['Test Score'].idxmax()]\n",
        "print(f\"\\n\ud83c\udfc6 Best Model: {best_model['Model']} (Test Score: {best_model['Test Score']:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nested-cv",
      "metadata": {},
      "source": [
        "## 4. Nested Cross-Validation\n",
        "\n",
        "**Why Nested CV?**\n",
        "- Regular GridSearchCV uses same data for tuning and evaluation\n",
        "- This can lead to overfitting to validation set\n",
        "- Nested CV provides unbiased performance estimate\n",
        "\n",
        "**Structure:**\n",
        "```\n",
        "Outer Loop (evaluation): K folds\n",
        "  Inner Loop (tuning): M folds\n",
        "    - Find best parameters on training folds\n",
        "  - Evaluate with best parameters on test fold\n",
        "Final Score = Average of outer loop scores\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nested-cv",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load wine dataset for nested CV example\n",
        "wine = load_wine()\n",
        "X_wine, y_wine = wine.data, wine.target\n",
        "\n",
        "print(\"Wine Dataset:\")\n",
        "print(f\"Samples: {len(X_wine)}, Features: {X_wine.shape[1]}, Classes: {len(np.unique(y_wine))}\")\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid_nested = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': [0.001, 0.01, 0.1],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# Nested cross-validation\n",
        "print(\"\\nPerforming Nested Cross-Validation...\")\n",
        "print(\"  Outer CV: 5 folds (for evaluation)\")\n",
        "print(\"  Inner CV: 3 folds (for hyperparameter tuning)\")\n",
        "\n",
        "# Outer CV\n",
        "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# Inner CV\n",
        "inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "nested_scores = []\n",
        "best_params_list = []\n",
        "\n",
        "for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X_wine, y_wine), 1):\n",
        "    print(f\"\\nOuter Fold {fold}/5:\")\n",
        "    \n",
        "    X_train_outer, X_test_outer = X_wine[train_idx], X_wine[test_idx]\n",
        "    y_train_outer, y_test_outer = y_wine[train_idx], y_wine[test_idx]\n",
        "    \n",
        "    # Inner grid search\n",
        "    inner_grid = GridSearchCV(\n",
        "        SVC(random_state=42),\n",
        "        param_grid_nested,\n",
        "        cv=inner_cv,\n",
        "        scoring='accuracy',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    inner_grid.fit(X_train_outer, y_train_outer)\n",
        "    \n",
        "    # Evaluate on outer test fold\n",
        "    score = inner_grid.score(X_test_outer, y_test_outer)\n",
        "    nested_scores.append(score)\n",
        "    best_params_list.append(inner_grid.best_params_)\n",
        "    \n",
        "    print(f\"  Best params: {inner_grid.best_params_}\")\n",
        "    print(f\"  Inner CV score: {inner_grid.best_score_:.4f}\")\n",
        "    print(f\"  Outer test score: {score:.4f}\")\n",
        "\n",
        "# Final results\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"NESTED CROSS-VALIDATION RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nOuter fold scores: {[f'{s:.4f}' for s in nested_scores]}\")\n",
        "print(f\"\\nMean Score: {np.mean(nested_scores):.4f}\")\n",
        "print(f\"Std Score: {np.std(nested_scores):.4f}\")\n",
        "print(f\"\\n\u2713 This is an unbiased estimate of model performance!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nested-comparison",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare with regular GridSearchCV\n",
        "print(\"\\nComparison: Nested CV vs Regular GridSearchCV\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Regular GridSearchCV (potentially biased)\n",
        "regular_grid = GridSearchCV(\n",
        "    SVC(random_state=42),\n",
        "    param_grid_nested,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Get CV score (this is what grid search reports)\n",
        "regular_grid.fit(X_wine, y_wine)\n",
        "regular_cv_score = regular_grid.best_score_\n",
        "\n",
        "print(f\"\\nRegular GridSearchCV:\")\n",
        "print(f\"  Best CV Score: {regular_cv_score:.4f}\")\n",
        "print(f\"  (May be optimistically biased)\")\n",
        "\n",
        "print(f\"\\nNested CV:\")\n",
        "print(f\"  Mean Score: {np.mean(nested_scores):.4f} \u00b1 {np.std(nested_scores):.4f}\")\n",
        "print(f\"  (Unbiased estimate)\")\n",
        "\n",
        "print(f\"\\nDifference: {regular_cv_score - np.mean(nested_scores):.4f}\")\n",
        "print(\"\\n\ud83d\udca1 Nested CV usually gives lower (more realistic) scores\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "best-practices",
      "metadata": {},
      "source": [
        "## 5. Best Practices and Tips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "best-practices-guide",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Hyperparameter Tuning Best Practices\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "practices = [\n",
        "    (\"1. Start with coarse grid, refine iteratively\",\n",
        "     \"Grid: [0.1, 1, 10, 100] \u2192 Refine: [8, 9, 10, 11, 12]\"),\n",
        "    \n",
        "    (\"2. Use RandomizedSearch for initial exploration\",\n",
        "     \"Then GridSearch around best region\"),\n",
        "    \n",
        "    (\"3. Use log scale for large ranges\",\n",
        "     \"C: [0.001, 0.01, 0.1, 1, 10, 100] instead of linear\"),\n",
        "    \n",
        "    (\"4. Set n_jobs=-1 for parallel processing\",\n",
        "     \"Speeds up search significantly\"),\n",
        "    \n",
        "    (\"5. Always use stratified splits for classification\",\n",
        "     \"cv=StratifiedKFold() instead of int\"),\n",
        "    \n",
        "    (\"6. Include preprocessing in pipeline\",\n",
        "     \"Prevents data leakage\"),\n",
        "    \n",
        "    (\"7. Use nested CV for unbiased evaluation\",\n",
        "     \"When reporting final model performance\"),\n",
        "    \n",
        "    (\"8. Consider computational cost\",\n",
        "     \"Large grid \u00d7 deep trees \u00d7 big data = very slow\"),\n",
        "    \n",
        "    (\"9. Save cv_results_ for analysis\",\n",
        "     \"grid.cv_results_ contains detailed information\"),\n",
        "    \n",
        "    (\"10. Set random_state for reproducibility\",\n",
        "     \"In both model and search object\")\n",
        "]\n",
        "\n",
        "for practice, explanation in practices:\n",
        "    print(f\"\\n{practice}\")\n",
        "    print(f\"  \u2192 {explanation}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "decision-guide",
      "metadata": {},
      "source": [
        "## 6. Decision Guide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decision-table",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create decision guide\n",
        "guide = pd.DataFrame({\n",
        "    'Scenario': [\n",
        "        'Small parameter space (<100 combinations)',\n",
        "        'Large parameter space (>1000 combinations)',\n",
        "        'Continuous parameters',\n",
        "        'Limited computational resources',\n",
        "        'Need best possible model',\n",
        "        'Initial exploration',\n",
        "        'Final model evaluation',\n",
        "        'Model comparison',\n",
        "        'Very small dataset',\n",
        "        'Production deployment'\n",
        "    ],\n",
        "    'Recommended Method': [\n",
        "        'GridSearchCV',\n",
        "        'RandomizedSearchCV',\n",
        "        'RandomizedSearchCV',\n",
        "        'RandomizedSearchCV with fewer iterations',\n",
        "        'GridSearchCV (after RandomizedSearch)',\n",
        "        'RandomizedSearchCV',\n",
        "        'Nested CV',\n",
        "        'GridSearchCV on each model',\n",
        "        'Nested CV with fewer folds',\n",
        "        'Retrain with best params on all data'\n",
        "    ],\n",
        "    'CV Folds': [\n",
        "        '5',\n",
        "        '3-5',\n",
        "        '5',\n",
        "        '3',\n",
        "        '5-10',\n",
        "        '3',\n",
        "        '5 outer, 3 inner',\n",
        "        '5',\n",
        "        '3',\n",
        "        'All data'\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\nDecision Guide: When to Use Which Method\")\n",
        "print(\"=\"*90)\n",
        "print(guide.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Quick Reference\n",
        "\n",
        "```python\n",
        "# GridSearchCV - Exhaustive search\n",
        "param_grid = {'C': [0.1, 1, 10], 'kernel': ['rbf', 'linear']}\n",
        "grid = GridSearchCV(model, param_grid, cv=5, n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "# RandomizedSearchCV - Random sampling\n",
        "param_dist = {'C': uniform(0.1, 100), 'gamma': uniform(0.001, 0.1)}\n",
        "random = RandomizedSearchCV(model, param_dist, n_iter=50, cv=5, n_jobs=-1)\n",
        "random.fit(X_train, y_train)\n",
        "\n",
        "# Pipeline tuning\n",
        "pipeline = Pipeline([('scaler', StandardScaler()), ('clf', SVC())])\n",
        "params = {'clf__C': [0.1, 1, 10], 'clf__gamma': [0.001, 0.01]}\n",
        "grid = GridSearchCV(pipeline, params, cv=5)\n",
        "\n",
        "# Nested CV - Unbiased evaluation\n",
        "outer_cv = KFold(n_splits=5)\n",
        "for train, test in outer_cv.split(X):\n",
        "    inner_grid = GridSearchCV(model, params, cv=3)\n",
        "    inner_grid.fit(X[train], y[train])\n",
        "    score = inner_grid.score(X[test], y[test])\n",
        "```\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **GridSearch**: Guaranteed to find best in grid, but expensive\n",
        "2. **RandomizedSearch**: Much faster, often finds near-optimal solutions\n",
        "3. **Start random, refine with grid**: Best strategy for large spaces\n",
        "4. **Nested CV**: Use for unbiased final performance estimate\n",
        "5. **Pipeline tuning**: Prevents data leakage, cleaner code\n",
        "6. **Use parallel processing**: Set n_jobs=-1\n",
        "7. **Stratified CV**: Essential for classification\n",
        "8. **Compare multiple models**: Tune each, then compare on test set\n",
        "\n",
        "### Common Pitfalls\n",
        "\n",
        "- \u274c Testing on same data used for tuning\n",
        "- \u274c Not scaling before tuning distance-based models\n",
        "- \u274c Using too many folds with small datasets\n",
        "- \u274c Forgetting to set random_state\n",
        "- \u274c Tuning on test set (never do this!)\n",
        "- \u274c Grid search with very large parameter spaces"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}