{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Scoring and Evaluation Metrics\n",
        "\n",
        "## Overview\n",
        "\n",
        "**Why Multiple Metrics?**\n",
        "- Accuracy alone can be misleading (especially with imbalanced data)\n",
        "- Different problems need different metrics\n",
        "- Understanding trade-offs is crucial\n",
        "\n",
        "## Metric Categories\n",
        "\n",
        "### Classification Metrics\n",
        "- **Accuracy**: Overall correctness\n",
        "- **Precision**: Positive prediction accuracy\n",
        "- **Recall**: True positive detection rate\n",
        "- **F1-Score**: Harmonic mean of precision and recall\n",
        "- **ROC-AUC**: Discrimination ability\n",
        "- **Confusion Matrix**: Detailed error analysis\n",
        "\n",
        "### Regression Metrics\n",
        "- **MAE**: Mean Absolute Error\n",
        "- **MSE**: Mean Squared Error\n",
        "- **RMSE**: Root Mean Squared Error\n",
        "- **R\u00b2 Score**: Explained variance\n",
        "- **MAPE**: Mean Absolute Percentage Error"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer, load_diabetes, make_classification\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import (\n",
        "    # Classification metrics\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report,\n",
        "    roc_curve, roc_auc_score, auc,\n",
        "    precision_recall_curve, average_precision_score,\n",
        "    # Regression metrics\n",
        "    mean_absolute_error, mean_squared_error, r2_score,\n",
        "    mean_absolute_percentage_error, max_error\n",
        ")\n",
        "\n",
        "np.random.seed(42)\n",
        "sns.set_style('whitegrid')\n",
        "print(\"\u2713 Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "classification-basics",
      "metadata": {},
      "source": [
        "## Part 1: Classification Metrics\n",
        "\n",
        "### 1.1 Confusion Matrix - The Foundation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "confusion-matrix",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load binary classification dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "print(\"Breast Cancer Dataset:\")\n",
        "print(f\"Samples: {len(X)}\")\n",
        "print(f\"Classes: {cancer.target_names}\")\n",
        "print(f\"Class distribution: {np.bincount(y)} ({np.bincount(y)/len(y)*100})\")\n",
        "\n",
        "# Train model\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "model = LogisticRegression(max_iter=10000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(\"=\" * 50)\n",
        "print(\"                 Predicted\")\n",
        "print(\"               Negative  Positive\")\n",
        "print(f\"Actual Negative    {cm[0,0]:4d}      {cm[0,1]:4d}    (TN, FP)\")\n",
        "print(f\"       Positive    {cm[1,0]:4d}      {cm[1,1]:4d}    (FN, TP)\")\n",
        "\n",
        "# Calculate metrics from confusion matrix\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "print(f\"\\nBreakdown:\")\n",
        "print(f\"  True Negatives (TN):  {TN} - Correctly predicted as malignant\")\n",
        "print(f\"  False Positives (FP): {FP} - Incorrectly predicted as benign\")\n",
        "print(f\"  False Negatives (FN): {FN} - Incorrectly predicted as malignant\")\n",
        "print(f\"  True Positives (TP):  {TP} - Correctly predicted as benign\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "confusion-visual",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "           xticklabels=['Malignant', 'Benign'],\n",
        "           yticklabels=['Malignant', 'Benign'])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "basic-metrics",
      "metadata": {},
      "source": [
        "### 1.2 Accuracy, Precision, Recall, F1-Score\n",
        "\n",
        "**Formulas:**\n",
        "\n",
        "\\[\n",
        "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "\\text{Recall (Sensitivity)} = \\frac{TP}{TP + FN}\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "\\]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "basic-metrics-calc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Manual calculation from confusion matrix\n",
        "accuracy_manual = (TP + TN) / (TP + TN + FP + FN)\n",
        "precision_manual = TP / (TP + FP)\n",
        "recall_manual = TP / (TP + FN)\n",
        "f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual)\n",
        "\n",
        "print(\"Classification Metrics:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Accuracy:  {accuracy:.4f} (manual: {accuracy_manual:.4f})\")\n",
        "print(f\"Precision: {precision:.4f} (manual: {precision_manual:.4f})\")\n",
        "print(f\"Recall:    {recall:.4f} (manual: {recall_manual:.4f})\")\n",
        "print(f\"F1-Score:  {f1:.4f} (manual: {f1_manual:.4f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Interpretation:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Accuracy:  {accuracy:.1%} of predictions are correct\")\n",
        "print(f\"Precision: {precision:.1%} of predicted benign are actually benign\")\n",
        "print(f\"Recall:    {recall:.1%} of actual benign cases were detected\")\n",
        "print(f\"F1-Score:  {f1:.4f} (harmonic mean of precision and recall)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "when-to-use",
      "metadata": {},
      "source": [
        "### When to Use Each Metric?\n",
        "\n",
        "| Scenario | Metric | Reason |\n",
        "|----------|--------|--------|\n",
        "| Balanced classes, equal cost | **Accuracy** | Simple, interpretable |\n",
        "| Spam detection | **Precision** | Minimize false positives (don't block important emails) |\n",
        "| Cancer screening | **Recall** | Minimize false negatives (don't miss cancer cases) |\n",
        "| General balance | **F1-Score** | Balances precision and recall |\n",
        "| Imbalanced data | **F1, Precision, Recall** | Accuracy is misleading |\n",
        "| Cost-sensitive | **Custom metric** | Weight errors differently |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "imbalanced-example",
      "metadata": {},
      "source": [
        "### 1.3 Why Accuracy Can Be Misleading (Imbalanced Data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imbalanced-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create highly imbalanced dataset (99% negative, 1% positive)\n",
        "X_imb, y_imb = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_classes=2,\n",
        "    weights=[0.99, 0.01],  # Highly imbalanced!\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Highly Imbalanced Dataset:\")\n",
        "print(f\"Class distribution: {np.bincount(y_imb)} ({np.bincount(y_imb)/len(y_imb)*100})\")\n",
        "\n",
        "# Train model\n",
        "X_tr_imb, X_te_imb, y_tr_imb, y_te_imb = train_test_split(\n",
        "    X_imb, y_imb, test_size=0.3, random_state=42, stratify=y_imb\n",
        ")\n",
        "\n",
        "model_imb = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model_imb.fit(X_tr_imb, y_tr_imb)\n",
        "y_pred_imb = model_imb.predict(X_te_imb)\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"\\nModel Performance:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Accuracy:  {accuracy_score(y_te_imb, y_pred_imb):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_te_imb, y_pred_imb, zero_division=0):.4f}\")\n",
        "print(f\"Recall:    {recall_score(y_te_imb, y_pred_imb):.4f}\")\n",
        "print(f\"F1-Score:  {f1_score(y_te_imb, y_pred_imb):.4f}\")\n",
        "\n",
        "# Dummy classifier that always predicts majority class\n",
        "y_dummy = np.zeros_like(y_te_imb)  # Always predict class 0\n",
        "\n",
        "print(\"\\nDummy Classifier (always predicts negative):\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Accuracy:  {accuracy_score(y_te_imb, y_dummy):.4f}  \u2190 High! But useless!\")\n",
        "print(f\"Precision: {precision_score(y_te_imb, y_dummy, zero_division=0):.4f}\")\n",
        "print(f\"Recall:    {recall_score(y_te_imb, y_dummy):.4f}  \u2190 Zero! Misses all positives!\")\n",
        "print(f\"F1-Score:  {f1_score(y_te_imb, y_dummy):.4f}\")\n",
        "\n",
        "print(\"\\n\u26a0\ufe0f High accuracy doesn't mean good model with imbalanced data!\")\n",
        "print(\"\u2713 Use precision, recall, and F1-score instead!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "classification-report",
      "metadata": {},
      "source": [
        "### 1.4 Classification Report - All Metrics at Once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "class-report",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full classification report\n",
        "print(\"Classification Report (Original Cancer Dataset):\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(y_test, y_pred, target_names=cancer.target_names))\n",
        "\n",
        "print(\"\\nWhat Each Column Means:\")\n",
        "print(\"  - precision: Of predictions for this class, how many were correct?\")\n",
        "print(\"  - recall: Of actual instances of this class, how many did we find?\")\n",
        "print(\"  - f1-score: Harmonic mean of precision and recall\")\n",
        "print(\"  - support: Number of actual instances in test set\")\n",
        "print(\"\\nAveraging Methods:\")\n",
        "print(\"  - macro avg: Unweighted mean (treats all classes equally)\")\n",
        "print(\"  - weighted avg: Weighted by support (accounts for imbalance)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "roc-auc",
      "metadata": {},
      "source": [
        "### 1.5 ROC Curve and AUC Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "roc-curve",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get probability predictions\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
        "print(\"\\nInterpretation:\")\n",
        "print(f\"  - AUC = {roc_auc:.2f} means {roc_auc:.1%} chance that model ranks random\")\n",
        "print(\"    positive instance higher than random negative instance\")\n",
        "print(\"\\nAUC Scale:\")\n",
        "print(\"  - 1.0 = Perfect classifier\")\n",
        "print(\"  - 0.9-1.0 = Excellent\")\n",
        "print(\"  - 0.8-0.9 = Good\")\n",
        "print(\"  - 0.7-0.8 = Fair\")\n",
        "print(\"  - 0.5 = No discrimination (random guessing)\")\n",
        "print(\"  - < 0.5 = Worse than random\")\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, 'b-', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], 'r--', lw=2, label='Random classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR / Recall)')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "precision-recall-curve",
      "metadata": {},
      "source": [
        "### 1.6 Precision-Recall Curve (Better for Imbalanced Data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pr-curve",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate PR curve\n",
        "precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_test, y_proba)\n",
        "avg_precision = average_precision_score(y_test, y_proba)\n",
        "\n",
        "print(f\"Average Precision Score: {avg_precision:.4f}\")\n",
        "print(\"\\nWhen to use PR curve vs ROC curve:\")\n",
        "print(\"  - Balanced data: ROC AUC\")\n",
        "print(\"  - Imbalanced data: PR curve (more informative)\")\n",
        "print(\"  - Care about positive class: PR curve\")\n",
        "\n",
        "# Plot PR curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall_curve, precision_curve, 'b-', lw=2, \n",
        "        label=f'PR curve (AP = {avg_precision:.3f})')\n",
        "baseline = np.sum(y_test) / len(y_test)\n",
        "plt.axhline(y=baseline, color='r', linestyle='--', lw=2, \n",
        "           label=f'Baseline (no skill) = {baseline:.3f}')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "multiclass",
      "metadata": {},
      "source": [
        "### 1.7 Multiclass Classification Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "multiclass-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load multiclass dataset\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X_iris, y_iris = iris.data, iris.target\n",
        "\n",
        "# Train model\n",
        "X_tr_iris, X_te_iris, y_tr_iris, y_te_iris = train_test_split(\n",
        "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
        ")\n",
        "\n",
        "model_iris = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model_iris.fit(X_tr_iris, y_tr_iris)\n",
        "y_pred_iris = model_iris.predict(X_te_iris)\n",
        "\n",
        "print(\"Multiclass Classification (Iris):\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Different averaging methods\n",
        "print(\"\\nPrecision Scores:\")\n",
        "print(f\"  Macro average:    {precision_score(y_te_iris, y_pred_iris, average='macro'):.4f}\")\n",
        "print(f\"  Weighted average: {precision_score(y_te_iris, y_pred_iris, average='weighted'):.4f}\")\n",
        "print(f\"  Micro average:    {precision_score(y_te_iris, y_pred_iris, average='micro'):.4f}\")\n",
        "\n",
        "print(\"\\nAveraging Methods:\")\n",
        "print(\"  - macro: Unweighted mean (all classes equal importance)\")\n",
        "print(\"  - weighted: Weighted by class frequency\")\n",
        "print(\"  - micro: Global average (same as accuracy for multiclass)\")\n",
        "\n",
        "# Full report\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(classification_report(y_te_iris, y_pred_iris, target_names=iris.target_names))\n",
        "\n",
        "# Confusion matrix for multiclass\n",
        "cm_iris = confusion_matrix(y_te_iris, y_pred_iris)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_iris, annot=True, fmt='d', cmap='Blues',\n",
        "           xticklabels=iris.target_names,\n",
        "           yticklabels=iris.target_names)\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Multiclass Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "regression-metrics",
      "metadata": {},
      "source": [
        "## Part 2: Regression Metrics\n",
        "\n",
        "### 2.1 Basic Regression Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "regression-basic",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load regression dataset\n",
        "diabetes = load_diabetes()\n",
        "X_reg, y_reg = diabetes.data, diabetes.target\n",
        "\n",
        "print(\"Diabetes Dataset (Regression):\")\n",
        "print(f\"Samples: {len(X_reg)}\")\n",
        "print(f\"Features: {X_reg.shape[1]}\")\n",
        "print(f\"Target range: [{y_reg.min():.0f}, {y_reg.max():.0f}]\")\n",
        "print(f\"Target mean: {y_reg.mean():.2f}, std: {y_reg.std():.2f}\")\n",
        "\n",
        "# Train model\n",
        "X_tr_reg, X_te_reg, y_tr_reg, y_te_reg = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "model_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model_reg.fit(X_tr_reg, y_tr_reg)\n",
        "y_pred_reg = model_reg.predict(X_te_reg)\n",
        "\n",
        "# Calculate all metrics\n",
        "mae = mean_absolute_error(y_te_reg, y_pred_reg)\n",
        "mse = mean_squared_error(y_te_reg, y_pred_reg)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_te_reg, y_pred_reg)\n",
        "mape = mean_absolute_percentage_error(y_te_reg, y_pred_reg)\n",
        "max_err = max_error(y_te_reg, y_pred_reg)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"REGRESSION METRICS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"MAE (Mean Absolute Error):          {mae:.2f}\")\n",
        "print(f\"MSE (Mean Squared Error):           {mse:.2f}\")\n",
        "print(f\"RMSE (Root Mean Squared Error):     {rmse:.2f}\")\n",
        "print(f\"R\u00b2 Score (Coefficient of Determination): {r2:.4f}\")\n",
        "print(f\"MAPE (Mean Absolute % Error):       {mape:.2%}\")\n",
        "print(f\"Max Error:                          {max_err:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "regression-formulas",
      "metadata": {},
      "source": [
        "### Regression Metric Formulas\n",
        "\n",
        "**Mean Absolute Error (MAE)**:\n",
        "\\[\n",
        "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
        "\\]\n",
        "\n",
        "**Mean Squared Error (MSE)**:\n",
        "\\[\n",
        "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "\\]\n",
        "\n",
        "**Root Mean Squared Error (RMSE)**:\n",
        "\\[\n",
        "\\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
        "\\]\n",
        "\n",
        "**R\u00b2 Score**:\n",
        "\\[\n",
        "R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
        "\\]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "regression-interpretation",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Metric Interpretations:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nMAE = {mae:.2f}\")\n",
        "print(f\"  \u2192 On average, predictions are off by {mae:.2f} units\")\n",
        "print(f\"  \u2192 {mae/y_reg.mean():.1%} relative to mean target value\")\n",
        "\n",
        "print(f\"\\nRMSE = {rmse:.2f}\")\n",
        "print(f\"  \u2192 Root mean squared error (penalizes large errors more)\")\n",
        "print(f\"  \u2192 Compare to target std ({y_reg.std():.2f}): {rmse/y_reg.std():.2f}x std\")\n",
        "\n",
        "print(f\"\\nR\u00b2 = {r2:.4f}\")\n",
        "print(f\"  \u2192 Model explains {r2:.1%} of variance in target\")\n",
        "print(f\"  \u2192 Remaining {1-r2:.1%} is unexplained variance\")\n",
        "if r2 > 0.9:\n",
        "    print(\"  \u2192 Excellent fit!\")\n",
        "elif r2 > 0.7:\n",
        "    print(\"  \u2192 Good fit\")\n",
        "elif r2 > 0.5:\n",
        "    print(\"  \u2192 Moderate fit\")\n",
        "else:\n",
        "    print(\"  \u2192 Poor fit\")\n",
        "\n",
        "print(f\"\\nMAPE = {mape:.1%}\")\n",
        "print(f\"  \u2192 On average, {mape:.1%} percentage error\")\n",
        "print(f\"  \u2192 Lower is better (but undefined if y=0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "regression-visual",
      "metadata": {},
      "source": [
        "### 2.2 Visualizing Regression Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "regression-plots",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "# 1. Predicted vs Actual\n",
        "axes[0].scatter(y_te_reg, y_pred_reg, alpha=0.5)\n",
        "axes[0].plot([y_te_reg.min(), y_te_reg.max()], \n",
        "            [y_te_reg.min(), y_te_reg.max()], \n",
        "            'r--', lw=2)\n",
        "axes[0].set_xlabel('Actual Values')\n",
        "axes[0].set_ylabel('Predicted Values')\n",
        "axes[0].set_title(f'Predictions vs Actual\\n(R\u00b2 = {r2:.3f})')\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# 2. Residuals\n",
        "residuals = y_te_reg - y_pred_reg\n",
        "axes[1].scatter(y_pred_reg, residuals, alpha=0.5)\n",
        "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "axes[1].set_xlabel('Predicted Values')\n",
        "axes[1].set_ylabel('Residuals (Actual - Predicted)')\n",
        "axes[1].set_title(f'Residual Plot\\n(MAE = {mae:.2f})')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "# 3. Residual distribution\n",
        "axes[2].hist(residuals, bins=30, edgecolor='black')\n",
        "axes[2].axvline(x=0, color='r', linestyle='--', lw=2)\n",
        "axes[2].set_xlabel('Residual Value')\n",
        "axes[2].set_ylabel('Frequency')\n",
        "axes[2].set_title(f'Residual Distribution\\n(RMSE = {rmse:.2f})')\n",
        "axes[2].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\u2713 Good residual plot should show:\")\n",
        "print(\"  - Random scatter around zero (no pattern)\")\n",
        "print(\"  - Constant variance across predictions (homoscedasticity)\")\n",
        "print(\"  - Approximately normal distribution\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "metric-comparison",
      "metadata": {},
      "source": [
        "### 2.3 Choosing the Right Regression Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "regression-comparison",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare metrics under different scenarios\n",
        "comparison = pd.DataFrame({\n",
        "    'Metric': ['MAE', 'MSE', 'RMSE', 'R\u00b2', 'MAPE'],\n",
        "    'Scale': ['Same as target', 'Squared scale', 'Same as target', 'Unitless (0-1)', 'Percentage'],\n",
        "    'Outlier Sensitivity': ['Low', 'High', 'High', 'Medium', 'Medium'],\n",
        "    'Interpretability': ['High', 'Low', 'High', 'Very High', 'High'],\n",
        "    'Best For': [\n",
        "        'Equal error importance',\n",
        "        'Penalize large errors',\n",
        "        'Interpretable + penalize large errors',\n",
        "        'Model comparison (R\u00b2>0 is better than baseline)',\n",
        "        'Percentage-based errors'\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\nRegression Metric Comparison:\")\n",
        "print(\"=\" * 90)\n",
        "print(comparison.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 90)\n",
        "print(\"When to Use Each:\")\n",
        "print(\"=\" * 90)\n",
        "recommendations = [\n",
        "    (\"MAE\", \"All errors equally important, easy interpretation\"),\n",
        "    (\"RMSE\", \"Want to penalize large errors more (default for many algorithms)\"),\n",
        "    (\"R\u00b2\", \"Compare models or assess overall fit (0=baseline, 1=perfect)\"),\n",
        "    (\"MAPE\", \"Errors should be relative to true values (not if y can be 0!)\"),\n",
        "    (\"Multiple metrics\", \"Get complete picture of model performance\")\n",
        "]\n",
        "\n",
        "for metric, usage in recommendations:\n",
        "    print(f\"  {metric:20s}: {usage}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cv-with-metrics",
      "metadata": {},
      "source": [
        "## Part 3: Using Metrics with Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cv-metrics-classification",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification with multiple metrics\n",
        "print(\"Classification Cross-Validation with Multiple Metrics:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "model_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Multiple scoring metrics\n",
        "scoring = {\n",
        "    'accuracy': 'accuracy',\n",
        "    'precision': 'precision',\n",
        "    'recall': 'recall',\n",
        "    'f1': 'f1',\n",
        "    'roc_auc': 'roc_auc'\n",
        "}\n",
        "\n",
        "cv_results_clf = cross_validate(\n",
        "    model_clf, X, y,\n",
        "    cv=5,\n",
        "    scoring=scoring,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Display results\n",
        "for metric in scoring.keys():\n",
        "    test_scores = cv_results_clf[f'test_{metric}']\n",
        "    print(f\"\\n{metric.upper()}:\")\n",
        "    print(f\"  Test:  {test_scores.mean():.4f} \u00b1 {test_scores.std():.4f}\")\n",
        "    if f'train_{metric}' in cv_results_clf:\n",
        "        train_scores = cv_results_clf[f'train_{metric}']\n",
        "        print(f\"  Train: {train_scores.mean():.4f} \u00b1 {train_scores.std():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cv-metrics-regression",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Regression with multiple metrics\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Regression Cross-Validation with Multiple Metrics:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "model_regr = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Multiple regression metrics\n",
        "scoring_reg = {\n",
        "    'r2': 'r2',\n",
        "    'neg_mae': 'neg_mean_absolute_error',\n",
        "    'neg_mse': 'neg_mean_squared_error',\n",
        "    'neg_rmse': 'neg_root_mean_squared_error'\n",
        "}\n",
        "\n",
        "cv_results_reg = cross_validate(\n",
        "    model_regr, X_reg, y_reg,\n",
        "    cv=5,\n",
        "    scoring=scoring_reg\n",
        ")\n",
        "\n",
        "# Display results (convert negative scores back to positive)\n",
        "print(f\"\\nR\u00b2 Score: {cv_results_reg['test_r2'].mean():.4f} \u00b1 {cv_results_reg['test_r2'].std():.4f}\")\n",
        "print(f\"MAE:      {-cv_results_reg['test_neg_mae'].mean():.2f} \u00b1 {cv_results_reg['test_neg_mae'].std():.2f}\")\n",
        "print(f\"MSE:      {-cv_results_reg['test_neg_mse'].mean():.2f} \u00b1 {cv_results_reg['test_neg_mse'].std():.2f}\")\n",
        "print(f\"RMSE:     {-cv_results_reg['test_neg_rmse'].mean():.2f} \u00b1 {cv_results_reg['test_neg_rmse'].std():.2f}\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Note: Sklearn uses 'neg_*' for error metrics (higher is better)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "custom-scoring",
      "metadata": {},
      "source": [
        "## Part 4: Custom Scoring Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "custom-scorer",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "# Example: Custom cost-sensitive metric\n",
        "# False negatives cost 10x more than false positives\n",
        "def custom_cost_metric(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    TN, FP, FN, TP = cm.ravel()\n",
        "    \n",
        "    # Cost: FP costs 1, FN costs 10\n",
        "    cost = FP * 1 + FN * 10\n",
        "    \n",
        "    # Return negative (sklearn maximizes, we want to minimize cost)\n",
        "    return -cost\n",
        "\n",
        "# Create scorer\n",
        "cost_scorer = make_scorer(custom_cost_metric)\n",
        "\n",
        "# Use in cross-validation\n",
        "scores = cross_val_score(model, X, y, cv=5, scoring=cost_scorer)\n",
        "\n",
        "print(\"Custom Cost-Sensitive Scoring:\")\n",
        "print(f\"Mean cost: {-scores.mean():.2f} (lower is better)\")\n",
        "print(f\"Std: {scores.std():.2f}\")\n",
        "print(\"\\n\ud83d\udca1 Use custom scorers for domain-specific cost functions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Summary and Best Practices\n",
        "\n",
        "### Classification Metrics Quick Guide\n",
        "\n",
        "```python\n",
        "# Balanced classes\n",
        "scoring='accuracy'\n",
        "\n",
        "# Imbalanced classes\n",
        "scoring=['precision', 'recall', 'f1', 'roc_auc']\n",
        "\n",
        "# Minimize false positives (e.g., spam)\n",
        "scoring='precision'\n",
        "\n",
        "# Minimize false negatives (e.g., disease)\n",
        "scoring='recall'\n",
        "\n",
        "# Balance precision and recall\n",
        "scoring='f1'\n",
        "\n",
        "# Probability-based ranking\n",
        "scoring='roc_auc'\n",
        "```\n",
        "\n",
        "### Regression Metrics Quick Guide\n",
        "\n",
        "```python\n",
        "# General purpose\n",
        "scoring='r2'  # or 'neg_root_mean_squared_error'\n",
        "\n",
        "# Interpretable error\n",
        "scoring='neg_mean_absolute_error'\n",
        "\n",
        "# Penalize large errors\n",
        "scoring='neg_mean_squared_error'\n",
        "\n",
        "# Percentage-based\n",
        "scoring='neg_mean_absolute_percentage_error'\n",
        "```\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Never rely on accuracy alone** - especially with imbalanced data\n",
        "2. **Use multiple metrics** - get a complete picture\n",
        "3. **Choose metrics based on problem** - cost of errors matters\n",
        "4. **Report confidence intervals** - mean \u00b1 std from CV\n",
        "5. **Visualize performance** - confusion matrix, ROC, residual plots\n",
        "6. **For imbalanced classification**: Use precision, recall, F1, PR curve\n",
        "7. **For regression**: Use RMSE for interpretability, R\u00b2 for comparison\n",
        "8. **Custom metrics**: Use make_scorer for domain-specific requirements\n",
        "\n",
        "### Available Scoring Strings\n",
        "\n",
        "**Classification**: 'accuracy', 'balanced_accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'average_precision'\n",
        "\n",
        "**Regression**: 'r2', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_root_mean_squared_error', 'neg_mean_absolute_percentage_error'\n",
        "\n",
        "**Multiclass/Multilabel**: Add '_macro', '_micro', '_weighted' suffixes (e.g., 'f1_macro')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}