{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Train-Test Split and Cross-Validation Basics\n",
        "\n",
        "## Overview\n",
        "\n",
        "**Why Split Data?**\n",
        "- Training on entire dataset \u2192 Can't estimate real-world performance\n",
        "- Model might just memorize training data (overfitting)\n",
        "- Need **unseen data** to evaluate generalization\n",
        "\n",
        "**The Golden Rule**: Never test on training data!\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "### 1. Train-Test Split\n",
        "- Simple split: 70-80% train, 20-30% test\n",
        "- Fast and straightforward\n",
        "- \u26a0\ufe0f High variance (depends on random split)\n",
        "\n",
        "### 2. Cross-Validation (CV)\n",
        "- Split data into K folds\n",
        "- Train on K-1 folds, test on remaining fold\n",
        "- Repeat K times, average results\n",
        "- \u2713 More reliable performance estimate\n",
        "- \u2713 Uses all data for both training and testing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_wine, load_diabetes, load_breast_cancer\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, \n",
        "    cross_val_score, \n",
        "    cross_validate,\n",
        "    KFold\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\u2713 Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "train-test-split",
      "metadata": {},
      "source": [
        "## 1. Train-Test Split Basics\n",
        "\n",
        "### Simple Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "basic-split",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "print(\"Dataset Info:\")\n",
        "print(f\"Total samples: {len(X)}\")\n",
        "print(f\"Features: {X.shape[1]}\")\n",
        "print(f\"Classes: {iris.target_names}\")\n",
        "print(f\"Class distribution: {np.bincount(y)}\")\n",
        "\n",
        "# Basic split (80-20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.2,      # 20% for testing\n",
        "    random_state=42     # For reproducibility\n",
        ")\n",
        "\n",
        "print(f\"\\nAfter Split:\")\n",
        "print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X):.1%})\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X):.1%})\")\n",
        "print(f\"\\nTrain class distribution: {np.bincount(y_train)}\")\n",
        "print(f\"Test class distribution: {np.bincount(y_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "stratified-split",
      "metadata": {},
      "source": [
        "### Stratified Split (For Imbalanced Data)\n",
        "\n",
        "**Problem**: Random split might create unbalanced train/test sets\n",
        "\n",
        "**Solution**: `stratify` parameter maintains class distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "stratified",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create imbalanced dataset\n",
        "X_imbalanced = X[y != 2]  # Remove class 2\n",
        "y_imbalanced = y[y != 2]\n",
        "\n",
        "print(\"Imbalanced Dataset:\")\n",
        "print(f\"Class distribution: {np.bincount(y_imbalanced)}\")\n",
        "print(f\"Class ratio: {np.bincount(y_imbalanced)[0]/len(y_imbalanced):.1%} vs {np.bincount(y_imbalanced)[1]/len(y_imbalanced):.1%}\")\n",
        "\n",
        "# Split WITHOUT stratification\n",
        "X_tr1, X_te1, y_tr1, y_te1 = train_test_split(\n",
        "    X_imbalanced, y_imbalanced, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nWithout Stratification:\")\n",
        "print(f\"Train: {np.bincount(y_tr1)} \u2192 {np.bincount(y_tr1)[0]/len(y_tr1):.1%} vs {np.bincount(y_tr1)[1]/len(y_tr1):.1%}\")\n",
        "print(f\"Test:  {np.bincount(y_te1)} \u2192 {np.bincount(y_te1)[0]/len(y_te1):.1%} vs {np.bincount(y_te1)[1]/len(y_te1):.1%}\")\n",
        "\n",
        "# Split WITH stratification\n",
        "X_tr2, X_te2, y_tr2, y_te2 = train_test_split(\n",
        "    X_imbalanced, y_imbalanced, \n",
        "    test_size=0.3, \n",
        "    random_state=42,\n",
        "    stratify=y_imbalanced  # Maintain class distribution\n",
        ")\n",
        "\n",
        "print(f\"\\nWith Stratification:\")\n",
        "print(f\"Train: {np.bincount(y_tr2)} \u2192 {np.bincount(y_tr2)[0]/len(y_tr2):.1%} vs {np.bincount(y_tr2)[1]/len(y_tr2):.1%}\")\n",
        "print(f\"Test:  {np.bincount(y_te2)} \u2192 {np.bincount(y_te2)[0]/len(y_te2):.1%} vs {np.bincount(y_te2)[1]/len(y_te2):.1%}\")\n",
        "\n",
        "print(\"\\n\u2713 Stratification preserves class distribution!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "train-test-demo",
      "metadata": {},
      "source": [
        "### Training and Evaluating a Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "model-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model on training set\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on both sets\n",
        "train_score = model.score(X_train, y_train)\n",
        "test_score = model.score(X_test, y_test)\n",
        "\n",
        "print(\"Model Performance:\")\n",
        "print(f\"Training accuracy: {train_score:.4f}\")\n",
        "print(f\"Test accuracy: {test_score:.4f}\")\n",
        "print(f\"\\nDifference: {abs(train_score - test_score):.4f}\")\n",
        "\n",
        "if train_score - test_score > 0.1:\n",
        "    print(\"\u26a0\ufe0f Warning: Possible overfitting (train >> test)\")\n",
        "elif test_score > train_score:\n",
        "    print(\"\u2713 Good generalization (test \u2265 train)\")\n",
        "else:\n",
        "    print(\"\u2713 Model generalizes well\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "split-variance",
      "metadata": {},
      "source": [
        "### Problem with Single Train-Test Split\n",
        "\n",
        "Performance varies with different random splits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "split-variance-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with different random states\n",
        "test_scores = []\n",
        "\n",
        "for seed in range(10):\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=seed\n",
        "    )\n",
        "    \n",
        "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    model.fit(X_tr, y_tr)\n",
        "    score = model.score(X_te, y_te)\n",
        "    test_scores.append(score)\n",
        "\n",
        "print(\"Test Accuracy Across Different Splits:\")\n",
        "print(f\"Scores: {[f'{s:.3f}' for s in test_scores]}\")\n",
        "print(f\"\\nMean: {np.mean(test_scores):.4f}\")\n",
        "print(f\"Std:  {np.std(test_scores):.4f}\")\n",
        "print(f\"Range: [{np.min(test_scores):.4f}, {np.max(test_scores):.4f}]\")\n",
        "\n",
        "print(\"\\n\u26a0\ufe0f High variance! Need more reliable evaluation method...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cross-validation",
      "metadata": {},
      "source": [
        "## 2. Cross-Validation: More Reliable Evaluation\n",
        "\n",
        "### How K-Fold CV Works\n",
        "\n",
        "```\n",
        "Fold 1: [Test] [Train] [Train] [Train] [Train]\n",
        "Fold 2: [Train] [Test] [Train] [Train] [Train]\n",
        "Fold 3: [Train] [Train] [Test] [Train] [Train]\n",
        "Fold 4: [Train] [Train] [Train] [Test] [Train]\n",
        "Fold 5: [Train] [Train] [Train] [Train] [Test]\n",
        "\n",
        "Final Score = Average of all fold scores\n",
        "```\n",
        "\n",
        "### Using cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cv-basic",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform 5-fold cross-validation\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "cv_scores = cross_val_score(\n",
        "    model, X, y, \n",
        "    cv=5,              # 5 folds\n",
        "    scoring='accuracy' # Metric to compute\n",
        ")\n",
        "\n",
        "print(\"5-Fold Cross-Validation Results:\")\n",
        "print(f\"Fold scores: {cv_scores}\")\n",
        "print(f\"\\nMean accuracy: {cv_scores.mean():.4f}\")\n",
        "print(f\"Std deviation: {cv_scores.std():.4f}\")\n",
        "print(f\"95% confidence interval: [{cv_scores.mean() - 1.96*cv_scores.std():.4f}, {cv_scores.mean() + 1.96*cv_scores.std():.4f}]\")\n",
        "\n",
        "# Compare with single split\n",
        "print(f\"\\nComparison:\")\n",
        "print(f\"Single split std: {np.std(test_scores):.4f}\")\n",
        "print(f\"CV std: {cv_scores.std():.4f}\")\n",
        "print(f\"\\n\u2713 Cross-validation provides more stable estimates!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cv-different-k",
      "metadata": {},
      "source": [
        "### Effect of Different K Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cv-k-values",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different K values\n",
        "k_values = [3, 5, 10, 15]\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "print(\"Cross-Validation with Different K:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for k in k_values:\n",
        "    scores = cross_val_score(model, X, y, cv=k)\n",
        "    print(f\"\\nK={k:2d}: Mean={scores.mean():.4f}, Std={scores.std():.4f}\")\n",
        "    print(f\"      Training size per fold: {len(X) * (k-1) / k:.0f} samples\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Trade-off:\")\n",
        "print(\"  - Higher K \u2192 More training data per fold, but more computation\")\n",
        "print(\"  - Lower K \u2192 Faster, but less stable estimates\")\n",
        "print(\"  - K=5 or K=10 are common choices\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cross-validate",
      "metadata": {},
      "source": [
        "## 3. cross_validate: More Information\n",
        "\n",
        "`cross_validate` returns more details than `cross_val_score`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cross-validate-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Get detailed CV results\n",
        "cv_results = cross_validate(\n",
        "    model, X, y,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    return_train_score=True,  # Also return training scores\n",
        "    return_estimator=False     # Don't return fitted models (saves memory)\n",
        ")\n",
        "\n",
        "print(\"Detailed Cross-Validation Results:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"\\nTest scores:  {cv_results['test_score']}\")\n",
        "print(f\"Train scores: {cv_results['train_score']}\")\n",
        "print(f\"\\nFit time (s): {cv_results['fit_time']}\")\n",
        "print(f\"Score time (s): {cv_results['score_time']}\")\n",
        "\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"  Test accuracy:  {cv_results['test_score'].mean():.4f} \u00b1 {cv_results['test_score'].std():.4f}\")\n",
        "print(f\"  Train accuracy: {cv_results['train_score'].mean():.4f} \u00b1 {cv_results['train_score'].std():.4f}\")\n",
        "print(f\"  Average fit time: {cv_results['fit_time'].mean():.4f}s\")\n",
        "\n",
        "# Check for overfitting\n",
        "train_mean = cv_results['train_score'].mean()\n",
        "test_mean = cv_results['test_score'].mean()\n",
        "if train_mean - test_mean > 0.1:\n",
        "    print(\"\\n\u26a0\ufe0f Warning: Possible overfitting\")\n",
        "else:\n",
        "    print(\"\\n\u2713 Model generalizes well\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "multiple-metrics",
      "metadata": {},
      "source": [
        "### Multiple Metrics at Once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "multiple-metrics-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate multiple metrics\n",
        "scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
        "\n",
        "cv_results = cross_validate(\n",
        "    model, X, y,\n",
        "    cv=5,\n",
        "    scoring=scoring\n",
        ")\n",
        "\n",
        "print(\"Multiple Metrics Cross-Validation:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for metric in scoring:\n",
        "    scores = cv_results[f'test_{metric}']\n",
        "    print(f\"\\n{metric.upper()}:\")\n",
        "    print(f\"  Mean: {scores.mean():.4f}\")\n",
        "    print(f\"  Std:  {scores.std():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "real-example",
      "metadata": {},
      "source": [
        "## 4. Real-World Example: Wine Quality Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wine-example",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X_wine, y_wine = wine.data, wine.target\n",
        "\n",
        "print(\"Wine Dataset:\")\n",
        "print(f\"Samples: {len(X_wine)}\")\n",
        "print(f\"Features: {X_wine.shape[1]}\")\n",
        "print(f\"Classes: {wine.target_names}\")\n",
        "\n",
        "# Compare models with train-test split\n",
        "X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(\n",
        "    X_wine, y_wine, test_size=0.3, random_state=42, stratify=y_wine\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_w_scaled = scaler.fit_transform(X_train_w)\n",
        "X_test_w_scaled = scaler.transform(X_test_w)\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"METHOD 1: Single Train-Test Split\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "split_results = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_w_scaled, y_train_w)\n",
        "    test_score = model.score(X_test_w_scaled, y_test_w)\n",
        "    split_results[name] = test_score\n",
        "    print(f\"{name:25s}: {test_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wine-cv",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare with cross-validation\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"METHOD 2: 5-Fold Cross-Validation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "cv_results = {}\n",
        "for name, model in models.items():\n",
        "    # Create pipeline with scaling\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('model', model)\n",
        "    ])\n",
        "    \n",
        "    scores = cross_val_score(pipeline, X_wine, y_wine, cv=5)\n",
        "    cv_results[name] = scores\n",
        "    print(f\"{name:25s}: {scores.mean():.4f} \u00b1 {scores.std():.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "for name in models.keys():\n",
        "    print(f\"{name}:\")\n",
        "    print(f\"  Single split: {split_results[name]:.4f}\")\n",
        "    print(f\"  CV mean:      {cv_results[name].mean():.4f} \u00b1 {cv_results[name].std():.4f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "regression-cv",
      "metadata": {},
      "source": [
        "## 5. Cross-Validation for Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "regression-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Diabetes dataset (regression)\n",
        "diabetes = load_diabetes()\n",
        "X_diab, y_diab = diabetes.data, diabetes.target\n",
        "\n",
        "print(\"Diabetes Dataset (Regression):\")\n",
        "print(f\"Samples: {len(X_diab)}\")\n",
        "print(f\"Features: {X_diab.shape[1]}\")\n",
        "print(f\"Target range: [{y_diab.min():.0f}, {y_diab.max():.0f}]\")\n",
        "\n",
        "# Compare regression models\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "reg_models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Ridge': Ridge(alpha=1.0),\n",
        "    'Lasso': Lasso(alpha=1.0),\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"REGRESSION CROSS-VALIDATION (R\u00b2 Score)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for name, model in reg_models.items():\n",
        "    # Use negative MSE and R\u00b2 scoring\n",
        "    r2_scores = cross_val_score(model, X_diab, y_diab, cv=5, scoring='r2')\n",
        "    neg_mse_scores = cross_val_score(model, X_diab, y_diab, cv=5, scoring='neg_mean_squared_error')\n",
        "    rmse_scores = np.sqrt(-neg_mse_scores)\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  R\u00b2 Score:  {r2_scores.mean():.4f} \u00b1 {r2_scores.std():.4f}\")\n",
        "    print(f\"  RMSE:      {rmse_scores.mean():.2f} \u00b1 {rmse_scores.std():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cv-tips",
      "metadata": {},
      "source": [
        "## 6. Cross-Validation Best Practices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "best-practices",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Cross-Validation Best Practices:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "practices = [\n",
        "    (\"1. Always use stratified splits for classification\",\n",
        "     \"Use: cv=StratifiedKFold() or stratify=y\"),\n",
        "    \n",
        "    (\"2. Include preprocessing in pipeline\",\n",
        "     \"Prevents data leakage from scaling/imputation\"),\n",
        "    \n",
        "    (\"3. Set random_state for reproducibility\",\n",
        "     \"KFold(n_splits=5, shuffle=True, random_state=42)\"),\n",
        "    \n",
        "    (\"4. Choose K based on dataset size\",\n",
        "     \"Small: K=5, Medium: K=10, Large: K=3\"),\n",
        "    \n",
        "    (\"5. Report mean AND std deviation\",\n",
        "     \"Shows both performance and stability\"),\n",
        "    \n",
        "    (\"6. Use cross_validate for detailed info\",\n",
        "     \"Get train scores, timing, multiple metrics\"),\n",
        "    \n",
        "    (\"7. For time series, use TimeSeriesSplit\",\n",
        "     \"Respects temporal ordering\"),\n",
        "    \n",
        "    (\"8. For small datasets, consider LOOCV\",\n",
        "     \"Leave-One-Out: K=n (expensive but thorough)\")\n",
        "]\n",
        "\n",
        "for i, (practice, tip) in enumerate(practices, 1):\n",
        "    print(f\"\\n{practice}\")\n",
        "    print(f\"   \u2192 {tip}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "when-to-use",
      "metadata": {},
      "source": [
        "## 7. When to Use What?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decision-guide",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create decision guide\n",
        "guide = pd.DataFrame({\n",
        "    'Scenario': [\n",
        "        'Large dataset (>100k samples)',\n",
        "        'Small dataset (<1000 samples)',\n",
        "        'Very small dataset (<100 samples)',\n",
        "        'Imbalanced classes',\n",
        "        'Time series data',\n",
        "        'Quick experimentation',\n",
        "        'Final model evaluation',\n",
        "        'Hyperparameter tuning'\n",
        "    ],\n",
        "    'Recommended Method': [\n",
        "        'Single train-test split (80-20)',\n",
        "        '5-fold or 10-fold CV',\n",
        "        'Leave-One-Out CV (LOOCV)',\n",
        "        'Stratified K-Fold CV',\n",
        "        'TimeSeriesSplit CV',\n",
        "        'Single split or 3-fold CV',\n",
        "        '5-fold or 10-fold CV',\n",
        "        'Nested CV or CV with GridSearch'\n",
        "    ],\n",
        "    'K Value': [\n",
        "        'N/A',\n",
        "        '5-10',\n",
        "        'n (LOOCV)',\n",
        "        '5-10',\n",
        "        '5',\n",
        "        'N/A or 3',\n",
        "        '5-10',\n",
        "        '3-5 (outer)'\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\nDecision Guide: Which Method to Use?\")\n",
        "print(\"=\" * 80)\n",
        "print(guide.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### Train-Test Split\n",
        "- \u2713 Fast and simple\n",
        "- \u2713 Good for large datasets\n",
        "- \u2717 High variance (depends on split)\n",
        "- \u2717 Wastes some data (test set not used for training)\n",
        "\n",
        "### Cross-Validation\n",
        "- \u2713 More reliable performance estimate\n",
        "- \u2713 Uses all data for both training and testing\n",
        "- \u2713 Lower variance\n",
        "- \u2717 K times slower than single split\n",
        "- \u2717 Not suitable for very large datasets\n",
        "\n",
        "### Critical Points\n",
        "\n",
        "1. **Never test on training data!**\n",
        "2. **Use stratification for imbalanced data**\n",
        "3. **Include preprocessing in pipeline** (prevents data leakage)\n",
        "4. **Report mean \u00b1 std** (not just mean)\n",
        "5. **Choose K wisely**: 5-10 for most cases\n",
        "6. **Set random_state** for reproducibility\n",
        "\n",
        "### Common Workflows\n",
        "\n",
        "```python\n",
        "# Quick Experiment\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_test, y_test)\n",
        "\n",
        "# Proper Evaluation\n",
        "pipeline = Pipeline([('scaler', StandardScaler()), ('model', model)])\n",
        "scores = cross_val_score(pipeline, X, y, cv=5)\n",
        "print(f\"{scores.mean():.3f} \u00b1 {scores.std():.3f}\")\n",
        "\n",
        "# Detailed Analysis\n",
        "cv_results = cross_validate(\n",
        "    pipeline, X, y, cv=5,\n",
        "    scoring=['accuracy', 'precision', 'recall'],\n",
        "    return_train_score=True\n",
        ")\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}