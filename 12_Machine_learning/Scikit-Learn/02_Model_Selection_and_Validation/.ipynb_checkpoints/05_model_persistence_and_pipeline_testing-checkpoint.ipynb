{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Model Persistence and Pipeline Testing\n",
        "\n",
        "## Overview\n",
        "\n",
        "**Why Model Persistence?**\n",
        "- Training can be time-consuming\n",
        "- Reuse models in production\n",
        "- Share models with team members\n",
        "- Version control for models\n",
        "\n",
        "## Topics Covered\n",
        "\n",
        "### Part 1: Saving and Loading Models\n",
        "- **joblib**: Preferred for sklearn (efficient for large numpy arrays)\n",
        "- **pickle**: Python's built-in serialization\n",
        "- **Comparison**: When to use which\n",
        "\n",
        "### Part 2: Pipeline Persistence\n",
        "- Saving complete pipelines (preprocessing + model)\n",
        "- Ensuring reproducibility\n",
        "- Version management\n",
        "\n",
        "### Part 3: Testing Strategies\n",
        "- Validating loaded models\n",
        "- Testing pipeline integrity\n",
        "- Production readiness checks\n",
        "- Model versioning and monitoring"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import joblib\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from sklearn.datasets import load_iris, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create models directory\n",
        "os.makedirs('models', exist_ok=True)\n",
        "print(\"\u2713 Libraries imported successfully\")\n",
        "print(\"\u2713 Models directory created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "basic-persistence",
      "metadata": {},
      "source": [
        "## Part 1: Basic Model Persistence\n",
        "\n",
        "### 1.1 Saving with joblib (Recommended)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "joblib-save",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train a model\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "train_score = model.score(X_train, y_train)\n",
        "test_score = model.score(X_test, y_test)\n",
        "\n",
        "print(\"Original Model:\")\n",
        "print(f\"  Train Accuracy: {train_score:.4f}\")\n",
        "print(f\"  Test Accuracy:  {test_score:.4f}\")\n",
        "\n",
        "# Save with joblib\n",
        "model_path = 'models/iris_rf_model.joblib'\n",
        "joblib.dump(model, model_path)\n",
        "print(f\"\\n\u2713 Model saved to: {model_path}\")\n",
        "\n",
        "# Check file size\n",
        "file_size = os.path.getsize(model_path) / 1024  # KB\n",
        "print(f\"  File size: {file_size:.2f} KB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "joblib-load",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model\n",
        "loaded_model = joblib.load(model_path)\n",
        "\n",
        "print(\"Loaded Model:\")\n",
        "print(f\"  Type: {type(loaded_model)}\")\n",
        "print(f\"  Parameters: {loaded_model.get_params()}\")\n",
        "\n",
        "# Verify it works\n",
        "loaded_train_score = loaded_model.score(X_train, y_train)\n",
        "loaded_test_score = loaded_model.score(X_test, y_test)\n",
        "\n",
        "print(f\"\\nLoaded Model Performance:\")\n",
        "print(f\"  Train Accuracy: {loaded_train_score:.4f}\")\n",
        "print(f\"  Test Accuracy:  {loaded_test_score:.4f}\")\n",
        "\n",
        "# Verify exact match\n",
        "assert train_score == loaded_train_score, \"Train scores don't match!\"\n",
        "assert test_score == loaded_test_score, \"Test scores don't match!\"\n",
        "print(\"\\n\u2713 Model loaded successfully and produces identical results!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pickle-persistence",
      "metadata": {},
      "source": [
        "### 1.2 Saving with pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pickle-save-load",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save with pickle\n",
        "pickle_path = 'models/iris_rf_model.pkl'\n",
        "\n",
        "with open(pickle_path, 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "print(f\"\u2713 Model saved with pickle to: {pickle_path}\")\n",
        "print(f\"  File size: {os.path.getsize(pickle_path) / 1024:.2f} KB\")\n",
        "\n",
        "# Load with pickle\n",
        "with open(pickle_path, 'rb') as f:\n",
        "    pickle_model = pickle.load(f)\n",
        "\n",
        "# Verify\n",
        "pickle_test_score = pickle_model.score(X_test, y_test)\n",
        "print(f\"\\nPickle Model Test Accuracy: {pickle_test_score:.4f}\")\n",
        "print(\"\u2713 Model loaded from pickle successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comparison",
      "metadata": {},
      "source": [
        "### 1.3 joblib vs pickle Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comparison-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Train larger model for comparison\n",
        "cancer = load_breast_cancer()\n",
        "X_cancer, y_cancer = cancer.data, cancer.target\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X_cancer, y_cancer, test_size=0.2, random_state=42)\n",
        "\n",
        "large_model = RandomForestClassifier(n_estimators=500, random_state=42)\n",
        "large_model.fit(X_tr, y_tr)\n",
        "\n",
        "print(\"Comparing joblib vs pickle for larger model:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# joblib timing\n",
        "start = time.time()\n",
        "joblib.dump(large_model, 'models/large_model.joblib')\n",
        "joblib_save_time = time.time() - start\n",
        "\n",
        "start = time.time()\n",
        "joblib.load('models/large_model.joblib')\n",
        "joblib_load_time = time.time() - start\n",
        "\n",
        "joblib_size = os.path.getsize('models/large_model.joblib') / (1024 * 1024)  # MB\n",
        "\n",
        "# pickle timing\n",
        "start = time.time()\n",
        "with open('models/large_model.pkl', 'wb') as f:\n",
        "    pickle.dump(large_model, f)\n",
        "pickle_save_time = time.time() - start\n",
        "\n",
        "start = time.time()\n",
        "with open('models/large_model.pkl', 'rb') as f:\n",
        "    pickle.load(f)\n",
        "pickle_load_time = time.time() - start\n",
        "\n",
        "pickle_size = os.path.getsize('models/large_model.pkl') / (1024 * 1024)  # MB\n",
        "\n",
        "# Comparison table\n",
        "comparison = pd.DataFrame({\n",
        "    'Method': ['joblib', 'pickle'],\n",
        "    'Save Time (s)': [joblib_save_time, pickle_save_time],\n",
        "    'Load Time (s)': [joblib_load_time, pickle_load_time],\n",
        "    'File Size (MB)': [joblib_size, pickle_size]\n",
        "})\n",
        "\n",
        "print(comparison.to_string(index=False))\n",
        "print(\"\\n\ud83d\udca1 joblib is usually faster and more efficient for sklearn models!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pipeline-persistence",
      "metadata": {},
      "source": [
        "## Part 2: Pipeline Persistence\n",
        "\n",
        "### 2.1 Saving Complete Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pipeline-save",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and train pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
        "])\n",
        "\n",
        "print(\"Pipeline Structure:\")\n",
        "print(pipeline)\n",
        "\n",
        "# Fit pipeline\n",
        "pipeline.fit(X_train, y_train)\n",
        "pipeline_score = pipeline.score(X_test, y_test)\n",
        "\n",
        "print(f\"\\nPipeline Test Accuracy: {pipeline_score:.4f}\")\n",
        "\n",
        "# Save pipeline\n",
        "pipeline_path = 'models/iris_pipeline.joblib'\n",
        "joblib.dump(pipeline, pipeline_path)\n",
        "print(f\"\\n\u2713 Pipeline saved to: {pipeline_path}\")\n",
        "\n",
        "# What gets saved?\n",
        "print(\"\\nWhat's saved in the pipeline:\")\n",
        "print(\"  1. StandardScaler with fitted mean and std\")\n",
        "print(\"  2. LogisticRegression with trained coefficients\")\n",
        "print(\"  3. All hyperparameters\")\n",
        "print(\"  4. Complete preprocessing logic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pipeline-load",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pipeline\n",
        "loaded_pipeline = joblib.load(pipeline_path)\n",
        "\n",
        "print(\"Loaded Pipeline:\")\n",
        "print(loaded_pipeline)\n",
        "\n",
        "# Use on new data (simulated)\n",
        "new_data = np.array([[5.1, 3.5, 1.4, 0.2]])  # Single sample\n",
        "prediction = loaded_pipeline.predict(new_data)\n",
        "probabilities = loaded_pipeline.predict_proba(new_data)\n",
        "\n",
        "print(f\"\\nPrediction on new data:\")\n",
        "print(f\"  Features: {new_data[0]}\")\n",
        "print(f\"  Predicted class: {prediction[0]} ({iris.target_names[prediction[0]]})\")\n",
        "print(f\"  Probabilities: {probabilities[0]}\")\n",
        "\n",
        "# Verify pipeline consistency\n",
        "loaded_score = loaded_pipeline.score(X_test, y_test)\n",
        "assert pipeline_score == loaded_score, \"Pipeline scores don't match!\"\n",
        "print(f\"\\n\u2713 Loaded pipeline produces identical results: {loaded_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "metadata-versioning",
      "metadata": {},
      "source": [
        "### 2.2 Model Metadata and Versioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "metadata-system",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive model metadata\n",
        "def save_model_with_metadata(model, X_train, X_test, y_train, y_test, \n",
        "                            model_name, version, description):\n",
        "    \"\"\"\n",
        "    Save model with complete metadata for production use\n",
        "    \"\"\"\n",
        "    # Create version directory\n",
        "    model_dir = f'models/{model_name}/v{version}'\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    \n",
        "    # Save model\n",
        "    model_path = f'{model_dir}/model.joblib'\n",
        "    joblib.dump(model, model_path)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    train_score = model.score(X_train, y_train)\n",
        "    test_score = model.score(X_test, y_test)\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    # Create metadata\n",
        "    metadata = {\n",
        "        'model_name': model_name,\n",
        "        'version': version,\n",
        "        'description': description,\n",
        "        'created_at': datetime.now().isoformat(),\n",
        "        'model_type': type(model).__name__,\n",
        "        'parameters': model.get_params() if hasattr(model, 'get_params') else {},\n",
        "        'training_data': {\n",
        "            'n_train_samples': len(X_train),\n",
        "            'n_test_samples': len(X_test),\n",
        "            'n_features': X_train.shape[1],\n",
        "            'n_classes': len(np.unique(y_train))\n",
        "        },\n",
        "        'performance': {\n",
        "            'train_accuracy': float(train_score),\n",
        "            'test_accuracy': float(test_score),\n",
        "            'confusion_matrix': confusion_matrix(y_test, y_pred).tolist()\n",
        "        },\n",
        "        'sklearn_version': '1.0+',  # In real code, import sklearn.__version__\n",
        "        'python_version': '3.9+'\n",
        "    }\n",
        "    \n",
        "    # Save metadata as JSON\n",
        "    metadata_path = f'{model_dir}/metadata.json'\n",
        "    with open(metadata_path, 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    \n",
        "    # Save sample predictions for validation\n",
        "    sample_data = {\n",
        "        'input': X_test[:5].tolist(),\n",
        "        'expected_output': y_test[:5].tolist(),\n",
        "        'predictions': model.predict(X_test[:5]).tolist()\n",
        "    }\n",
        "    \n",
        "    sample_path = f'{model_dir}/sample_predictions.json'\n",
        "    with open(sample_path, 'w') as f:\n",
        "        json.dump(sample_data, f, indent=2)\n",
        "    \n",
        "    print(f\"\u2713 Model saved with metadata:\")\n",
        "    print(f\"  Model: {model_path}\")\n",
        "    print(f\"  Metadata: {metadata_path}\")\n",
        "    print(f\"  Samples: {sample_path}\")\n",
        "    \n",
        "    return model_dir\n",
        "\n",
        "# Save model with metadata\n",
        "model_dir = save_model_with_metadata(\n",
        "    pipeline,\n",
        "    X_train, X_test, y_train, y_test,\n",
        "    model_name='iris_classifier',\n",
        "    version='1.0',\n",
        "    description='Logistic regression with standard scaling for Iris classification'\n",
        ")\n",
        "\n",
        "print(f\"\\n\u2713 Complete model package created at: {model_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-with-metadata",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model with metadata\n",
        "def load_model_with_metadata(model_dir):\n",
        "    \"\"\"\n",
        "    Load model and its metadata\n",
        "    \"\"\"\n",
        "    # Load model\n",
        "    model_path = f'{model_dir}/model.joblib'\n",
        "    model = joblib.load(model_path)\n",
        "    \n",
        "    # Load metadata\n",
        "    metadata_path = f'{model_dir}/metadata.json'\n",
        "    with open(metadata_path, 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "    \n",
        "    # Load sample predictions\n",
        "    sample_path = f'{model_dir}/sample_predictions.json'\n",
        "    with open(sample_path, 'r') as f:\n",
        "        samples = json.load(f)\n",
        "    \n",
        "    return model, metadata, samples\n",
        "\n",
        "# Load and display\n",
        "loaded_model, metadata, samples = load_model_with_metadata(model_dir)\n",
        "\n",
        "print(\"Loaded Model Information:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Model: {metadata['model_name']} v{metadata['version']}\")\n",
        "print(f\"Type: {metadata['model_type']}\")\n",
        "print(f\"Created: {metadata['created_at']}\")\n",
        "print(f\"Description: {metadata['description']}\")\n",
        "print(f\"\\nPerformance:\")\n",
        "print(f\"  Train Accuracy: {metadata['performance']['train_accuracy']:.4f}\")\n",
        "print(f\"  Test Accuracy: {metadata['performance']['test_accuracy']:.4f}\")\n",
        "print(f\"\\nTraining Data:\")\n",
        "print(f\"  Samples: {metadata['training_data']['n_train_samples']} train, \"\n",
        "      f\"{metadata['training_data']['n_test_samples']} test\")\n",
        "print(f\"  Features: {metadata['training_data']['n_features']}\")\n",
        "print(f\"  Classes: {metadata['training_data']['n_classes']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "testing-strategies",
      "metadata": {},
      "source": [
        "## Part 3: Testing Strategies\n",
        "\n",
        "### 3.1 Model Validation Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "validation-tests",
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_loaded_model(model, metadata, samples, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Comprehensive validation of loaded model\n",
        "    \"\"\"\n",
        "    print(\"Running Model Validation Tests...\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    tests_passed = 0\n",
        "    tests_failed = 0\n",
        "    \n",
        "    # Test 1: Model type check\n",
        "    print(\"\\n[Test 1] Model Type Check\")\n",
        "    expected_type = metadata['model_type']\n",
        "    actual_type = type(model).__name__\n",
        "    if expected_type == actual_type or hasattr(model, 'named_steps'):\n",
        "        print(f\"  \u2713 PASS: Model type matches ({actual_type})\")\n",
        "        tests_passed += 1\n",
        "    else:\n",
        "        print(f\"  \u2717 FAIL: Expected {expected_type}, got {actual_type}\")\n",
        "        tests_failed += 1\n",
        "    \n",
        "    # Test 2: Sample predictions match\n",
        "    print(\"\\n[Test 2] Sample Predictions Consistency\")\n",
        "    sample_inputs = np.array(samples['input'])\n",
        "    expected_preds = np.array(samples['predictions'])\n",
        "    actual_preds = model.predict(sample_inputs)\n",
        "    \n",
        "    if np.array_equal(expected_preds, actual_preds):\n",
        "        print(f\"  \u2713 PASS: Predictions match saved samples\")\n",
        "        tests_passed += 1\n",
        "    else:\n",
        "        print(f\"  \u2717 FAIL: Predictions don't match\")\n",
        "        print(f\"    Expected: {expected_preds}\")\n",
        "        print(f\"    Got: {actual_preds}\")\n",
        "        tests_failed += 1\n",
        "    \n",
        "    # Test 3: Performance within tolerance\n",
        "    print(\"\\n[Test 3] Performance Validation\")\n",
        "    current_score = model.score(X_test, y_test)\n",
        "    saved_score = metadata['performance']['test_accuracy']\n",
        "    tolerance = 0.001\n",
        "    \n",
        "    if abs(current_score - saved_score) < tolerance:\n",
        "        print(f\"  \u2713 PASS: Performance matches (current: {current_score:.4f}, saved: {saved_score:.4f})\")\n",
        "        tests_passed += 1\n",
        "    else:\n",
        "        print(f\"  \u2717 FAIL: Performance differs significantly\")\n",
        "        print(f\"    Current: {current_score:.4f}\")\n",
        "        print(f\"    Saved: {saved_score:.4f}\")\n",
        "        tests_failed += 1\n",
        "    \n",
        "    # Test 4: Input shape validation\n",
        "    print(\"\\n[Test 4] Input Shape Validation\")\n",
        "    expected_features = metadata['training_data']['n_features']\n",
        "    test_features = X_test.shape[1]\n",
        "    \n",
        "    if expected_features == test_features:\n",
        "        print(f\"  \u2713 PASS: Input shape matches ({test_features} features)\")\n",
        "        tests_passed += 1\n",
        "    else:\n",
        "        print(f\"  \u2717 FAIL: Input shape mismatch\")\n",
        "        print(f\"    Expected: {expected_features} features\")\n",
        "        print(f\"    Got: {test_features} features\")\n",
        "        tests_failed += 1\n",
        "    \n",
        "    # Test 5: Model has required methods\n",
        "    print(\"\\n[Test 5] Required Methods Check\")\n",
        "    required_methods = ['predict', 'score']\n",
        "    missing_methods = [m for m in required_methods if not hasattr(model, m)]\n",
        "    \n",
        "    if not missing_methods:\n",
        "        print(f\"  \u2713 PASS: All required methods present\")\n",
        "        tests_passed += 1\n",
        "    else:\n",
        "        print(f\"  \u2717 FAIL: Missing methods: {missing_methods}\")\n",
        "        tests_failed += 1\n",
        "    \n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"VALIDATION SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Tests Passed: {tests_passed}/{tests_passed + tests_failed}\")\n",
        "    print(f\"Tests Failed: {tests_failed}/{tests_passed + tests_failed}\")\n",
        "    \n",
        "    if tests_failed == 0:\n",
        "        print(\"\\n\u2713 All validation tests passed! Model is ready for use.\")\n",
        "    else:\n",
        "        print(\"\\n\u26a0 Some validation tests failed. Review before using in production.\")\n",
        "    \n",
        "    return tests_failed == 0\n",
        "\n",
        "# Run validation\n",
        "is_valid = validate_loaded_model(loaded_model, metadata, samples, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "production-checks",
      "metadata": {},
      "source": [
        "### 3.2 Production Readiness Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "production-checks",
      "metadata": {},
      "outputs": [],
      "source": [
        "def production_readiness_check(model, metadata):\n",
        "    \"\"\"\n",
        "    Check if model is ready for production deployment\n",
        "    \"\"\"\n",
        "    print(\"Production Readiness Checklist\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    checks = []\n",
        "    \n",
        "    # Check 1: Performance threshold\n",
        "    test_acc = metadata['performance']['test_accuracy']\n",
        "    min_accuracy = 0.85  # 85% minimum\n",
        "    status = \"\u2713\" if test_acc >= min_accuracy else \"\u2717\"\n",
        "    checks.append({\n",
        "        'Check': 'Performance Threshold',\n",
        "        'Status': status,\n",
        "        'Details': f\"{test_acc:.1%} (min: {min_accuracy:.1%})\"\n",
        "    })\n",
        "    \n",
        "    # Check 2: Metadata completeness\n",
        "    required_fields = ['model_name', 'version', 'created_at', 'model_type', 'performance']\n",
        "    has_all_fields = all(field in metadata for field in required_fields)\n",
        "    status = \"\u2713\" if has_all_fields else \"\u2717\"\n",
        "    checks.append({\n",
        "        'Check': 'Metadata Completeness',\n",
        "        'Status': status,\n",
        "        'Details': 'All required fields present' if has_all_fields else 'Missing fields'\n",
        "    })\n",
        "    \n",
        "    # Check 3: Training data sufficiency\n",
        "    n_train = metadata['training_data']['n_train_samples']\n",
        "    n_features = metadata['training_data']['n_features']\n",
        "    min_samples = n_features * 10  # Rule of thumb: 10x features\n",
        "    status = \"\u2713\" if n_train >= min_samples else \"\u26a0\"\n",
        "    checks.append({\n",
        "        'Check': 'Training Data Sufficiency',\n",
        "        'Status': status,\n",
        "        'Details': f\"{n_train} samples, {n_features} features\"\n",
        "    })\n",
        "    \n",
        "    # Check 4: Model size (for deployment)\n",
        "    model_size = 0.5  # MB (placeholder)\n",
        "    max_size = 100  # 100 MB limit\n",
        "    status = \"\u2713\" if model_size < max_size else \"\u26a0\"\n",
        "    checks.append({\n",
        "        'Check': 'Model Size',\n",
        "        'Status': status,\n",
        "        'Details': f\"{model_size:.1f} MB (max: {max_size} MB)\"\n",
        "    })\n",
        "    \n",
        "    # Check 5: Prediction methods available\n",
        "    has_predict = hasattr(model, 'predict')\n",
        "    has_proba = hasattr(model, 'predict_proba')\n",
        "    status = \"\u2713\" if has_predict else \"\u2717\"\n",
        "    details = \"predict\" + (\", predict_proba\" if has_proba else \"\")\n",
        "    checks.append({\n",
        "        'Check': 'Prediction Methods',\n",
        "        'Status': status,\n",
        "        'Details': details\n",
        "    })\n",
        "    \n",
        "    # Check 6: Version control\n",
        "    has_version = 'version' in metadata and metadata['version'] is not None\n",
        "    status = \"\u2713\" if has_version else \"\u2717\"\n",
        "    checks.append({\n",
        "        'Check': 'Version Control',\n",
        "        'Status': status,\n",
        "        'Details': f\"v{metadata['version']}\" if has_version else 'No version'\n",
        "    })\n",
        "    \n",
        "    # Display results\n",
        "    checks_df = pd.DataFrame(checks)\n",
        "    print(checks_df.to_string(index=False))\n",
        "    \n",
        "    # Overall assessment\n",
        "    passed = sum(1 for c in checks if c['Status'] == '\u2713')\n",
        "    total = len(checks)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"Overall: {passed}/{total} checks passed\")\n",
        "    \n",
        "    if passed == total:\n",
        "        print(\"\\n\u2713 Model is production-ready!\")\n",
        "    elif passed >= total * 0.8:\n",
        "        print(\"\\n\u26a0 Model is mostly ready, but review warnings\")\n",
        "    else:\n",
        "        print(\"\\n\u2717 Model needs improvements before production deployment\")\n",
        "    \n",
        "    return passed == total\n",
        "\n",
        "# Run production checks\n",
        "is_production_ready = production_readiness_check(loaded_model, metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "inference-testing",
      "metadata": {},
      "source": [
        "### 3.3 Inference Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "inference-test",
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_inference_pipeline(model, n_samples=100):\n",
        "    \"\"\"\n",
        "    Test model inference with various inputs\n",
        "    \"\"\"\n",
        "    print(\"Testing Inference Pipeline\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Test 1: Single sample prediction\n",
        "    print(\"\\n[Test 1] Single Sample Prediction\")\n",
        "    try:\n",
        "        single_input = X_test[0:1]\n",
        "        pred = model.predict(single_input)\n",
        "        print(f\"  \u2713 Single prediction works: {pred[0]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  \u2717 Single prediction failed: {e}\")\n",
        "    \n",
        "    # Test 2: Batch prediction\n",
        "    print(\"\\n[Test 2] Batch Prediction\")\n",
        "    try:\n",
        "        batch_input = X_test[:10]\n",
        "        preds = model.predict(batch_input)\n",
        "        print(f\"  \u2713 Batch prediction works: {len(preds)} predictions\")\n",
        "    except Exception as e:\n",
        "        print(f\"  \u2717 Batch prediction failed: {e}\")\n",
        "    \n",
        "    # Test 3: Probability predictions (if available)\n",
        "    print(\"\\n[Test 3] Probability Predictions\")\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        try:\n",
        "            proba = model.predict_proba(X_test[:5])\n",
        "            print(f\"  \u2713 Probability predictions work\")\n",
        "            print(f\"    Shape: {proba.shape}\")\n",
        "            print(f\"    Sum to 1: {np.allclose(proba.sum(axis=1), 1.0)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  \u2717 Probability prediction failed: {e}\")\n",
        "    else:\n",
        "        print(\"  - Probability predictions not available\")\n",
        "    \n",
        "    # Test 4: Edge cases\n",
        "    print(\"\\n[Test 4] Edge Cases\")\n",
        "    \n",
        "    # Minimum values\n",
        "    try:\n",
        "        min_input = np.min(X_test, axis=0).reshape(1, -1)\n",
        "        pred = model.predict(min_input)\n",
        "        print(f\"  \u2713 Minimum values: prediction = {pred[0]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  \u2717 Minimum values failed: {e}\")\n",
        "    \n",
        "    # Maximum values\n",
        "    try:\n",
        "        max_input = np.max(X_test, axis=0).reshape(1, -1)\n",
        "        pred = model.predict(max_input)\n",
        "        print(f\"  \u2713 Maximum values: prediction = {pred[0]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  \u2717 Maximum values failed: {e}\")\n",
        "    \n",
        "    # Test 5: Performance timing\n",
        "    print(\"\\n[Test 5] Inference Speed\")\n",
        "    import time\n",
        "    \n",
        "    n_iterations = 100\n",
        "    start = time.time()\n",
        "    for _ in range(n_iterations):\n",
        "        model.predict(X_test[:1])\n",
        "    elapsed = time.time() - start\n",
        "    \n",
        "    avg_time = (elapsed / n_iterations) * 1000  # milliseconds\n",
        "    print(f\"  Average prediction time: {avg_time:.3f} ms\")\n",
        "    \n",
        "    if avg_time < 10:\n",
        "        print(\"  \u2713 Very fast (< 10ms)\")\n",
        "    elif avg_time < 100:\n",
        "        print(\"  \u2713 Fast enough for most applications\")\n",
        "    else:\n",
        "        print(\"  \u26a0 Slower than expected, may need optimization\")\n",
        "    \n",
        "    print(\"\\n\u2713 Inference testing complete!\")\n",
        "\n",
        "# Run inference tests\n",
        "test_inference_pipeline(loaded_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "versioning-system",
      "metadata": {},
      "source": [
        "## Part 4: Model Versioning System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "version-manager",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModelVersionManager:\n",
        "    \"\"\"\n",
        "    Simple model version management system\n",
        "    \"\"\"\n",
        "    def __init__(self, base_dir='models'):\n",
        "        self.base_dir = base_dir\n",
        "        os.makedirs(base_dir, exist_ok=True)\n",
        "    \n",
        "    def save_version(self, model, model_name, version, metadata):\n",
        "        \"\"\"Save a new model version\"\"\"\n",
        "        version_dir = f\"{self.base_dir}/{model_name}/v{version}\"\n",
        "        os.makedirs(version_dir, exist_ok=True)\n",
        "        \n",
        "        # Save model\n",
        "        model_path = f\"{version_dir}/model.joblib\"\n",
        "        joblib.dump(model, model_path)\n",
        "        \n",
        "        # Save metadata\n",
        "        metadata['version'] = version\n",
        "        metadata['created_at'] = datetime.now().isoformat()\n",
        "        metadata_path = f\"{version_dir}/metadata.json\"\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "        \n",
        "        print(f\"\u2713 Saved {model_name} v{version}\")\n",
        "        return version_dir\n",
        "    \n",
        "    def load_version(self, model_name, version):\n",
        "        \"\"\"Load specific model version\"\"\"\n",
        "        version_dir = f\"{self.base_dir}/{model_name}/v{version}\"\n",
        "        model_path = f\"{version_dir}/model.joblib\"\n",
        "        metadata_path = f\"{version_dir}/metadata.json\"\n",
        "        \n",
        "        model = joblib.load(model_path)\n",
        "        with open(metadata_path, 'r') as f:\n",
        "            metadata = json.load(f)\n",
        "        \n",
        "        return model, metadata\n",
        "    \n",
        "    def list_versions(self, model_name):\n",
        "        \"\"\"List all versions of a model\"\"\"\n",
        "        model_dir = f\"{self.base_dir}/{model_name}\"\n",
        "        if not os.path.exists(model_dir):\n",
        "            return []\n",
        "        \n",
        "        versions = []\n",
        "        for item in os.listdir(model_dir):\n",
        "            if item.startswith('v'):\n",
        "                version = item[1:]  # Remove 'v' prefix\n",
        "                metadata_path = f\"{model_dir}/{item}/metadata.json\"\n",
        "                if os.path.exists(metadata_path):\n",
        "                    with open(metadata_path, 'r') as f:\n",
        "                        metadata = json.load(f)\n",
        "                    versions.append({\n",
        "                        'version': version,\n",
        "                        'created_at': metadata.get('created_at', 'Unknown'),\n",
        "                        'test_accuracy': metadata.get('performance', {}).get('test_accuracy', 0)\n",
        "                    })\n",
        "        \n",
        "        return sorted(versions, key=lambda x: x['version'])\n",
        "    \n",
        "    def compare_versions(self, model_name, versions):\n",
        "        \"\"\"Compare multiple versions\"\"\"\n",
        "        comparison = []\n",
        "        for version in versions:\n",
        "            try:\n",
        "                _, metadata = self.load_version(model_name, version)\n",
        "                comparison.append({\n",
        "                    'Version': version,\n",
        "                    'Created': metadata.get('created_at', 'Unknown')[:10],\n",
        "                    'Test Accuracy': metadata.get('performance', {}).get('test_accuracy', 0),\n",
        "                    'Model Type': metadata.get('model_type', 'Unknown')\n",
        "                })\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "        return pd.DataFrame(comparison)\n",
        "\n",
        "# Example usage\n",
        "version_manager = ModelVersionManager()\n",
        "\n",
        "# Save multiple versions\n",
        "print(\"Saving multiple model versions...\\n\")\n",
        "\n",
        "# Version 1.0 - Logistic Regression\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model.fit(X_train, y_train)\n",
        "version_manager.save_version(\n",
        "    lr_model, 'iris_model', '1.0',\n",
        "    {'model_type': 'LogisticRegression', 'performance': {'test_accuracy': lr_model.score(X_test, y_test)}}\n",
        ")\n",
        "\n",
        "# Version 2.0 - Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "version_manager.save_version(\n",
        "    rf_model, 'iris_model', '2.0',\n",
        "    {'model_type': 'RandomForestClassifier', 'performance': {'test_accuracy': rf_model.score(X_test, y_test)}}\n",
        ")\n",
        "\n",
        "# Version 2.1 - Random Forest (improved)\n",
        "rf_model2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model2.fit(X_train, y_train)\n",
        "version_manager.save_version(\n",
        "    rf_model2, 'iris_model', '2.1',\n",
        "    {'model_type': 'RandomForestClassifier', 'performance': {'test_accuracy': rf_model2.score(X_test, y_test)}}\n",
        ")\n",
        "\n",
        "# List all versions\n",
        "print(\"\\nAvailable versions:\")\n",
        "versions = version_manager.list_versions('iris_model')\n",
        "for v in versions:\n",
        "    print(f\"  v{v['version']}: {v['test_accuracy']:.4f} accuracy (created: {v['created_at'][:10]})\")\n",
        "\n",
        "# Compare versions\n",
        "print(\"\\nVersion Comparison:\")\n",
        "comparison_df = version_manager.compare_versions('iris_model', ['1.0', '2.0', '2.1'])\n",
        "print(comparison_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "best-practices-summary",
      "metadata": {},
      "source": [
        "## Best Practices Summary\n",
        "\n",
        "### Saving Models\n",
        "\n",
        "```python\n",
        "# \u2713 DO: Use joblib for sklearn models\n",
        "joblib.dump(model, 'model.joblib')\n",
        "\n",
        "# \u2713 DO: Save entire pipelines, not just models\n",
        "pipeline = Pipeline([('scaler', StandardScaler()), ('model', model)])\n",
        "joblib.dump(pipeline, 'pipeline.joblib')\n",
        "\n",
        "# \u2713 DO: Include version and metadata\n",
        "metadata = {\n",
        "    'version': '1.0',\n",
        "    'created_at': datetime.now().isoformat(),\n",
        "    'performance': {'test_accuracy': score}\n",
        "}\n",
        "\n",
        "# \u2717 DON'T: Forget to save preprocessing steps\n",
        "# \u2717 DON'T: Hardcode file paths without version control\n",
        "```\n",
        "\n",
        "### Loading Models\n",
        "\n",
        "```python\n",
        "# \u2713 DO: Validate after loading\n",
        "loaded_model = joblib.load('model.joblib')\n",
        "assert loaded_model.score(X_test, y_test) == expected_score\n",
        "\n",
        "# \u2713 DO: Check model type and version\n",
        "# \u2713 DO: Test with sample predictions\n",
        "\n",
        "# \u2717 DON'T: Use without validation\n",
        "# \u2717 DON'T: Assume compatibility across sklearn versions\n",
        "```\n",
        "\n",
        "### Versioning\n",
        "\n",
        "1. **Use semantic versioning**: major.minor.patch (e.g., 1.2.3)\n",
        "2. **Save metadata**: performance, training data info, hyperparameters\n",
        "3. **Track dependencies**: sklearn version, Python version\n",
        "4. **Keep multiple versions**: For rollback capability\n",
        "5. **Document changes**: What improved between versions\n",
        "\n",
        "### Testing\n",
        "\n",
        "1. **Model type validation**: Verify correct model loaded\n",
        "2. **Performance validation**: Check accuracy matches metadata\n",
        "3. **Sample prediction tests**: Ensure predictions are consistent\n",
        "4. **Input validation**: Test with various input shapes\n",
        "5. **Edge case testing**: Min/max values, empty inputs\n",
        "6. **Inference speed testing**: Ensure acceptable latency\n",
        "\n",
        "### Production Deployment\n",
        "\n",
        "```python\n",
        "# Checklist before deployment:\n",
        "\u2713 Model performance meets threshold\n",
        "\u2713 Complete metadata available\n",
        "\u2713 Validation tests pass\n",
        "\u2713 Inference speed acceptable\n",
        "\u2713 Version control in place\n",
        "\u2713 Rollback plan ready\n",
        "\u2713 Monitoring configured\n",
        "```\n",
        "\n",
        "### Common Pitfalls\n",
        "\n",
        "- \u274c Saving model without preprocessing steps\n",
        "- \u274c No version control or metadata\n",
        "- \u274c Not testing loaded model before use\n",
        "- \u274c Using pickle instead of joblib for sklearn\n",
        "- \u274c Hardcoding paths instead of relative paths\n",
        "- \u274c No validation or testing strategy\n",
        "- \u274c Forgetting about sklearn version compatibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cleanup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup (optional)\n",
        "import shutil\n",
        "\n",
        "print(\"Model persistence demonstration complete!\")\n",
        "print(\"\\nGenerated files:\")\n",
        "for root, dirs, files in os.walk('models'):\n",
        "    level = root.replace('models', '').count(os.sep)\n",
        "    indent = ' ' * 2 * level\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    subindent = ' ' * 2 * (level + 1)\n",
        "    for file in files:\n",
        "        print(f\"{subindent}{file}\")\n",
        "\n",
        "# Uncomment to clean up\n",
        "# shutil.rmtree('models')\n",
        "# print(\"\\n\u2713 Cleaned up models directory\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}