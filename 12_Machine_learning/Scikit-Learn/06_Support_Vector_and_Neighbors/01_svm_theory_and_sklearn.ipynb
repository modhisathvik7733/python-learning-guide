{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Support Vector Machines (SVM)\n",
        "\n",
        "## Overview\n",
        "\n",
        "**Support Vector Machines (SVM)** are powerful supervised learning models for classification and regression that find the optimal decision boundary (hyperplane) separating different classes.\n",
        "\n",
        "### Core Concept\n",
        "\n",
        "*\"Find the hyperplane that maximally separates the classes\"*\n",
        "\n",
        "### Key Ideas\n",
        "\n",
        "1. **Maximum Margin**: SVM finds the decision boundary with the largest distance to the nearest data points (support vectors)\n",
        "2. **Kernel Trick**: Transform data to higher dimensions to handle non-linear patterns\n",
        "3. **Support Vectors**: Only boundary points matter for defining the decision surface\n",
        "4. **Soft Margin**: Allow some misclassifications to handle noisy data\n",
        "\n",
        "## Mathematical Foundation\n",
        "\n",
        "### Linear SVM (Binary Classification)\n",
        "\n",
        "**Goal**: Find hyperplane \\(w^T x + b = 0\\) that maximizes margin.\n",
        "\n",
        "**Decision Function**:\n",
        "\\[\n",
        "f(x) = \\text{sign}(w^T x + b)\n",
        "\\]\n",
        "\n",
        "**Margin**: Distance from hyperplane to nearest point\n",
        "\\[\n",
        "\\text{margin} = \\frac{2}{||w||}\n",
        "\\]\n",
        "\n",
        "**Hard Margin Optimization**:\n",
        "\\[\n",
        "\\min_{w,b} \\frac{1}{2}||w||^2 \\quad \\text{subject to} \\quad y_i(w^T x_i + b) \\geq 1 \\quad \\forall i\n",
        "\\]\n",
        "\n",
        "**Soft Margin (with slack variables \\(\\xi_i\\))**:\n",
        "\\[\n",
        "\\min_{w,b,\\xi} \\frac{1}{2}||w||^2 + C\\sum_{i=1}^{n}\\xi_i\n",
        "\\]\n",
        "\\[\n",
        "\\text{subject to} \\quad y_i(w^T x_i + b) \\geq 1 - \\xi_i \\quad \\text{and} \\quad \\xi_i \\geq 0\n",
        "\\]\n",
        "\n",
        "where:\n",
        "- \\(C\\) = regularization parameter (inverse of margin width)\n",
        "- \\(\\xi_i\\) = slack variables (allow misclassifications)\n",
        "- Large \\(C\\) \u2192 hard margin (less tolerance)\n",
        "- Small \\(C\\) \u2192 soft margin (more tolerance)\n",
        "\n",
        "### Kernel Trick\n",
        "\n",
        "**Idea**: Map data to higher dimensions where it becomes linearly separable.\n",
        "\n",
        "\\[\n",
        "\\phi: \\mathbb{R}^d \\rightarrow \\mathbb{R}^D \\quad (D >> d)\n",
        "\\]\n",
        "\n",
        "**Kernel Function**: Compute inner product in high-dimensional space without explicit mapping\n",
        "\\[\n",
        "K(x_i, x_j) = \\phi(x_i)^T \\phi(x_j)\n",
        "\\]\n",
        "\n",
        "**Common Kernels**:\n",
        "\n",
        "1. **Linear**: \\(K(x_i, x_j) = x_i^T x_j\\)\n",
        "\n",
        "2. **Polynomial**: \\(K(x_i, x_j) = (\\gamma x_i^T x_j + r)^d\\)\n",
        "\n",
        "3. **RBF (Gaussian)**: \\(K(x_i, x_j) = \\exp(-\\gamma ||x_i - x_j||^2)\\)\n",
        "\n",
        "4. **Sigmoid**: \\(K(x_i, x_j) = \\tanh(\\gamma x_i^T x_j + r)\\)\n",
        "\n",
        "## Topics Covered\n",
        "\n",
        "1. Linear SVM theory and implementation\n",
        "2. Kernel methods and the kernel trick\n",
        "3. Hyperparameter tuning (C, gamma, kernel)\n",
        "4. Feature scaling importance\n",
        "5. SVC vs LinearSVC\n",
        "6. Multi-class classification\n",
        "7. SVR (Support Vector Regression)\n",
        "8. Practical applications and best practices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from time import time\n",
        "\n",
        "# SVM models\n",
        "from sklearn.svm import SVC, SVR, LinearSVC, LinearSVR\n",
        "\n",
        "# Other models for comparison\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Utilities\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, cross_val_score, GridSearchCV,\n",
        "    learning_curve, validation_curve\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, confusion_matrix,\n",
        "    mean_squared_error, r2_score, make_scorer\n",
        ")\n",
        "from sklearn.datasets import (\n",
        "    make_classification, make_circles, make_moons,\n",
        "    load_breast_cancer, load_wine, load_diabetes\n",
        ")\n",
        "\n",
        "np.random.seed(42)\n",
        "sns.set_style('whitegrid')\n",
        "print(\"\u2713 Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "linear-svm-intuition",
      "metadata": {},
      "source": [
        "## 1. Linear SVM Intuition\n",
        "\n",
        "### 1.1 Visualizing the Maximum Margin Concept"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "linear-svm-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate linearly separable data\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X_linear, y_linear = make_blobs(n_samples=100, n_features=2, centers=2, \n",
        "                                cluster_std=1.0, center_box=(-5, 5), random_state=42)\n",
        "\n",
        "print(\"Linear SVM Demonstration\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_linear.shape[0]}\")\n",
        "print(f\"Features: {X_linear.shape[1]}\")\n",
        "print(f\"Classes: {np.unique(y_linear)}\")\n",
        "\n",
        "# Train linear SVM\n",
        "svm_linear = SVC(kernel='linear', C=1.0)\n",
        "svm_linear.fit(X_linear, y_linear)\n",
        "\n",
        "print(f\"\\nSupport Vectors: {svm_linear.n_support_}\")\n",
        "print(f\"Total: {svm_linear.support_vectors_.shape[0]} out of {X_linear.shape[0]} samples\")\n",
        "print(f\"Percentage: {svm_linear.support_vectors_.shape[0]/X_linear.shape[0]*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "plot-decision-boundary",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_svm_decision_boundary(model, X, y, title=\"SVM Decision Boundary\"):\n",
        "    \"\"\"Plot SVM decision boundary with margins.\"\"\"\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    \n",
        "    # Create mesh\n",
        "    h = 0.02\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    # Predict\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    # Plot decision boundary\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
        "    \n",
        "    # Plot margins (if available)\n",
        "    if hasattr(model, 'decision_function'):\n",
        "        decision_values = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "        decision_values = decision_values.reshape(xx.shape)\n",
        "        \n",
        "        # Decision boundary and margins\n",
        "        plt.contour(xx, yy, decision_values, colors='k', levels=[-1, 0, 1],\n",
        "                   alpha=0.5, linestyles=['--', '-', '--'])\n",
        "    \n",
        "    # Plot data points\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='k', s=50)\n",
        "    \n",
        "    # Highlight support vectors\n",
        "    if hasattr(model, 'support_vectors_'):\n",
        "        plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
        "                   s=200, linewidth=2, facecolors='none', edgecolors='green',\n",
        "                   label='Support Vectors')\n",
        "    \n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot\n",
        "plot_svm_decision_boundary(svm_linear, X_linear, y_linear, \n",
        "                          \"Linear SVM: Maximum Margin Classifier\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Key Observations:\")\n",
        "print(\"   - Solid line = Decision boundary (w^T x + b = 0)\")\n",
        "print(\"   - Dashed lines = Margins (w^T x + b = \u00b11)\")\n",
        "print(\"   - Green circles = Support vectors (only these points define the boundary)\")\n",
        "print(\"   - SVM maximizes the distance between margins\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c-parameter",
      "metadata": {},
      "source": [
        "### 1.2 Effect of C Parameter (Regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c-parameter-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Effect of C Parameter\")\n",
        "print(\"=\"*70)\n",
        "print(\"C = Regularization parameter (inverse of margin width)\")\n",
        "print(\"  - Large C: Hard margin (less tolerance for errors)\")\n",
        "print(\"  - Small C: Soft margin (more tolerance for errors)\\n\")\n",
        "\n",
        "# Test different C values\n",
        "C_values = [0.01, 1.0, 100.0]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for idx, C in enumerate(C_values):\n",
        "    svm = SVC(kernel='linear', C=C)\n",
        "    svm.fit(X_linear, y_linear)\n",
        "    \n",
        "    # Create mesh\n",
        "    h = 0.02\n",
        "    x_min, x_max = X_linear[:, 0].min() - 1, X_linear[:, 0].max() + 1\n",
        "    y_min, y_max = X_linear[:, 1].min() - 1, X_linear[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
        "    \n",
        "    # Decision boundary\n",
        "    decision_values = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    decision_values = decision_values.reshape(xx.shape)\n",
        "    axes[idx].contour(xx, yy, decision_values, colors='k', levels=[-1, 0, 1],\n",
        "                     alpha=0.5, linestyles=['--', '-', '--'])\n",
        "    \n",
        "    axes[idx].scatter(X_linear[:, 0], X_linear[:, 1], c=y_linear, \n",
        "                     cmap='RdYlBu', edgecolors='k', s=50)\n",
        "    axes[idx].scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1],\n",
        "                     s=200, linewidth=2, facecolors='none', edgecolors='green')\n",
        "    \n",
        "    axes[idx].set_title(f'C = {C}\\n{svm.n_support_.sum()} Support Vectors')\n",
        "    axes[idx].set_xlabel('Feature 1')\n",
        "    axes[idx].set_ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Observations:\")\n",
        "print(\"   - Small C (0.01): Wide margin, many support vectors, may underfit\")\n",
        "print(\"   - Medium C (1.0): Balanced margin\")\n",
        "print(\"   - Large C (100.0): Narrow margin, few support vectors, may overfit\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kernel-trick",
      "metadata": {},
      "source": [
        "## 2. The Kernel Trick\n",
        "\n",
        "### 2.1 Non-Linear Data: When Linear SVM Fails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nonlinear-data",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate non-linearly separable data\n",
        "X_circles, y_circles = make_circles(n_samples=200, noise=0.1, factor=0.3, random_state=42)\n",
        "X_moons, y_moons = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
        "\n",
        "print(\"Non-Linear Datasets\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Circles\n",
        "axes[0].scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, \n",
        "               cmap='RdYlBu', edgecolors='k', s=50)\n",
        "axes[0].set_title('Concentric Circles (Non-Linearly Separable)')\n",
        "axes[0].set_xlabel('Feature 1')\n",
        "axes[0].set_ylabel('Feature 2')\n",
        "\n",
        "# Moons\n",
        "axes[1].scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, \n",
        "               cmap='RdYlBu', edgecolors='k', s=50)\n",
        "axes[1].set_title('Half Moons (Non-Linearly Separable)')\n",
        "axes[1].set_xlabel('Feature 1')\n",
        "axes[1].set_ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 These patterns cannot be separated by a straight line!\")\n",
        "print(\"   Solution: Use kernel trick to map to higher dimensions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rbf-kernel",
      "metadata": {},
      "source": [
        "### 2.2 RBF (Gaussian) Kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rbf-kernel-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"RBF Kernel Demonstration\")\n",
        "print(\"=\"*70)\n",
        "print(\"RBF Kernel: K(x, x') = exp(-gamma * ||x - x'||^2)\")\n",
        "print(\"  - Transforms data to infinite-dimensional space\")\n",
        "print(\"  - gamma controls the 'reach' of a single training example\\n\")\n",
        "\n",
        "# Compare linear vs RBF kernel on circles dataset\n",
        "kernels = ['linear', 'rbf']\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "for idx, kernel in enumerate(kernels):\n",
        "    svm = SVC(kernel=kernel, C=1.0, gamma='scale')\n",
        "    svm.fit(X_circles, y_circles)\n",
        "    \n",
        "    # Create mesh\n",
        "    h = 0.02\n",
        "    x_min, x_max = X_circles[:, 0].min() - 0.5, X_circles[:, 0].max() + 0.5\n",
        "    y_min, y_max = X_circles[:, 1].min() - 0.5, X_circles[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
        "    axes[idx].scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles,\n",
        "                     cmap='RdYlBu', edgecolors='k', s=50)\n",
        "    \n",
        "    accuracy = svm.score(X_circles, y_circles)\n",
        "    axes[idx].set_title(f'{kernel.upper()} Kernel\\nAccuracy: {accuracy:.3f}')\n",
        "    axes[idx].set_xlabel('Feature 1')\n",
        "    axes[idx].set_ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 RBF kernel perfectly captures the circular pattern!\")\n",
        "print(\"   Linear kernel fails on non-linear data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gamma-parameter",
      "metadata": {},
      "source": [
        "### 2.3 Effect of Gamma Parameter (RBF Kernel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gamma-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Effect of Gamma Parameter (RBF Kernel)\")\n",
        "print(\"=\"*70)\n",
        "print(\"gamma = Kernel coefficient\")\n",
        "print(\"  - Large gamma: High curvature, narrow decision boundaries (may overfit)\")\n",
        "print(\"  - Small gamma: Low curvature, smooth decision boundaries\\n\")\n",
        "\n",
        "gamma_values = [0.1, 1.0, 10.0]\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for idx, gamma in enumerate(gamma_values):\n",
        "    svm = SVC(kernel='rbf', C=1.0, gamma=gamma)\n",
        "    svm.fit(X_moons, y_moons)\n",
        "    \n",
        "    h = 0.02\n",
        "    x_min, x_max = X_moons[:, 0].min() - 0.5, X_moons[:, 0].max() + 0.5\n",
        "    y_min, y_max = X_moons[:, 1].min() - 0.5, X_moons[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
        "    axes[idx].scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons,\n",
        "                     cmap='RdYlBu', edgecolors='k', s=50)\n",
        "    \n",
        "    accuracy = svm.score(X_moons, y_moons)\n",
        "    axes[idx].set_title(f'gamma = {gamma}\\nAccuracy: {accuracy:.3f}')\n",
        "    axes[idx].set_xlabel('Feature 1')\n",
        "    axes[idx].set_ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Gamma controls model complexity:\")\n",
        "print(\"   - Too small: Underfitting (overly smooth)\")\n",
        "print(\"   - Too large: Overfitting (too wiggly)\")\n",
        "print(\"   - Use cross-validation to find optimal gamma\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kernel-comparison",
      "metadata": {},
      "source": [
        "### 2.4 Comparing All Kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kernel-comparison-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Kernel Comparison on Moons Dataset\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, kernel in enumerate(kernels):\n",
        "    # Special parameters for polynomial\n",
        "    if kernel == 'poly':\n",
        "        svm = SVC(kernel=kernel, C=1.0, degree=3, gamma='scale')\n",
        "    else:\n",
        "        svm = SVC(kernel=kernel, C=1.0, gamma='scale')\n",
        "    \n",
        "    svm.fit(X_moons, y_moons)\n",
        "    \n",
        "    h = 0.02\n",
        "    x_min, x_max = X_moons[:, 0].min() - 0.5, X_moons[:, 0].max() + 0.5\n",
        "    y_min, y_max = X_moons[:, 1].min() - 0.5, X_moons[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
        "    axes[idx].scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons,\n",
        "                     cmap='RdYlBu', edgecolors='k', s=50)\n",
        "    \n",
        "    accuracy = svm.score(X_moons, y_moons)\n",
        "    n_sv = svm.support_vectors_.shape[0]\n",
        "    axes[idx].set_title(f'{kernel.upper()} Kernel\\nAccuracy: {accuracy:.3f}, Support Vectors: {n_sv}')\n",
        "    axes[idx].set_xlabel('Feature 1')\n",
        "    axes[idx].set_ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKernel Summary:\")\n",
        "print(\"  Linear: Fast, interpretable, good for linearly separable data\")\n",
        "print(\"  Polynomial: Good for polynomial relationships, needs degree tuning\")\n",
        "print(\"  RBF: Most versatile, good default choice, needs C and gamma tuning\")\n",
        "print(\"  Sigmoid: Similar to neural networks, rarely used\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "scaling-importance",
      "metadata": {},
      "source": [
        "## 3. Importance of Feature Scaling\n",
        "\n",
        "### 3.1 SVM with and without Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "scaling-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset with different scales\n",
        "np.random.seed(42)\n",
        "X_unscaled = np.random.randn(200, 2)\n",
        "X_unscaled[:, 0] *= 10  # First feature has large scale\n",
        "X_unscaled[:, 1] *= 0.1  # Second feature has small scale\n",
        "y_unscaled = (X_unscaled[:, 0] + X_unscaled[:, 1] > 0).astype(int)\n",
        "\n",
        "print(\"Importance of Feature Scaling for SVM\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Feature 1 range: [{X_unscaled[:, 0].min():.2f}, {X_unscaled[:, 0].max():.2f}]\")\n",
        "print(f\"Feature 2 range: [{X_unscaled[:, 1].min():.2f}, {X_unscaled[:, 1].max():.2f}]\")\n",
        "print(f\"\\nFeature 1 is ~100x larger scale than Feature 2!\\n\")\n",
        "\n",
        "# Scale data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_unscaled)\n",
        "\n",
        "# Train SVMs\n",
        "svm_unscaled = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "svm_scaled = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "\n",
        "svm_unscaled.fit(X_unscaled, y_unscaled)\n",
        "svm_scaled.fit(X_scaled, y_unscaled)\n",
        "\n",
        "# Compare\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "for idx, (X, svm, title) in enumerate([\n",
        "    (X_unscaled, svm_unscaled, 'Without Scaling'),\n",
        "    (X_scaled, svm_scaled, 'With Scaling')\n",
        "]):\n",
        "    h = 0.02 if idx == 1 else 0.5\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
        "    axes[idx].scatter(X[:, 0], X[:, 1], c=y_unscaled,\n",
        "                     cmap='RdYlBu', edgecolors='k', s=50)\n",
        "    \n",
        "    accuracy = svm.score(X, y_unscaled)\n",
        "    axes[idx].set_title(f'{title}\\nAccuracy: {accuracy:.3f}')\n",
        "    axes[idx].set_xlabel('Feature 1')\n",
        "    axes[idx].set_ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 KEY LESSON: Always scale features for SVM!\")\n",
        "print(\"   SVM is sensitive to feature scales because:\")\n",
        "print(\"   - Distance-based algorithm (uses Euclidean distance)\")\n",
        "print(\"   - Large-scale features dominate the distance calculation\")\n",
        "print(\"   - Scaling ensures all features contribute equally\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "real-world-classification",
      "metadata": {},
      "source": [
        "## 4. Real-World Classification: Breast Cancer Detection\n",
        "\n",
        "### 4.1 Dataset Exploration and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "breast-cancer-load",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X_cancer = cancer.data\n",
        "y_cancer = cancer.target\n",
        "\n",
        "print(\"Breast Cancer Wisconsin Dataset\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_cancer.shape[0]}\")\n",
        "print(f\"Features: {X_cancer.shape[1]}\")\n",
        "print(f\"Classes: {cancer.target_names}\")\n",
        "print(f\"Class distribution: Malignant={np.sum(y_cancer==0)}, Benign={np.sum(y_cancer==1)}\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
        ")\n",
        "\n",
        "# Scale features (CRITICAL for SVM!)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"\\n\u2713 Features scaled using StandardScaler\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "svm-train-cancer",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train SVM with RBF kernel\n",
        "print(\"\\nTraining SVM (RBF Kernel)...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "svm_cancer = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
        "\n",
        "start_time = time()\n",
        "svm_cancer.fit(X_train_scaled, y_train)\n",
        "train_time = time() - start_time\n",
        "\n",
        "# Predictions\n",
        "y_pred = svm_cancer.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"Training time: {train_time:.3f}s\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Support vectors: {svm_cancer.n_support_}\")\n",
        "print(f\"Total support vectors: {svm_cancer.support_vectors_.shape[0]} / {X_train.shape[0]}\")\n",
        "print(f\"Percentage: {svm_cancer.support_vectors_.shape[0]/X_train.shape[0]*100:.1f}%\")\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=cancer.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hyperparameter-tuning",
      "metadata": {},
      "source": [
        "### 4.2 Hyperparameter Tuning with Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "grid-search-svm",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Hyperparameter Tuning for SVM\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1, 'scale', 'auto'],\n",
        "    'kernel': ['rbf', 'poly']\n",
        "}\n",
        "\n",
        "print(\"Parameter Grid:\")\n",
        "for param, values in param_grid.items():\n",
        "    print(f\"  {param}: {values}\")\n",
        "\n",
        "print(f\"\\nTotal combinations: {len(param_grid['C']) * len(param_grid['gamma']) * len(param_grid['kernel'])}\")\n",
        "print(\"Performing Grid Search with 5-fold CV...\\n\")\n",
        "\n",
        "# Grid search\n",
        "grid_search = GridSearchCV(\n",
        "    SVC(random_state=42),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "start_time = time()\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "grid_time = time() - start_time\n",
        "\n",
        "print(f\"\\nGrid Search completed in {grid_time:.2f}s\")\n",
        "print(f\"\\nBest Parameters:\")\n",
        "for param, value in grid_search.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "print(f\"\\nBest CV Score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "best_svm = grid_search.best_estimator_\n",
        "y_pred_best = best_svm.predict(X_test_scaled)\n",
        "test_accuracy = accuracy_score(y_test, y_pred_best)\n",
        "\n",
        "print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Improvement over default: {(test_accuracy - accuracy)*100:+.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "grid-search-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize grid search results\n",
        "results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "\n",
        "# Plot for RBF kernel\n",
        "rbf_results = results_df[results_df['param_kernel'] == 'rbf']\n",
        "\n",
        "# Create pivot table for heatmap\n",
        "pivot_data = []\n",
        "for gamma_val in param_grid['gamma']:\n",
        "    if gamma_val not in ['scale', 'auto']:\n",
        "        row_data = []\n",
        "        for c_val in param_grid['C']:\n",
        "            mask = (rbf_results['param_C'] == c_val) & (rbf_results['param_gamma'] == gamma_val)\n",
        "            if mask.any():\n",
        "                score = rbf_results[mask]['mean_test_score'].values[0]\n",
        "                row_data.append(score)\n",
        "        if row_data:\n",
        "            pivot_data.append(row_data)\n",
        "\n",
        "if pivot_data:\n",
        "    pivot_array = np.array(pivot_data)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(pivot_array, annot=True, fmt='.3f', cmap='YlGnBu',\n",
        "                xticklabels=param_grid['C'],\n",
        "                yticklabels=[g for g in param_grid['gamma'] if g not in ['scale', 'auto']])\n",
        "    plt.xlabel('C')\n",
        "    plt.ylabel('gamma')\n",
        "    plt.title('SVM Grid Search Results (RBF Kernel)\\nCV Accuracy Scores')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Heatmap shows CV accuracy for different C and gamma combinations\")\n",
        "print(\"   Darker blue = Better performance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "svc-vs-linearsvc",
      "metadata": {},
      "source": [
        "## 5. SVC vs LinearSVC\n",
        "\n",
        "### Key Differences\n",
        "\n",
        "| Aspect | SVC | LinearSVC |\n",
        "|--------|-----|----------|\n",
        "| Algorithm | libsvm (SMO) | liblinear |\n",
        "| Kernels | All (linear, rbf, poly, sigmoid) | Linear only |\n",
        "| Scalability | O(n\u00b2) to O(n\u00b3) | O(n) - Much faster |\n",
        "| Best For | Small-medium datasets, non-linear | Large datasets, linear problems |\n",
        "| Probability | `probability=True` required | Not available |\n",
        "| Multi-class | One-vs-One | One-vs-Rest |\n",
        "\n",
        "### 5.1 Performance Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "svc-linearsvc-comparison",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"SVC vs LinearSVC Comparison\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Train both models\n",
        "models = {\n",
        "    'SVC (linear kernel)': SVC(kernel='linear', C=1.0, random_state=42),\n",
        "    'LinearSVC': LinearSVC(C=1.0, random_state=42, max_iter=10000)\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Train\n",
        "    start_time = time()\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    train_time = time() - start_time\n",
        "    \n",
        "    # Predict\n",
        "    start_time = time()\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    predict_time = time() - start_time\n",
        "    \n",
        "    # Evaluate\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    \n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Train Time (s)': train_time,\n",
        "        'Predict Time (s)': predict_time\n",
        "    })\n",
        "    \n",
        "    print(f\"{name:25} - Accuracy: {accuracy:.4f}, Train: {train_time:.3f}s\")\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n\" + results_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\ud83d\udca1 LinearSVC is much faster for linear problems!\")\n",
        "print(\"   Use LinearSVC for:\")\n",
        "print(\"   - Large datasets (>10,000 samples)\")\n",
        "print(\"   - Text classification\")\n",
        "print(\"   - High-dimensional data\")\n",
        "print(\"   \\n   Use SVC when:\")\n",
        "print(\"   - Non-linear kernel needed\")\n",
        "print(\"   - Small-medium datasets\")\n",
        "print(\"   - Need probability estimates\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "multiclass",
      "metadata": {},
      "source": [
        "## 6. Multi-class Classification\n",
        "\n",
        "### 6.1 Wine Dataset (3 classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "multiclass-svm",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load wine dataset\n",
        "wine = load_wine()\n",
        "X_wine = wine.data\n",
        "y_wine = wine.target\n",
        "\n",
        "print(\"Multi-class Classification with SVM - Wine Dataset\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_wine.shape[0]}\")\n",
        "print(f\"Features: {X_wine.shape[1]}\")\n",
        "print(f\"Classes: {wine.target_names}\")\n",
        "print(f\"Class distribution: {np.bincount(y_wine)}\")\n",
        "\n",
        "# Split and scale\n",
        "X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(\n",
        "    X_wine, y_wine, test_size=0.2, random_state=42, stratify=y_wine\n",
        ")\n",
        "\n",
        "scaler_wine = StandardScaler()\n",
        "X_train_wine_scaled = scaler_wine.fit_transform(X_train_wine)\n",
        "X_test_wine_scaled = scaler_wine.transform(X_test_wine)\n",
        "\n",
        "# Train SVM\n",
        "svm_wine = SVC(kernel='rbf', C=10, gamma='scale', random_state=42)\n",
        "svm_wine.fit(X_train_wine_scaled, y_train_wine)\n",
        "\n",
        "# Predict\n",
        "y_pred_wine = svm_wine.predict(X_test_wine_scaled)\n",
        "\n",
        "# Evaluate\n",
        "accuracy_wine = accuracy_score(y_test_wine, y_pred_wine)\n",
        "\n",
        "print(f\"\\nMulti-class SVM Accuracy: {accuracy_wine:.4f}\")\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_wine, y_pred_wine, target_names=wine.target_names))\n",
        "\n",
        "# Confusion matrix\n",
        "cm_wine = confusion_matrix(y_test_wine, y_pred_wine)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_wine, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=wine.target_names,\n",
        "            yticklabels=wine.target_names)\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.title('SVM Multi-class Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 SVM handles multi-class automatically using One-vs-One strategy\")\n",
        "print(f\"   Number of binary classifiers: {len(svm_wine.classes_) * (len(svm_wine.classes_) - 1) // 2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "svr",
      "metadata": {},
      "source": [
        "## 7. Support Vector Regression (SVR)\n",
        "\n",
        "### Concept\n",
        "\n",
        "**SVR** finds a function that deviates from actual targets by at most \\(\\epsilon\\), while being as flat as possible.\n",
        "\n",
        "**Epsilon-insensitive loss**:\n",
        "\\[\n",
        "|y - f(x)|_\\epsilon = \\begin{cases}\n",
        "0 & \\text{if } |y - f(x)| \\leq \\epsilon \\\\\n",
        "|y - f(x)| - \\epsilon & \\text{otherwise}\n",
        "\\end{cases}\n",
        "\\]\n",
        "\n",
        "**Epsilon tube**: Predictions within \\(\\epsilon\\) of true value incur no penalty.\n",
        "\n",
        "### 7.1 SVR Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "svr-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X_diabetes = diabetes.data\n",
        "y_diabetes = diabetes.target\n",
        "\n",
        "print(\"Support Vector Regression - Diabetes Dataset\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_diabetes.shape[0]}\")\n",
        "print(f\"Features: {X_diabetes.shape[1]}\")\n",
        "\n",
        "# Split data\n",
        "X_train_diab, X_test_diab, y_train_diab, y_test_diab = train_test_split(\n",
        "    X_diabetes, y_diabetes, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale (IMPORTANT for SVR too!)\n",
        "scaler_diab = StandardScaler()\n",
        "X_train_diab_scaled = scaler_diab.fit_transform(X_train_diab)\n",
        "X_test_diab_scaled = scaler_diab.transform(X_test_diab)\n",
        "\n",
        "# Train SVR with different kernels\n",
        "svr_models = {\n",
        "    'Linear': SVR(kernel='linear', C=1.0),\n",
        "    'RBF': SVR(kernel='rbf', C=1.0, gamma='scale'),\n",
        "    'Polynomial': SVR(kernel='poly', C=1.0, degree=3, gamma='scale')\n",
        "}\n",
        "\n",
        "svr_results = []\n",
        "\n",
        "for name, svr in svr_models.items():\n",
        "    svr.fit(X_train_diab_scaled, y_train_diab)\n",
        "    y_pred = svr.predict(X_test_diab_scaled)\n",
        "    \n",
        "    mse = mean_squared_error(y_test_diab, y_pred)\n",
        "    r2 = r2_score(y_test_diab, y_pred)\n",
        "    \n",
        "    svr_results.append({\n",
        "        'Kernel': name,\n",
        "        'MSE': mse,\n",
        "        'RMSE': np.sqrt(mse),\n",
        "        'R\u00b2 Score': r2\n",
        "    })\n",
        "    \n",
        "    print(f\"{name:15} - R\u00b2: {r2:.4f}, RMSE: {np.sqrt(mse):.2f}\")\n",
        "\n",
        "svr_df = pd.DataFrame(svr_results)\n",
        "print(\"\\n\" + svr_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\ud83d\udca1 SVR can handle regression with kernel trick too!\")\n",
        "print(\"   RBF kernel often performs best for non-linear relationships\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "best-practices",
      "metadata": {},
      "source": [
        "## 8. Best Practices and Guidelines\n",
        "\n",
        "### 8.1 Decision Guide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decision-guide",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"SVM Decision Guide\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "guide = [\n",
        "    {\n",
        "        'Scenario': 'Linearly separable data',\n",
        "        'Model': 'LinearSVC',\n",
        "        'Reason': 'Fastest, simplest, works well'\n",
        "    },\n",
        "    {\n",
        "        'Scenario': 'Non-linear patterns',\n",
        "        'Model': 'SVC with RBF kernel',\n",
        "        'Reason': 'Most flexible, handles complexity'\n",
        "    },\n",
        "    {\n",
        "        'Scenario': 'Large dataset (>10k samples)',\n",
        "        'Model': 'LinearSVC or SGDClassifier',\n",
        "        'Reason': 'Better scalability'\n",
        "    },\n",
        "    {\n",
        "        'Scenario': 'High-dimensional data',\n",
        "        'Model': 'LinearSVC',\n",
        "        'Reason': 'Often linear patterns emerge in high-D'\n",
        "    },\n",
        "    {\n",
        "        'Scenario': 'Need probability estimates',\n",
        "        'Model': 'SVC(probability=True)',\n",
        "        'Reason': 'Enables predict_proba()'\n",
        "    },\n",
        "    {\n",
        "        'Scenario': 'Regression task',\n",
        "        'Model': 'SVR (kernel depends on data)',\n",
        "        'Reason': 'Handles outliers well with epsilon-tube'\n",
        "    },\n",
        "]\n",
        "\n",
        "guide_df = pd.DataFrame(guide)\n",
        "print(guide_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pitfalls",
      "metadata": {},
      "source": [
        "### 8.2 Common Pitfalls and Solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pitfalls-guide",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nCommon Pitfalls with SVM\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "pitfalls = [\n",
        "    (\"\u274c Pitfall\", \"\u2713 Solution\"),\n",
        "    (\"-\" * 40, \"-\" * 40),\n",
        "    (\"Not scaling features\", \"Always use StandardScaler\"),\n",
        "    (\"Using default C and gamma\", \"Grid search to find optimal values\"),\n",
        "    (\"Wrong kernel choice\", \"Try RBF first, then linear if needed\"),\n",
        "    (\"Using SVC on large datasets\", \"Use LinearSVC for >10k samples\"),\n",
        "    (\"Ignoring convergence warnings\", \"Increase max_iter parameter\"),\n",
        "    (\"Not setting random_state\", \"Set for reproducibility\"),\n",
        "    (\"Using probability=True by default\", \"Only use when needed (slower training)\"),\n",
        "    (\"Forgetting to scale test data\", \"Use fitted scaler from training\"),\n",
        "    (\"Overfitting with large gamma\", \"Use cross-validation to tune\"),\n",
        "]\n",
        "\n",
        "for pitfall, solution in pitfalls:\n",
        "    print(f\"{pitfall:<45} {solution}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Summary and Quick Reference\n",
        "\n",
        "### Quick Reference Code\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVC, SVR, LinearSVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ALWAYS SCALE FEATURES FOR SVM!\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ===== CLASSIFICATION =====\n",
        "\n",
        "# Linear SVM (for linearly separable data)\n",
        "svm_linear = LinearSVC(C=1.0, max_iter=10000, random_state=42)\n",
        "\n",
        "# SVC with RBF kernel (most common choice)\n",
        "svm_rbf = SVC(\n",
        "    kernel='rbf',      # 'linear', 'poly', 'rbf', 'sigmoid'\n",
        "    C=1.0,             # Regularization (smaller = softer margin)\n",
        "    gamma='scale',     # Kernel coefficient ('scale', 'auto', or float)\n",
        "    probability=False, # True for predict_proba (slower)\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Polynomial kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3, C=1.0, gamma='scale')\n",
        "\n",
        "# ===== REGRESSION =====\n",
        "\n",
        "# SVR with RBF kernel\n",
        "svr = SVR(\n",
        "    kernel='rbf',\n",
        "    C=1.0,\n",
        "    gamma='scale',\n",
        "    epsilon=0.1  # Width of epsilon-insensitive tube\n",
        ")\n",
        "\n",
        "# Train and predict\n",
        "model.fit(X_train_scaled, y_train)\n",
        "predictions = model.predict(X_test_scaled)\n",
        "```\n",
        "\n",
        "### Key Hyperparameters\n",
        "\n",
        "**C (Regularization)**:\n",
        "- Controls margin width vs misclassification penalty\n",
        "- Small C \u2192 Wider margin, more tolerance (may underfit)\n",
        "- Large C \u2192 Narrower margin, less tolerance (may overfit)\n",
        "- Typical range: [0.01, 0.1, 1, 10, 100]\n",
        "\n",
        "**gamma (RBF/Poly kernel)**:\n",
        "- Controls reach of single training example\n",
        "- Small gamma \u2192 Far reach (smooth, may underfit)\n",
        "- Large gamma \u2192 Close reach (complex, may overfit)\n",
        "- Typical values: [0.001, 0.01, 0.1, 1, 'scale', 'auto']\n",
        "- 'scale': 1 / (n_features * X.var())\n",
        "- 'auto': 1 / n_features\n",
        "\n",
        "**kernel**:\n",
        "- 'linear': Best for linearly separable, high-dimensional data\n",
        "- 'rbf': Best default choice for non-linear\n",
        "- 'poly': For polynomial relationships (specify degree)\n",
        "- 'sigmoid': Rarely used (similar to neural net)\n",
        "\n",
        "### Kernel Selection Strategy\n",
        "\n",
        "```\n",
        "1. Start with LinearSVC\n",
        "   \u251c\u2500 Good performance? \u2192 Done!\n",
        "   \u2514\u2500 Poor performance? \u2192 Try RBF\n",
        "\n",
        "2. Try SVC with RBF kernel\n",
        "   \u251c\u2500 Tune C and gamma with grid search\n",
        "   \u2514\u2500 Usually best for non-linear problems\n",
        "\n",
        "3. If RBF doesn't work well:\n",
        "   \u251c\u2500 Try polynomial kernel\n",
        "   \u2514\u2500 Consider other algorithms (RF, XGBoost)\n",
        "```\n",
        "\n",
        "### Computational Complexity\n",
        "\n",
        "| Model | Training | Prediction | Scalability |\n",
        "|-------|----------|------------|-------------|\n",
        "| LinearSVC | O(n \u00d7 d) | O(d) | Excellent |\n",
        "| SVC | O(n\u00b2 \u00d7 d) to O(n\u00b3 \u00d7 d) | O(n_sv \u00d7 d) | Poor for n>10k |\n",
        "\n",
        "where:\n",
        "- n = number of samples\n",
        "- d = number of features\n",
        "- n_sv = number of support vectors\n",
        "\n",
        "### Feature Scaling Rules\n",
        "\n",
        "\u2713 **Always scale** for SVM (use StandardScaler)\n",
        "\u2713 Fit scaler on training data only\n",
        "\u2713 Transform both train and test with same scaler\n",
        "\u2713 Scale before grid search (include in pipeline)\n",
        "\n",
        "### SVM Strengths\n",
        "\n",
        "\u2713 Effective in high-dimensional spaces\n",
        "\u2713 Memory efficient (only stores support vectors)\n",
        "\u2713 Versatile (different kernels)\n",
        "\u2713 Works well with clear margin of separation\n",
        "\u2713 Robust to overfitting in high dimensions\n",
        "\n",
        "### SVM Limitations\n",
        "\n",
        "\u2717 Poor scalability to large datasets (n > 10k)\n",
        "\u2717 Sensitive to feature scaling\n",
        "\u2717 No probabilistic interpretation (needs probability=True)\n",
        "\u2717 Difficult to interpret (black box)\n",
        "\u2717 Sensitive to noise (use soft margin)\n",
        "\u2717 Slow training with RBF kernel\n",
        "\n",
        "### When to Use SVM\n",
        "\n",
        "\u2713 **Use SVM when:**\n",
        "- Small to medium datasets (<10k samples)\n",
        "- High-dimensional data\n",
        "- Clear margin of separation\n",
        "- Need robust classifier\n",
        "- Text classification, image classification\n",
        "\n",
        "\u2717 **Avoid SVM when:**\n",
        "- Very large datasets (>100k samples)\n",
        "- Need probability estimates (or use probability=True)\n",
        "- Need interpretability\n",
        "- Many noisy features\n",
        "\n",
        "### Comparison with Other Algorithms\n",
        "\n",
        "| Algorithm | Pros over SVM | Cons vs SVM |\n",
        "|-----------|---------------|-------------|\n",
        "| Logistic Regression | Faster, probabilistic, interpretable | Only linear |\n",
        "| Random Forest | Handles non-linearity, faster, no scaling | Less accurate margin |\n",
        "| XGBoost | Better performance, faster | Less theoretical foundation |\n",
        "| Neural Networks | More flexible, scalable | Needs more data, harder to tune |\n",
        "\n",
        "### Further Reading\n",
        "\n",
        "- **Original Paper**: Vapnik (1995) - \"The Nature of Statistical Learning Theory\"\n",
        "- **Tutorial**: \"A Practical Guide to SVM\" by Hsu et al.\n",
        "- **Book**: \"Learning with Kernels\" by Sch\u00f6lkopf & Smola\n",
        "- **sklearn Documentation**: https://scikit-learn.org/stable/modules/svm.html\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Custom kernels\n",
        "- One-Class SVM (anomaly detection)\n",
        "- Nu-SVM variants\n",
        "- Kernel approximation for large datasets\n",
        "- SVM for structured prediction"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}