{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# K-Nearest Neighbors (KNN) and Radius Neighbors\n",
        "\n",
        "## Overview\n",
        "\n",
        "**K-Nearest Neighbors (KNN)** is a simple, intuitive, non-parametric algorithm that makes predictions based on the most similar examples in the training data.\n",
        "\n",
        "### Core Concept\n",
        "\n",
        "*\"You are the average of your k closest neighbors\"*\n",
        "\n",
        "### Key Ideas\n",
        "\n",
        "1. **Instance-Based Learning**: No explicit training phase - stores all data\n",
        "2. **Lazy Learning**: Computation happens at prediction time\n",
        "3. **Distance-Based**: Uses distance metrics to find neighbors\n",
        "4. **Non-Parametric**: Makes no assumptions about data distribution\n",
        "\n",
        "## Mathematical Foundation\n",
        "\n",
        "### Classification\n",
        "\n",
        "**Prediction**: Majority vote among k nearest neighbors\n",
        "\\[\n",
        "\\hat{y} = \\text{mode}\\{y_1, y_2, ..., y_k\\}\n",
        "\\]\n",
        "\n",
        "Or with distance weighting:\n",
        "\\[\n",
        "\\hat{y} = \\arg\\max_c \\sum_{i \\in N_k(x)} w_i \\cdot \\mathbb{1}(y_i = c)\n",
        "\\]\n",
        "\n",
        "where \\(w_i = \\frac{1}{d(x, x_i)}\\) or \\(w_i = \\frac{1}{d(x, x_i)^2}\\)\n",
        "\n",
        "### Regression\n",
        "\n",
        "**Prediction**: Average of k nearest neighbors\n",
        "\\[\n",
        "\\hat{y} = \\frac{1}{k}\\sum_{i=1}^{k} y_i\n",
        "\\]\n",
        "\n",
        "Or with distance weighting:\n",
        "\\[\n",
        "\\hat{y} = \\frac{\\sum_{i \\in N_k(x)} w_i \\cdot y_i}{\\sum_{i \\in N_k(x)} w_i}\n",
        "\\]\n",
        "\n",
        "### Distance Metrics\n",
        "\n",
        "**Euclidean (L2)**:\n",
        "\\[\n",
        "d(x, x') = \\sqrt{\\sum_{j=1}^{d}(x_j - x'_j)^2}\n",
        "\\]\n",
        "\n",
        "**Manhattan (L1)**:\n",
        "\\[\n",
        "d(x, x') = \\sum_{j=1}^{d}|x_j - x'_j|\n",
        "\\]\n",
        "\n",
        "**Minkowski (general)**:\n",
        "\\[\n",
        "d(x, x') = \\left(\\sum_{j=1}^{d}|x_j - x'_j|^p\\right)^{1/p}\n",
        "\\]\n",
        "\n",
        "## Topics Covered\n",
        "\n",
        "1. KNN for classification and regression\n",
        "2. Effect of k parameter\n",
        "3. Distance metrics and their impact\n",
        "4. Weighted vs uniform voting\n",
        "5. Feature scaling importance\n",
        "6. Curse of dimensionality\n",
        "7. Radius Neighbors (fixed-radius approach)\n",
        "8. Computational efficiency and optimization\n",
        "9. Best practices and practical applications"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from time import time\n",
        "\n",
        "# KNN models\n",
        "from sklearn.neighbors import (\n",
        "    KNeighborsClassifier, KNeighborsRegressor,\n",
        "    RadiusNeighborsClassifier, RadiusNeighborsRegressor,\n",
        "    NearestNeighbors\n",
        ")\n",
        "\n",
        "# Other models for comparison\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "\n",
        "# Utilities\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, cross_val_score, GridSearchCV,\n",
        "    learning_curve, validation_curve\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, confusion_matrix,\n",
        "    mean_squared_error, r2_score, mean_absolute_error\n",
        ")\n",
        "from sklearn.datasets import (\n",
        "    make_classification, make_regression, make_moons,\n",
        "    load_breast_cancer, load_wine, load_diabetes, load_iris\n",
        ")\n",
        "\n",
        "np.random.seed(42)\n",
        "sns.set_style('whitegrid')\n",
        "print(\"\u2713 Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "knn-intuition",
      "metadata": {},
      "source": [
        "## 1. KNN Intuition and Visualization\n",
        "\n",
        "### 1.1 How KNN Works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "knn-visual",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate simple 2D dataset\n",
        "X_simple, y_simple = make_classification(\n",
        "    n_samples=100, n_features=2, n_redundant=0,\n",
        "    n_informative=2, n_clusters_per_class=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"KNN Classification Visualization\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_simple.shape[0]}\")\n",
        "print(f\"Features: {X_simple.shape[1]}\")\n",
        "print(f\"Classes: {np.unique(y_simple)}\")\n",
        "\n",
        "# Train KNN\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_simple, y_simple)\n",
        "\n",
        "# Create mesh for decision boundary\n",
        "h = 0.02\n",
        "x_min, x_max = X_simple[:, 0].min() - 1, X_simple[:, 0].max() + 1\n",
        "y_min, y_max = X_simple[:, 1].min() - 1, X_simple[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Decision boundary\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
        "plt.scatter(X_simple[:, 0], X_simple[:, 1], c=y_simple,\n",
        "           cmap='RdYlBu', edgecolors='k', s=50)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('KNN Decision Boundary (k=5)')\n",
        "\n",
        "# Demonstrate nearest neighbors for a test point\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_simple[:, 0], X_simple[:, 1], c=y_simple,\n",
        "           cmap='RdYlBu', edgecolors='k', s=50, alpha=0.6)\n",
        "\n",
        "# Pick a test point\n",
        "test_point = np.array([[0, 0]])\n",
        "plt.scatter(test_point[0, 0], test_point[0, 1], \n",
        "           marker='*', s=500, c='green', edgecolors='black', linewidths=2,\n",
        "           label='Test Point', zorder=10)\n",
        "\n",
        "# Find k nearest neighbors\n",
        "distances, indices = knn.kneighbors(test_point)\n",
        "neighbors = X_simple[indices[0]]\n",
        "\n",
        "# Highlight neighbors\n",
        "plt.scatter(neighbors[:, 0], neighbors[:, 1],\n",
        "           s=200, facecolors='none', edgecolors='green', linewidths=2,\n",
        "           label='5 Nearest Neighbors')\n",
        "\n",
        "# Draw lines to neighbors\n",
        "for neighbor in neighbors:\n",
        "    plt.plot([test_point[0, 0], neighbor[0]], \n",
        "            [test_point[0, 1], neighbor[1]],\n",
        "            'g--', alpha=0.5, linewidth=1)\n",
        "\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('KNN: Finding 5 Nearest Neighbors')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Show prediction\n",
        "prediction = knn.predict(test_point)[0]\n",
        "neighbor_labels = y_simple[indices[0]]\n",
        "\n",
        "print(f\"\\nTest Point: {test_point[0]}\")\n",
        "print(f\"Neighbor Labels: {neighbor_labels}\")\n",
        "print(f\"Majority Vote: {prediction}\")\n",
        "print(f\"Distances to neighbors: {distances[0]}\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 KNN predicts based on majority vote of k nearest neighbors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k-parameter",
      "metadata": {},
      "source": [
        "## 2. Effect of k Parameter\n",
        "\n",
        "### 2.1 Visualizing Different k Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k-parameter-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Effect of k Parameter\")\n",
        "print(\"=\"*70)\n",
        "print(\"k = Number of neighbors to consider\")\n",
        "print(\"  - Small k: More complex boundary (may overfit)\")\n",
        "print(\"  - Large k: Smoother boundary (may underfit)\\n\")\n",
        "\n",
        "# Test different k values\n",
        "k_values = [1, 5, 20, 50]\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, k in enumerate(k_values):\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_simple, y_simple)\n",
        "    \n",
        "    # Decision boundary\n",
        "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
        "    axes[idx].scatter(X_simple[:, 0], X_simple[:, 1], c=y_simple,\n",
        "                     cmap='RdYlBu', edgecolors='k', s=50)\n",
        "    \n",
        "    accuracy = knn.score(X_simple, y_simple)\n",
        "    axes[idx].set_title(f'k = {k}\\nTraining Accuracy: {accuracy:.3f}')\n",
        "    axes[idx].set_xlabel('Feature 1')\n",
        "    axes[idx].set_ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Observations:\")\n",
        "print(\"   k=1: Very complex boundary, perfect training accuracy (overfits!)\")\n",
        "print(\"   k=5-20: Balanced complexity\")\n",
        "print(\"   k=50: Very smooth boundary (may underfit)\")\n",
        "print(\"\\n   Use cross-validation to find optimal k!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k-tuning",
      "metadata": {},
      "source": [
        "### 2.2 Finding Optimal k with Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k-tuning-cv",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load iris dataset for more robust example\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target\n",
        "\n",
        "print(\"Finding Optimal k - Iris Dataset\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
        ")\n",
        "\n",
        "# Scale features (important for KNN!)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Test different k values\n",
        "k_range = range(1, 31)\n",
        "train_scores = []\n",
        "cv_scores = []\n",
        "test_scores = []\n",
        "\n",
        "for k in k_range:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    \n",
        "    # Train score\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    train_scores.append(knn.score(X_train_scaled, y_train))\n",
        "    \n",
        "    # CV score\n",
        "    cv_score = cross_val_score(knn, X_train_scaled, y_train, cv=5).mean()\n",
        "    cv_scores.append(cv_score)\n",
        "    \n",
        "    # Test score\n",
        "    test_scores.append(knn.score(X_test_scaled, y_test))\n",
        "\n",
        "# Find best k\n",
        "best_k = k_range[np.argmax(cv_scores)]\n",
        "best_cv_score = max(cv_scores)\n",
        "\n",
        "print(f\"Best k (by CV): {best_k}\")\n",
        "print(f\"Best CV Score: {best_cv_score:.4f}\")\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_range, train_scores, 'o-', label='Training Score', linewidth=2)\n",
        "plt.plot(k_range, cv_scores, 's-', label='CV Score', linewidth=2)\n",
        "plt.plot(k_range, test_scores, '^-', label='Test Score', linewidth=2)\n",
        "plt.axvline(x=best_k, color='red', linestyle='--', alpha=0.5, \n",
        "           label=f'Best k={best_k}')\n",
        "plt.xlabel('Number of Neighbors (k)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('KNN: Finding Optimal k')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Key Insights:\")\n",
        "print(\"   - Training accuracy decreases as k increases\")\n",
        "print(\"   - CV score shows bias-variance tradeoff\")\n",
        "print(\"   - Choose k where CV score is maximized\")\n",
        "print(\"   - Typically k = 3-10 works well for many problems\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "distance-metrics",
      "metadata": {},
      "source": [
        "## 3. Distance Metrics\n",
        "\n",
        "### 3.1 Comparing Different Distance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "distance-metrics-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Distance Metrics Comparison\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Distance metrics to test\n",
        "metrics = {\n",
        "    'euclidean': 'Euclidean (L2)',\n",
        "    'manhattan': 'Manhattan (L1)',\n",
        "    'chebyshev': 'Chebyshev (L\u221e)',\n",
        "    'minkowski': 'Minkowski (p=3)'\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for metric_name, metric_label in metrics.items():\n",
        "    if metric_name == 'minkowski':\n",
        "        knn = KNeighborsClassifier(n_neighbors=5, metric=metric_name, p=3)\n",
        "    else:\n",
        "        knn = KNeighborsClassifier(n_neighbors=5, metric=metric_name)\n",
        "    \n",
        "    # CV score\n",
        "    cv_score = cross_val_score(knn, X_train_scaled, y_train, cv=5).mean()\n",
        "    \n",
        "    # Train and test\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    test_score = knn.score(X_test_scaled, y_test)\n",
        "    \n",
        "    results.append({\n",
        "        'Metric': metric_label,\n",
        "        'CV Score': cv_score,\n",
        "        'Test Score': test_score\n",
        "    })\n",
        "    \n",
        "    print(f\"{metric_label:20} - CV: {cv_score:.4f}, Test: {test_score:.4f}\")\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "ax.bar(x - width/2, results_df['CV Score'], width, label='CV Score', alpha=0.8)\n",
        "ax.bar(x + width/2, results_df['Test Score'], width, label='Test Score', alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('Distance Metric')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('KNN Performance with Different Distance Metrics')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(results_df['Metric'], rotation=15, ha='right')\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Distance Metric Notes:\")\n",
        "print(\"   Euclidean: Most common, good default\")\n",
        "print(\"   Manhattan: Better for high-dimensional sparse data\")\n",
        "print(\"   Chebyshev: Maximum difference across dimensions\")\n",
        "print(\"   Minkowski: Generalization (p=1\u2192Manhattan, p=2\u2192Euclidean)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "weighting",
      "metadata": {},
      "source": [
        "## 4. Uniform vs Distance-Weighted Voting\n",
        "\n",
        "### 4.1 Comparing Weighting Strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "weighting-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Uniform vs Distance-Weighted Voting\")\n",
        "print(\"=\"*70)\n",
        "print(\"uniform: All neighbors have equal vote\")\n",
        "print(\"distance: Closer neighbors have more influence (1/distance)\\n\")\n",
        "\n",
        "weights_options = ['uniform', 'distance']\n",
        "k_values = [1, 3, 5, 10, 20]\n",
        "\n",
        "uniform_scores = []\n",
        "distance_scores = []\n",
        "\n",
        "for k in k_values:\n",
        "    # Uniform\n",
        "    knn_uniform = KNeighborsClassifier(n_neighbors=k, weights='uniform')\n",
        "    uniform_score = cross_val_score(knn_uniform, X_train_scaled, y_train, cv=5).mean()\n",
        "    uniform_scores.append(uniform_score)\n",
        "    \n",
        "    # Distance-weighted\n",
        "    knn_distance = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
        "    distance_score = cross_val_score(knn_distance, X_train_scaled, y_train, cv=5).mean()\n",
        "    distance_scores.append(distance_score)\n",
        "    \n",
        "    print(f\"k={k:2} - Uniform: {uniform_score:.4f}, Distance: {distance_score:.4f}\")\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_values, uniform_scores, 'o-', label='Uniform Weighting', linewidth=2)\n",
        "plt.plot(k_values, distance_scores, 's-', label='Distance Weighting', linewidth=2)\n",
        "plt.xlabel('Number of Neighbors (k)')\n",
        "plt.ylabel('CV Accuracy')\n",
        "plt.title('Uniform vs Distance-Weighted KNN')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 When to use distance weighting:\")\n",
        "print(\"   - When closer neighbors should have more influence\")\n",
        "print(\"   - Helps with larger k values\")\n",
        "print(\"   - Can prevent ties in voting\")\n",
        "print(\"   - Note: k=1 gives same result for both\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "scaling-importance",
      "metadata": {},
      "source": [
        "## 5. Importance of Feature Scaling\n",
        "\n",
        "### 5.1 KNN with and without Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "scaling-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset with different scales\n",
        "np.random.seed(42)\n",
        "X_unscaled = np.random.randn(200, 2)\n",
        "X_unscaled[:, 0] *= 10  # Feature 1: large scale\n",
        "X_unscaled[:, 1] *= 0.5  # Feature 2: small scale\n",
        "y_unscaled = (X_unscaled[:, 0] + 5 * X_unscaled[:, 1] > 0).astype(int)\n",
        "\n",
        "print(\"Importance of Feature Scaling for KNN\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Feature 1 range: [{X_unscaled[:, 0].min():.2f}, {X_unscaled[:, 0].max():.2f}]\")\n",
        "print(f\"Feature 2 range: [{X_unscaled[:, 1].min():.2f}, {X_unscaled[:, 1].max():.2f}]\")\n",
        "print(f\"\\nFeature 1 is ~20x larger scale than Feature 2!\\n\")\n",
        "\n",
        "# Scale data\n",
        "scaler_demo = StandardScaler()\n",
        "X_scaled_demo = scaler_demo.fit_transform(X_unscaled)\n",
        "\n",
        "# Train KNN\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "knn_unscaled.fit(X_unscaled, y_unscaled)\n",
        "knn_scaled.fit(X_scaled_demo, y_unscaled)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "for idx, (X, knn, title) in enumerate([\n",
        "    (X_unscaled, knn_unscaled, 'Without Scaling'),\n",
        "    (X_scaled_demo, knn_scaled, 'With Scaling')\n",
        "]):\n",
        "    h = 0.5 if idx == 0 else 0.02\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
        "    axes[idx].scatter(X[:, 0], X[:, 1], c=y_unscaled,\n",
        "                     cmap='RdYlBu', edgecolors='k', s=50)\n",
        "    \n",
        "    accuracy = knn.score(X, y_unscaled)\n",
        "    axes[idx].set_title(f'{title}\\nAccuracy: {accuracy:.3f}')\n",
        "    axes[idx].set_xlabel('Feature 1')\n",
        "    axes[idx].set_ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 CRITICAL: Always scale features for KNN!\")\n",
        "print(\"   KNN uses distance \u2192 large-scale features dominate\")\n",
        "print(\"   Without scaling, Feature 1 dominates distance calculation\")\n",
        "print(\"   With scaling, both features contribute equally\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "knn-regression",
      "metadata": {},
      "source": [
        "## 6. KNN for Regression\n",
        "\n",
        "### 6.1 Regression Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "knn-regression-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X_diabetes = diabetes.data\n",
        "y_diabetes = diabetes.target\n",
        "\n",
        "print(\"KNN Regression - Diabetes Dataset\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_diabetes.shape[0]}\")\n",
        "print(f\"Features: {X_diabetes.shape[1]}\")\n",
        "\n",
        "# Split\n",
        "X_train_diab, X_test_diab, y_train_diab, y_test_diab = train_test_split(\n",
        "    X_diabetes, y_diabetes, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale\n",
        "scaler_diab = StandardScaler()\n",
        "X_train_diab_scaled = scaler_diab.fit_transform(X_train_diab)\n",
        "X_test_diab_scaled = scaler_diab.transform(X_test_diab)\n",
        "\n",
        "# Test different k values\n",
        "k_values_reg = [1, 3, 5, 10, 20, 50]\n",
        "reg_results = []\n",
        "\n",
        "for k in k_values_reg:\n",
        "    knn_reg = KNeighborsRegressor(n_neighbors=k)\n",
        "    knn_reg.fit(X_train_diab_scaled, y_train_diab)\n",
        "    \n",
        "    y_pred = knn_reg.predict(X_test_diab_scaled)\n",
        "    \n",
        "    mse = mean_squared_error(y_test_diab, y_pred)\n",
        "    mae = mean_absolute_error(y_test_diab, y_pred)\n",
        "    r2 = r2_score(y_test_diab, y_pred)\n",
        "    \n",
        "    reg_results.append({\n",
        "        'k': k,\n",
        "        'MSE': mse,\n",
        "        'MAE': mae,\n",
        "        'R\u00b2': r2\n",
        "    })\n",
        "    \n",
        "    print(f\"k={k:2} - R\u00b2: {r2:.4f}, RMSE: {np.sqrt(mse):.2f}, MAE: {mae:.2f}\")\n",
        "\n",
        "reg_df = pd.DataFrame(reg_results)\n",
        "\n",
        "# Plot R\u00b2 vs k\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(reg_df['k'], reg_df['R\u00b2'], 'o-', linewidth=2, markersize=8)\n",
        "plt.xlabel('Number of Neighbors (k)')\n",
        "plt.ylabel('R\u00b2 Score')\n",
        "plt.title('KNN Regression: R\u00b2 Score vs k')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "best_k = reg_df.loc[reg_df['R\u00b2'].idxmax(), 'k']\n",
        "best_r2 = reg_df['R\u00b2'].max()\n",
        "\n",
        "print(f\"\\nBest k: {best_k:.0f}\")\n",
        "print(f\"Best R\u00b2: {best_r2:.4f}\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 KNN Regression: Predicts average of k nearest neighbors\")\n",
        "print(\"   Larger k \u2192 Smoother predictions\")\n",
        "print(\"   Smaller k \u2192 More localized predictions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "radius-neighbors",
      "metadata": {},
      "source": [
        "## 7. Radius Neighbors\n",
        "\n",
        "### Concept\n",
        "\n",
        "**Radius Neighbors**: Instead of finding k nearest neighbors, find all neighbors within a fixed radius.\n",
        "\n",
        "**Advantages**:\n",
        "- Adapts to local density\n",
        "- More neighbors in dense regions, fewer in sparse regions\n",
        "\n",
        "**Disadvantages**:\n",
        "- Radius must be chosen carefully\n",
        "- May have no neighbors (radius too small)\n",
        "- May have too many neighbors (radius too large)\n",
        "\n",
        "### 7.1 Radius Neighbors Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "radius-neighbors-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Radius Neighbors Classifier\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Use breast cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X_cancer = cancer.data\n",
        "y_cancer = cancer.target\n",
        "\n",
        "# Split and scale\n",
        "X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = train_test_split(\n",
        "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
        ")\n",
        "\n",
        "scaler_cancer = StandardScaler()\n",
        "X_train_cancer_scaled = scaler_cancer.fit_transform(X_train_cancer)\n",
        "X_test_cancer_scaled = scaler_cancer.transform(X_test_cancer)\n",
        "\n",
        "# Test different radius values\n",
        "radius_values = [0.5, 1.0, 2.0, 5.0, 10.0]\n",
        "radius_results = []\n",
        "\n",
        "for radius in radius_values:\n",
        "    try:\n",
        "        rad_clf = RadiusNeighborsClassifier(radius=radius, weights='distance',\n",
        "                                            outlier_label='most_frequent')\n",
        "        rad_clf.fit(X_train_cancer_scaled, y_train_cancer)\n",
        "        \n",
        "        y_pred = rad_clf.predict(X_test_cancer_scaled)\n",
        "        accuracy = accuracy_score(y_test_cancer, y_pred)\n",
        "        \n",
        "        # Count average neighbors\n",
        "        sample_neighbors = rad_clf.radius_neighbors(X_test_cancer_scaled[:10])\n",
        "        avg_neighbors = np.mean([len(n) for n in sample_neighbors[1]])\n",
        "        \n",
        "        radius_results.append({\n",
        "            'Radius': radius,\n",
        "            'Accuracy': accuracy,\n",
        "            'Avg Neighbors': avg_neighbors\n",
        "        })\n",
        "        \n",
        "        print(f\"Radius={radius:5.1f} - Accuracy: {accuracy:.4f}, Avg Neighbors: {avg_neighbors:.1f}\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Radius={radius:5.1f} - Error: {str(e)[:50]}...\")\n",
        "\n",
        "# Compare with KNN\n",
        "knn_cancer = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_cancer.fit(X_train_cancer_scaled, y_train_cancer)\n",
        "knn_accuracy = knn_cancer.score(X_test_cancer_scaled, y_test_cancer)\n",
        "\n",
        "print(f\"\\nKNN (k=5) Accuracy: {knn_accuracy:.4f}\")\n",
        "\n",
        "if radius_results:\n",
        "    radius_df = pd.DataFrame(radius_results)\n",
        "    best_radius = radius_df.loc[radius_df['Accuracy'].idxmax()]\n",
        "    print(f\"\\nBest Radius: {best_radius['Radius']}\")\n",
        "    print(f\"Best Accuracy: {best_radius['Accuracy']:.4f}\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Radius Neighbors:\")\n",
        "print(\"   - Use when local density varies significantly\")\n",
        "print(\"   - Requires careful radius tuning\")\n",
        "print(\"   - KNN is usually more robust\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "curse-of-dimensionality",
      "metadata": {},
      "source": [
        "## 8. Curse of Dimensionality\n",
        "\n",
        "### 8.1 KNN Performance vs Number of Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "curse-dimensionality-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Curse of Dimensionality\")\n",
        "print(\"=\"*70)\n",
        "print(\"As dimensions increase, distance between points becomes less meaningful\\n\")\n",
        "\n",
        "# Generate datasets with different dimensions\n",
        "n_samples = 200\n",
        "dimensions = [2, 5, 10, 20, 50, 100]\n",
        "results_dim = []\n",
        "\n",
        "for n_features in dimensions:\n",
        "    X, y = make_classification(\n",
        "        n_samples=n_samples,\n",
        "        n_features=n_features,\n",
        "        n_informative=min(n_features, 10),\n",
        "        n_redundant=0,\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42\n",
        "    )\n",
        "    \n",
        "    # Scale\n",
        "    scaler_dim = StandardScaler()\n",
        "    X_train_scaled = scaler_dim.fit_transform(X_train)\n",
        "    X_test_scaled = scaler_dim.transform(X_test)\n",
        "    \n",
        "    # Train KNN\n",
        "    knn = KNeighborsClassifier(n_neighbors=5)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Evaluate\n",
        "    cv_score = cross_val_score(knn, X_train_scaled, y_train, cv=5).mean()\n",
        "    test_score = knn.score(X_test_scaled, y_test)\n",
        "    \n",
        "    # Calculate average distance to nearest neighbor\n",
        "    distances, _ = knn.kneighbors(X_test_scaled)\n",
        "    avg_distance = distances[:, 1:].mean()  # Exclude self (distance 0)\n",
        "    \n",
        "    results_dim.append({\n",
        "        'Dimensions': n_features,\n",
        "        'CV Score': cv_score,\n",
        "        'Test Score': test_score,\n",
        "        'Avg Distance': avg_distance\n",
        "    })\n",
        "    \n",
        "    print(f\"Dimensions: {n_features:3d} - CV: {cv_score:.4f}, \"\n",
        "          f\"Test: {test_score:.4f}, Avg Distance: {avg_distance:.2f}\")\n",
        "\n",
        "dim_df = pd.DataFrame(results_dim)\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy vs dimensions\n",
        "axes[0].plot(dim_df['Dimensions'], dim_df['CV Score'], 'o-', label='CV Score', linewidth=2)\n",
        "axes[0].plot(dim_df['Dimensions'], dim_df['Test Score'], 's-', label='Test Score', linewidth=2)\n",
        "axes[0].set_xlabel('Number of Dimensions')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('KNN Performance vs Dimensionality')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Average distance vs dimensions\n",
        "axes[1].plot(dim_df['Dimensions'], dim_df['Avg Distance'], 'o-', linewidth=2, color='red')\n",
        "axes[1].set_xlabel('Number of Dimensions')\n",
        "axes[1].set_ylabel('Average Distance to Nearest Neighbor')\n",
        "axes[1].set_title('Distance Increase with Dimensionality')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Curse of Dimensionality:\")\n",
        "print(\"   - Performance degrades in high dimensions\")\n",
        "print(\"   - Points become equidistant (distance less meaningful)\")\n",
        "print(\"   - Need exponentially more data as dimensions increase\")\n",
        "print(\"   \\n   Solutions:\")\n",
        "print(\"   - Feature selection/dimensionality reduction (PCA)\")\n",
        "print(\"   - Use algorithms less affected (tree-based, linear models)\")\n",
        "print(\"   - Increase sample size significantly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "computational-efficiency",
      "metadata": {},
      "source": [
        "## 9. Computational Efficiency\n",
        "\n",
        "### 9.1 Training vs Prediction Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efficiency-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Computational Efficiency: KNN vs Other Algorithms\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Use larger dataset\n",
        "X_large, y_large = make_classification(\n",
        "    n_samples=5000, n_features=20, random_state=42\n",
        ")\n",
        "\n",
        "X_train_large, X_test_large, y_train_large, y_test_large = train_test_split(\n",
        "    X_large, y_large, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "scaler_large = StandardScaler()\n",
        "X_train_large_scaled = scaler_large.fit_transform(X_train_large)\n",
        "X_test_large_scaled = scaler_large.transform(X_test_large)\n",
        "\n",
        "# Models to compare\n",
        "models = {\n",
        "    'KNN (k=5)': KNeighborsClassifier(n_neighbors=5),\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "efficiency_results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Training time\n",
        "    start = time()\n",
        "    model.fit(X_train_large_scaled, y_train_large)\n",
        "    train_time = time() - start\n",
        "    \n",
        "    # Prediction time\n",
        "    start = time()\n",
        "    y_pred = model.predict(X_test_large_scaled)\n",
        "    predict_time = time() - start\n",
        "    \n",
        "    # Accuracy\n",
        "    accuracy = accuracy_score(y_test_large, y_pred)\n",
        "    \n",
        "    efficiency_results.append({\n",
        "        'Model': name,\n",
        "        'Train Time (s)': train_time,\n",
        "        'Predict Time (s)': predict_time,\n",
        "        'Accuracy': accuracy\n",
        "    })\n",
        "    \n",
        "    print(f\"{name:25} - Train: {train_time:.4f}s, Predict: {predict_time:.4f}s, \"\n",
        "          f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "eff_df = pd.DataFrame(efficiency_results)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Training time\n",
        "axes[0].barh(eff_df['Model'], eff_df['Train Time (s)'], alpha=0.8)\n",
        "axes[0].set_xlabel('Training Time (seconds)')\n",
        "axes[0].set_title('Training Time Comparison')\n",
        "axes[0].grid(alpha=0.3, axis='x')\n",
        "\n",
        "# Prediction time\n",
        "axes[1].barh(eff_df['Model'], eff_df['Predict Time (s)'], alpha=0.8, color='orange')\n",
        "axes[1].set_xlabel('Prediction Time (seconds)')\n",
        "axes[1].set_title('Prediction Time Comparison')\n",
        "axes[1].grid(alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 KNN Efficiency Characteristics:\")\n",
        "print(\"   Training: O(1) - Just stores data (fastest!)\")\n",
        "print(\"   Prediction: O(n*d) - Must compare to all training samples (slowest!)\")\n",
        "print(\"   \\n   Improvements:\")\n",
        "print(\"   - Use KD-Tree or Ball-Tree for faster neighbor search\")\n",
        "print(\"   - algorithm='auto' chooses best structure automatically\")\n",
        "print(\"   - Consider approximate methods for very large datasets\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "algorithm-parameter",
      "metadata": {},
      "source": [
        "### 9.2 Neighbor Search Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "algorithm-comparison",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Neighbor Search Algorithm Comparison\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "algorithms = ['brute', 'kd_tree', 'ball_tree', 'auto']\n",
        "algo_results = []\n",
        "\n",
        "for algo in algorithms:\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, algorithm=algo)\n",
        "    \n",
        "    # Training (building tree structure)\n",
        "    start = time()\n",
        "    knn.fit(X_train_large_scaled, y_train_large)\n",
        "    train_time = time() - start\n",
        "    \n",
        "    # Prediction\n",
        "    start = time()\n",
        "    y_pred = knn.predict(X_test_large_scaled)\n",
        "    predict_time = time() - start\n",
        "    \n",
        "    accuracy = accuracy_score(y_test_large, y_pred)\n",
        "    \n",
        "    algo_results.append({\n",
        "        'Algorithm': algo,\n",
        "        'Train Time (s)': train_time,\n",
        "        'Predict Time (s)': predict_time,\n",
        "        'Accuracy': accuracy\n",
        "    })\n",
        "    \n",
        "    print(f\"{algo:15} - Train: {train_time:.4f}s, Predict: {predict_time:.4f}s\")\n",
        "\n",
        "algo_df = pd.DataFrame(algo_results)\n",
        "print(\"\\n\" + algo_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Algorithm Selection:\")\n",
        "print(\"   brute: O(n*d) - Checks all points, slow but always works\")\n",
        "print(\"   kd_tree: O(log n) - Fast for low dimensions (d < 20)\")\n",
        "print(\"   ball_tree: O(log n) - Better for high dimensions\")\n",
        "print(\"   auto: Chooses best based on data characteristics\")\n",
        "print(\"   \\n   Recommendation: Use 'auto' (default)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "best-practices",
      "metadata": {},
      "source": [
        "## 10. Best Practices and Guidelines\n",
        "\n",
        "### 10.1 Decision Guide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decision-guide",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"KNN Decision Guide\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "guide = [\n",
        "    {\n",
        "        'Scenario': 'Small dataset (<1000 samples)',\n",
        "        'Recommendation': 'KNN works well',\n",
        "        'Note': 'Good baseline, easy to implement'\n",
        "    },\n",
        "    {\n",
        "        'Scenario': 'Large dataset (>10k samples)',\n",
        "        'Recommendation': 'Consider other algorithms',\n",
        "        'Note': 'Prediction time becomes bottleneck'\n",
        "    },\n",
        "    {\n",
        "        'Scenario': 'Low dimensions (d < 20)',\n",
        "        'Recommendation': 'KNN performs well',\n",
        "        'Note': 'Use kd_tree for speed'\n",
        "    },\n",
        "    {\n",
        "        'Scenario': 'High dimensions (d > 50)',\n",
        "        'Recommendation': 'Apply dimensionality reduction first',\n",
        "        'Note': 'Or use algorithms designed for high-D'\n",
        "    },\n",
        "    {\n",
        "        'Scenario': 'Non-linear decision boundary',\n",
        "        'Recommendation': 'KNN handles well',\n",
        "        'Note': 'No assumptions about data distribution'\n",
        "    },\n",
        "    {\n",
        "        'Scenario': 'Need fast predictions',\n",
        "        'Recommendation': 'Avoid KNN',\n",
        "        'Note': 'Use parametric models instead'\n",
        "    },\n",
        "    {\n",
        "        'Scenario': 'Imbalanced classes',\n",
        "        'Recommendation': 'Use weighted KNN',\n",
        "        'Note': 'Or combine with resampling'\n",
        "    },\n",
        "]\n",
        "\n",
        "guide_df = pd.DataFrame(guide)\n",
        "print(guide_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pitfalls",
      "metadata": {},
      "source": [
        "### 10.2 Common Pitfalls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pitfalls-guide",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nCommon Pitfalls with KNN\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "pitfalls = [\n",
        "    (\"\u274c Pitfall\", \"\u2713 Solution\"),\n",
        "    (\"-\" * 40, \"-\" * 40),\n",
        "    (\"Not scaling features\", \"Always use StandardScaler\"),\n",
        "    (\"Using default k=5 without tuning\", \"Cross-validate to find optimal k\"),\n",
        "    (\"Applying to high-dimensional data\", \"Use PCA or feature selection first\"),\n",
        "    (\"Using KNN on large datasets\", \"Consider other algorithms or sampling\"),\n",
        "    (\"Ignoring computational cost\", \"Profile prediction time for production\"),\n",
        "    (\"Not considering distance metric\", \"Try Manhattan for high-D sparse data\"),\n",
        "    (\"Using uniform weights always\", \"Try distance weighting for larger k\"),\n",
        "    (\"Forgetting algorithm parameter\", \"Use algorithm='auto' for optimization\"),\n",
        "    (\"Treating all features equally\", \"Consider feature weighting or selection\"),\n",
        "]\n",
        "\n",
        "for pitfall, solution in pitfalls:\n",
        "    print(f\"{pitfall:<45} {solution}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Summary and Quick Reference\n",
        "\n",
        "### Quick Reference Code\n",
        "\n",
        "```python\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ALWAYS SCALE FEATURES!\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ===== CLASSIFICATION =====\n",
        "\n",
        "knn_clf = KNeighborsClassifier(\n",
        "    n_neighbors=5,       # Number of neighbors (tune with CV!)\n",
        "    weights='uniform',   # 'uniform' or 'distance'\n",
        "    algorithm='auto',    # 'auto', 'brute', 'kd_tree', 'ball_tree'\n",
        "    metric='minkowski',  # Distance metric\n",
        "    p=2,                 # Power for Minkowski (2=Euclidean)\n",
        "    n_jobs=-1            # Parallel processing\n",
        ")\n",
        "\n",
        "# ===== REGRESSION =====\n",
        "\n",
        "knn_reg = KNeighborsRegressor(\n",
        "    n_neighbors=5,\n",
        "    weights='uniform',\n",
        "    algorithm='auto',\n",
        "    metric='minkowski',\n",
        "    p=2\n",
        ")\n",
        "\n",
        "# ===== RADIUS NEIGHBORS =====\n",
        "\n",
        "from sklearn.neighbors import RadiusNeighborsClassifier\n",
        "\n",
        "rad_clf = RadiusNeighborsClassifier(\n",
        "    radius=1.0,\n",
        "    weights='distance',\n",
        "    outlier_label='most_frequent'  # For points with no neighbors\n",
        ")\n",
        "\n",
        "# Train and predict\n",
        "model.fit(X_train_scaled, y_train)\n",
        "predictions = model.predict(X_test_scaled)\n",
        "probabilities = model.predict_proba(X_test_scaled)  # Classification only\n",
        "```\n",
        "\n",
        "### Key Hyperparameters\n",
        "\n",
        "**n_neighbors (k)**:\n",
        "- Most important parameter\n",
        "- Small k \u2192 Complex boundary (overfitting)\n",
        "- Large k \u2192 Smooth boundary (underfitting)\n",
        "- Typical range: [3, 5, 7, 10, 15, 20]\n",
        "- Use odd numbers for binary classification (avoid ties)\n",
        "- Rule of thumb: k = \u221an\n",
        "\n",
        "**weights**:\n",
        "- 'uniform': All neighbors equal vote\n",
        "- 'distance': Closer neighbors more influence (weight = 1/distance)\n",
        "- Distance weighting helpful for larger k\n",
        "\n",
        "**metric** (distance measure):\n",
        "- 'euclidean' (default): Standard L2 distance\n",
        "- 'manhattan': L1 distance, good for high-D\n",
        "- 'minkowski': General Lp metric (p parameter)\n",
        "- 'chebyshev': L\u221e (maximum difference)\n",
        "\n",
        "**algorithm** (neighbor search):\n",
        "- 'auto': Chooses best automatically (recommended)\n",
        "- 'brute': O(n) - Always works\n",
        "- 'kd_tree': O(log n) - Fast for d < 20\n",
        "- 'ball_tree': O(log n) - Better for high-D\n",
        "\n",
        "### Computational Complexity\n",
        "\n",
        "| Phase | Brute Force | Tree-based (low-D) |\n",
        "|-------|-------------|-------------------|\n",
        "| Training | O(1) | O(n log n) |\n",
        "| Prediction (per sample) | O(n \u00d7 d) | O(log n \u00d7 d) |\n",
        "| Memory | O(n \u00d7 d) | O(n \u00d7 d) |\n",
        "\n",
        "### Feature Scaling\n",
        "\n",
        "\u2713 **CRITICAL**: Always scale features for KNN\n",
        "\u2713 Use StandardScaler or MinMaxScaler\n",
        "\u2713 Fit on training data only\n",
        "\u2713 Transform both train and test\n",
        "\n",
        "### KNN Strengths\n",
        "\n",
        "\u2713 Simple and intuitive\n",
        "\u2713 No training phase (fast to update)\n",
        "\u2713 Non-parametric (no assumptions)\n",
        "\u2713 Handles non-linear decision boundaries\n",
        "\u2713 Multi-class naturally supported\n",
        "\u2713 Works for both classification and regression\n",
        "\n",
        "### KNN Limitations\n",
        "\n",
        "\u2717 Slow predictions on large datasets\n",
        "\u2717 Memory intensive (stores all training data)\n",
        "\u2717 Sensitive to irrelevant features\n",
        "\u2717 Suffers from curse of dimensionality\n",
        "\u2717 Sensitive to feature scaling\n",
        "\u2717 Requires choosing k\n",
        "\u2717 Poor with imbalanced data\n",
        "\n",
        "### When to Use KNN\n",
        "\n",
        "\u2713 **Use KNN when:**\n",
        "- Small to medium datasets (<10k samples)\n",
        "- Low dimensionality (d < 20)\n",
        "- Non-linear decision boundaries\n",
        "- Need simple baseline\n",
        "- Data frequently updated (no retraining)\n",
        "- Multi-modal class distributions\n",
        "\n",
        "\u2717 **Avoid KNN when:**\n",
        "- Large datasets (>100k samples)\n",
        "- High dimensionality (d > 50)\n",
        "- Need fast predictions\n",
        "- Many irrelevant features\n",
        "- Interpretability needed\n",
        "- Storage is limited\n",
        "\n",
        "### Comparison with Other Algorithms\n",
        "\n",
        "| vs Algorithm | KNN Advantages | KNN Disadvantages |\n",
        "|--------------|----------------|-------------------|\n",
        "| Logistic Reg | Non-linear, multi-modal | Slower, less interpretable |\n",
        "| Decision Tree | Smoother boundaries | Slower, needs scaling |\n",
        "| SVM | Faster prediction | Worse with noisy data |\n",
        "| Random Forest | Simple, no hyperparams | Much slower, worse accuracy |\n",
        "| Neural Networks | Much simpler | Less flexible, worse performance |\n",
        "\n",
        "### Practical Tips\n",
        "\n",
        "1. **Start with k=5**: Good default for most problems\n",
        "2. **Cross-validate k**: Test odd numbers [3, 5, 7, 9, 11, 15, 21]\n",
        "3. **Scale features**: Use StandardScaler (mandatory!)\n",
        "4. **Try distance weighting**: Especially with larger k\n",
        "5. **Use PCA**: For high-dimensional data (d > 20)\n",
        "6. **Feature selection**: Remove irrelevant features\n",
        "7. **Check prediction time**: Profile on production-sized data\n",
        "8. **Use algorithm='auto'**: Let sklearn choose best structure\n",
        "\n",
        "### Further Reading\n",
        "\n",
        "- **Classic Paper**: Cover & Hart (1967) - \"Nearest Neighbor Pattern Classification\"\n",
        "- **Book**: \"Introduction to Statistical Learning\" - Chapter on KNN\n",
        "- **sklearn Documentation**: https://scikit-learn.org/stable/modules/neighbors.html\n",
        "- **Efficiency**: Ball Tree and KD-Tree algorithms\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Approximate Nearest Neighbors (ANN) for large-scale\n",
        "- Locally weighted learning\n",
        "- Distance metric learning\n",
        "- Hybrid methods (KNN + other algorithms)\n",
        "- KNN for anomaly detection"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}