{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Linear Regression Basics\n",
        "\n",
        "## Overview\n",
        "\n",
        "**Linear Regression** is one of the most fundamental and widely used machine learning algorithms. It models the relationship between a dependent variable (target) and one or more independent variables (features) using a linear equation.\n",
        "\n",
        "## Mathematical Foundation\n",
        "\n",
        "**Simple Linear Regression** (1 feature):\n",
        "\\[\n",
        "y = \\beta_0 + \\beta_1 x + \\epsilon\n",
        "\\]\n",
        "\n",
        "**Multiple Linear Regression** (multiple features):\n",
        "\\[\n",
        "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p + \\epsilon\n",
        "\\]\n",
        "\n",
        "**Matrix Form**:\n",
        "\\[\n",
        "y = X\\beta + \\epsilon\n",
        "\\]\n",
        "\n",
        "**Optimization (Ordinary Least Squares)**:\n",
        "\\[\n",
        "\\min_{\\beta} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\min_{\\beta} ||y - X\\beta||^2\n",
        "\\]\n",
        "\n",
        "**Closed-form Solution**:\n",
        "\\[\n",
        "\\hat{\\beta} = (X^T X)^{-1} X^T y\n",
        "\\]\n",
        "\n",
        "## Topics Covered\n",
        "\n",
        "1. Simple Linear Regression (1 feature)\n",
        "2. Multiple Linear Regression (many features)\n",
        "3. Polynomial Features\n",
        "4. Model Evaluation (R\u00b2, MSE, MAE)\n",
        "5. Assumptions and Diagnostics\n",
        "6. Real-world Datasets (California Housing, Diabetes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.datasets import (\n",
        "    make_regression,\n",
        "    load_diabetes,\n",
        "    fetch_california_housing\n",
        ")\n",
        "\n",
        "np.random.seed(42)\n",
        "sns.set_style('whitegrid')\n",
        "print(\"\u2713 Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "simple-lr",
      "metadata": {},
      "source": [
        "## 1. Simple Linear Regression (1 Feature)\n",
        "\n",
        "### 1.1 Synthetic Data Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "simple-lr-synthetic",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate simple synthetic data\n",
        "np.random.seed(42)\n",
        "X_simple = 2 * np.random.rand(100, 1)\n",
        "y_simple = 4 + 3 * X_simple[:, 0] + np.random.randn(100)\n",
        "\n",
        "print(\"Simple Linear Regression Example\")\n",
        "print(\"=\"*70)\n",
        "print(f\"True relationship: y = 4 + 3*x + noise\")\n",
        "print(f\"Data shape: X={X_simple.shape}, y={y_simple.shape}\")\n",
        "\n",
        "# Create and train model\n",
        "model_simple = LinearRegression()\n",
        "model_simple.fit(X_simple, y_simple)\n",
        "\n",
        "# Get parameters\n",
        "intercept = model_simple.intercept_\n",
        "coefficient = model_simple.coef_[0]\n",
        "\n",
        "print(f\"\\nLearned parameters:\")\n",
        "print(f\"  Intercept (\u03b2\u2080): {intercept:.4f}  (true: 4.0)\")\n",
        "print(f\"  Coefficient (\u03b2\u2081): {coefficient:.4f}  (true: 3.0)\")\n",
        "print(f\"\\nLearned equation: y = {intercept:.4f} + {coefficient:.4f}*x\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_simple = model_simple.predict(X_simple)\n",
        "\n",
        "# Evaluate\n",
        "r2 = r2_score(y_simple, y_pred_simple)\n",
        "mse = mean_squared_error(y_simple, y_pred_simple)\n",
        "mae = mean_absolute_error(y_simple, y_pred_simple)\n",
        "\n",
        "print(f\"\\nModel Performance:\")\n",
        "print(f\"  R\u00b2 Score: {r2:.4f}\")\n",
        "print(f\"  MSE: {mse:.4f}\")\n",
        "print(f\"  MAE: {mae:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "simple-lr-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Scatter plot of data\n",
        "plt.scatter(X_simple, y_simple, alpha=0.6, label='Data points')\n",
        "\n",
        "# Regression line\n",
        "X_line = np.linspace(0, 2, 100).reshape(-1, 1)\n",
        "y_line = model_simple.predict(X_line)\n",
        "plt.plot(X_line, y_line, 'r-', linewidth=2, label=f'Fitted line: y = {intercept:.2f} + {coefficient:.2f}x')\n",
        "\n",
        "# True line\n",
        "y_true = 4 + 3 * X_line\n",
        "plt.plot(X_line, y_true, 'g--', linewidth=2, alpha=0.5, label='True line: y = 4.0 + 3.0x')\n",
        "\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title(f'Simple Linear Regression (R\u00b2 = {r2:.3f})')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "manual-implementation",
      "metadata": {},
      "source": [
        "### 1.2 Manual Implementation (Understanding the Math)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "manual-impl",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual computation using Normal Equation: \u03b2 = (X^T X)^(-1) X^T y\n",
        "X_with_bias = np.c_[np.ones((X_simple.shape[0], 1)), X_simple]\n",
        "\n",
        "# Normal equation\n",
        "beta_manual = np.linalg.inv(X_with_bias.T @ X_with_bias) @ X_with_bias.T @ y_simple\n",
        "\n",
        "print(\"Manual Implementation (Normal Equation)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nManually computed parameters:\")\n",
        "print(f\"  \u03b2\u2080 (intercept): {beta_manual[0]:.4f}\")\n",
        "print(f\"  \u03b2\u2081 (coefficient): {beta_manual[1]:.4f}\")\n",
        "\n",
        "print(f\"\\nSklearn parameters:\")\n",
        "print(f\"  \u03b2\u2080 (intercept): {intercept:.4f}\")\n",
        "print(f\"  \u03b2\u2081 (coefficient): {coefficient:.4f}\")\n",
        "\n",
        "print(f\"\\n\u2713 Manual and sklearn results match!\")\n",
        "\n",
        "# Make predictions manually\n",
        "y_pred_manual = X_with_bias @ beta_manual\n",
        "\n",
        "# Verify they match\n",
        "assert np.allclose(y_pred_simple, y_pred_manual), \"Predictions don't match!\"\n",
        "print(\"\u2713 Predictions also match!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "multiple-lr",
      "metadata": {},
      "source": [
        "## 2. Multiple Linear Regression\n",
        "\n",
        "### 2.1 Synthetic Data with Multiple Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "multiple-lr-synthetic",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate multiple regression data\n",
        "X_multi, y_multi = make_regression(\n",
        "    n_samples=200,\n",
        "    n_features=5,\n",
        "    n_informative=3,\n",
        "    noise=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Multiple Linear Regression Example\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Data shape: X={X_multi.shape}, y={y_multi.shape}\")\n",
        "print(f\"Features: {X_multi.shape[1]}\")\n",
        "print(f\"Informative features: 3\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_multi, y_multi, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "multiple-lr-train",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model\n",
        "model_multi = LinearRegression()\n",
        "model_multi.fit(X_train, y_train)\n",
        "\n",
        "# Get parameters\n",
        "print(\"\\nModel Parameters:\")\n",
        "print(f\"  Intercept: {model_multi.intercept_:.4f}\")\n",
        "print(f\"  Coefficients:\")\n",
        "for i, coef in enumerate(model_multi.coef_):\n",
        "    print(f\"    Feature {i}: {coef:.4f}\")\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = model_multi.predict(X_train)\n",
        "y_test_pred = model_multi.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "print(f\"\\nPerformance:\")\n",
        "print(f\"  Train R\u00b2: {train_r2:.4f}\")\n",
        "print(f\"  Test R\u00b2:  {test_r2:.4f}\")\n",
        "print(f\"  Train MSE: {train_mse:.2f}\")\n",
        "print(f\"  Test MSE:  {test_mse:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "multiple-lr-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions vs actual\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Training set\n",
        "axes[0].scatter(y_train, y_train_pred, alpha=0.6)\n",
        "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
        "axes[0].set_xlabel('Actual Values')\n",
        "axes[0].set_ylabel('Predicted Values')\n",
        "axes[0].set_title(f'Training Set (R\u00b2 = {train_r2:.3f})')\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Test set\n",
        "axes[1].scatter(y_test, y_test_pred, alpha=0.6, color='orange')\n",
        "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "axes[1].set_xlabel('Actual Values')\n",
        "axes[1].set_ylabel('Predicted Values')\n",
        "axes[1].set_title(f'Test Set (R\u00b2 = {test_r2:.3f})')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "coefficient-importance",
      "metadata": {},
      "source": [
        "### 2.2 Feature Importance (Coefficient Magnitude)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feature-importance",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize coefficient magnitudes\n",
        "feature_names = [f'Feature {i}' for i in range(X_multi.shape[1])]\n",
        "coef_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': model_multi.coef_,\n",
        "    'Abs_Coefficient': np.abs(model_multi.coef_)\n",
        "}).sort_values('Abs_Coefficient', ascending=False)\n",
        "\n",
        "print(\"Feature Importance (by coefficient magnitude):\")\n",
        "print(coef_df.to_string(index=False))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.barh(coef_df['Feature'], coef_df['Coefficient'])\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.title('Feature Coefficients')\n",
        "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
        "plt.grid(alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Note: Coefficient magnitude indicates feature importance\")\n",
        "print(\"   ONLY when features are on the same scale!\")\n",
        "print(\"   Otherwise, standardize features first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "diabetes-dataset",
      "metadata": {},
      "source": [
        "## 3. Real Dataset: Diabetes Progression\n",
        "\n",
        "### 3.1 Load and Explore Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "diabetes-load",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X_diabetes = diabetes.data\n",
        "y_diabetes = diabetes.target\n",
        "\n",
        "print(\"Diabetes Dataset\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Description: {diabetes.DESCR[:200]}...\")\n",
        "print(f\"\\nShape: {X_diabetes.shape}\")\n",
        "print(f\"Features: {len(diabetes.feature_names)}\")\n",
        "print(f\"Feature names: {diabetes.feature_names}\")\n",
        "print(f\"\\nTarget (disease progression):\")\n",
        "print(f\"  Range: [{y_diabetes.min():.0f}, {y_diabetes.max():.0f}]\")\n",
        "print(f\"  Mean: {y_diabetes.mean():.2f}\")\n",
        "print(f\"  Std: {y_diabetes.std():.2f}\")\n",
        "\n",
        "# Create DataFrame for easier analysis\n",
        "df_diabetes = pd.DataFrame(X_diabetes, columns=diabetes.feature_names)\n",
        "df_diabetes['target'] = y_diabetes\n",
        "\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df_diabetes.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "diabetes-correlation",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation analysis\n",
        "plt.figure(figsize=(12, 8))\n",
        "correlation_matrix = df_diabetes.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
        "plt.title('Feature Correlation Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation with target\n",
        "target_corr = correlation_matrix['target'].drop('target').sort_values(ascending=False)\n",
        "print(\"\\nCorrelation with Target (disease progression):\")\n",
        "print(target_corr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "diabetes-model",
      "metadata": {},
      "source": [
        "### 3.2 Train Linear Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "diabetes-train",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train_db, X_test_db, y_train_db, y_test_db = train_test_split(\n",
        "    X_diabetes, y_diabetes, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model_diabetes = LinearRegression()\n",
        "model_diabetes.fit(X_train_db, y_train_db)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred_db = model_diabetes.predict(X_train_db)\n",
        "y_test_pred_db = model_diabetes.predict(X_test_db)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Diabetes Model Performance\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTraining Set:\")\n",
        "print(f\"  R\u00b2 Score: {r2_score(y_train_db, y_train_pred_db):.4f}\")\n",
        "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_train_db, y_train_pred_db)):.2f}\")\n",
        "print(f\"  MAE: {mean_absolute_error(y_train_db, y_train_pred_db):.2f}\")\n",
        "\n",
        "print(f\"\\nTest Set:\")\n",
        "print(f\"  R\u00b2 Score: {r2_score(y_test_db, y_test_pred_db):.4f}\")\n",
        "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_test_db, y_test_pred_db)):.2f}\")\n",
        "print(f\"  MAE: {mean_absolute_error(y_test_db, y_test_pred_db):.2f}\")\n",
        "\n",
        "# Cross-validation\n",
        "cv_scores = cross_val_score(model_diabetes, X_diabetes, y_diabetes, cv=5, scoring='r2')\n",
        "print(f\"\\nCross-Validation (5-fold):\")\n",
        "print(f\"  R\u00b2 Scores: {[f'{s:.3f}' for s in cv_scores]}\")\n",
        "print(f\"  Mean R\u00b2: {cv_scores.mean():.4f} \u00b1 {cv_scores.std():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "diabetes-coef",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze coefficients\n",
        "coef_diabetes = pd.DataFrame({\n",
        "    'Feature': diabetes.feature_names,\n",
        "    'Coefficient': model_diabetes.coef_,\n",
        "    'Abs_Coefficient': np.abs(model_diabetes.coef_)\n",
        "}).sort_values('Abs_Coefficient', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Coefficients (sorted by magnitude):\")\n",
        "print(coef_diabetes.to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = ['green' if c > 0 else 'red' for c in coef_diabetes['Coefficient']]\n",
        "plt.barh(coef_diabetes['Feature'], coef_diabetes['Coefficient'], color=colors)\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.title('Diabetes Model: Feature Coefficients')\n",
        "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
        "plt.grid(alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(f\"  Intercept: {model_diabetes.intercept_:.2f}\")\n",
        "print(f\"  Most positive effect: {coef_diabetes.iloc[0]['Feature']} ({coef_diabetes.iloc[0]['Coefficient']:.2f})\")\n",
        "print(f\"  Most negative effect: {coef_diabetes.iloc[-1]['Feature']} ({coef_diabetes.iloc[-1]['Coefficient']:.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "polynomial",
      "metadata": {},
      "source": [
        "## 4. Polynomial Regression\n",
        "\n",
        "### 4.1 Non-linear Relationships"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "polynomial-data",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate non-linear data\n",
        "np.random.seed(42)\n",
        "X_poly = 6 * np.random.rand(100, 1) - 3\n",
        "y_poly = 0.5 * X_poly**2 + X_poly + 2 + np.random.randn(100, 1).ravel()\n",
        "\n",
        "print(\"Polynomial Regression Example\")\n",
        "print(\"=\"*70)\n",
        "print(f\"True relationship: y = 0.5*x\u00b2 + x + 2 + noise\")\n",
        "print(f\"Data shape: {X_poly.shape}\")\n",
        "\n",
        "# Try linear regression (will underfit)\n",
        "model_linear_poly = LinearRegression()\n",
        "model_linear_poly.fit(X_poly, y_poly)\n",
        "y_pred_linear = model_linear_poly.predict(X_poly)\n",
        "r2_linear = r2_score(y_poly, y_pred_linear)\n",
        "\n",
        "print(f\"\\nLinear Model R\u00b2: {r2_linear:.4f} (underfitting!)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "polynomial-fit",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create polynomial features\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly_transformed = poly_features.fit_transform(X_poly)\n",
        "\n",
        "print(\"Polynomial Features:\")\n",
        "print(f\"  Original features: {X_poly.shape[1]}\")\n",
        "print(f\"  Transformed features: {X_poly_transformed.shape[1]}\")\n",
        "print(f\"  Feature names: {poly_features.get_feature_names_out(['x'])}\")\n",
        "\n",
        "# Fit polynomial model\n",
        "model_poly = LinearRegression()\n",
        "model_poly.fit(X_poly_transformed, y_poly)\n",
        "y_pred_poly = model_poly.predict(X_poly_transformed)\n",
        "r2_poly = r2_score(y_poly, y_pred_poly)\n",
        "\n",
        "print(f\"\\nPolynomial Model (degree=2):\")\n",
        "print(f\"  R\u00b2: {r2_poly:.4f}\")\n",
        "print(f\"  Coefficients: {model_poly.coef_}\")\n",
        "print(f\"  Intercept: {model_poly.intercept_:.4f}\")\n",
        "print(f\"\\nLearned equation: y = {model_poly.intercept_:.2f} + {model_poly.coef_[0]:.2f}*x + {model_poly.coef_[1]:.2f}*x\u00b2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "polynomial-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize comparison\n",
        "X_plot = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
        "X_plot_poly = poly_features.transform(X_plot)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Linear model\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_poly, y_poly, alpha=0.6, label='Data')\n",
        "plt.plot(X_plot, model_linear_poly.predict(X_plot), 'r-', linewidth=2, label='Linear model')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title(f'Linear Model (R\u00b2 = {r2_linear:.3f}) - Underfitting')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "# Polynomial model\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_poly, y_poly, alpha=0.6, label='Data')\n",
        "plt.plot(X_plot, model_poly.predict(X_plot_poly), 'g-', linewidth=2, label='Polynomial model (degree=2)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title(f'Polynomial Model (R\u00b2 = {r2_poly:.3f})')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "polynomial-degrees",
      "metadata": {},
      "source": [
        "### 4.2 Effect of Polynomial Degree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "polynomial-degrees-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different degrees\n",
        "degrees = [1, 2, 3, 5, 10]\n",
        "results = []\n",
        "\n",
        "for degree in degrees:\n",
        "    poly_feat = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    X_transformed = poly_feat.fit_transform(X_poly)\n",
        "    \n",
        "    model = LinearRegression()\n",
        "    model.fit(X_transformed, y_poly)\n",
        "    \n",
        "    y_pred = model.predict(X_transformed)\n",
        "    r2 = r2_score(y_poly, y_pred)\n",
        "    mse = mean_squared_error(y_poly, y_pred)\n",
        "    \n",
        "    results.append({\n",
        "        'Degree': degree,\n",
        "        'Features': X_transformed.shape[1],\n",
        "        'R\u00b2': r2,\n",
        "        'MSE': mse\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"Effect of Polynomial Degree:\")\n",
        "print(\"=\"*70)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Observations:\")\n",
        "print(\"  - Degree 2: Good fit (matches true relationship)\")\n",
        "print(\"  - Higher degrees: Risk of overfitting\")\n",
        "print(\"  - More features \u2260 better model!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "california-housing",
      "metadata": {},
      "source": [
        "## 5. Real Dataset: California Housing\n",
        "\n",
        "### 5.1 Load and Explore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "housing-load",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X_housing = housing.data\n",
        "y_housing = housing.target\n",
        "\n",
        "print(\"California Housing Dataset\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_housing.shape[0]}\")\n",
        "print(f\"Features: {X_housing.shape[1]}\")\n",
        "print(f\"Feature names: {housing.feature_names}\")\n",
        "print(f\"\\nTarget: Median house value (in $100,000s)\")\n",
        "print(f\"  Range: ${y_housing.min()*100000:.0f} - ${y_housing.max()*100000:.0f}\")\n",
        "print(f\"  Mean: ${y_housing.mean()*100000:.0f}\")\n",
        "\n",
        "# Create DataFrame\n",
        "df_housing = pd.DataFrame(X_housing, columns=housing.feature_names)\n",
        "df_housing['Price'] = y_housing\n",
        "\n",
        "print(\"\\nDataset Statistics:\")\n",
        "print(df_housing.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "housing-model",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split and standardize\n",
        "X_train_hs, X_test_hs, y_train_hs, y_test_hs = train_test_split(\n",
        "    X_housing, y_housing, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_hs_scaled = scaler.fit_transform(X_train_hs)\n",
        "X_test_hs_scaled = scaler.transform(X_test_hs)\n",
        "\n",
        "# Train model\n",
        "model_housing = LinearRegression()\n",
        "model_housing.fit(X_train_hs_scaled, y_train_hs)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred_hs = model_housing.predict(X_train_hs_scaled)\n",
        "y_test_pred_hs = model_housing.predict(X_test_hs_scaled)\n",
        "\n",
        "# Evaluate\n",
        "print(\"California Housing Model Performance\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTraining Set:\")\n",
        "print(f\"  R\u00b2: {r2_score(y_train_hs, y_train_pred_hs):.4f}\")\n",
        "print(f\"  RMSE: ${np.sqrt(mean_squared_error(y_train_hs, y_train_pred_hs))*100000:.0f}\")\n",
        "print(f\"  MAE: ${mean_absolute_error(y_train_hs, y_train_pred_hs)*100000:.0f}\")\n",
        "\n",
        "print(f\"\\nTest Set:\")\n",
        "print(f\"  R\u00b2: {r2_score(y_test_hs, y_test_pred_hs):.4f}\")\n",
        "print(f\"  RMSE: ${np.sqrt(mean_squared_error(y_test_hs, y_test_pred_hs))*100000:.0f}\")\n",
        "print(f\"  MAE: ${mean_absolute_error(y_test_hs, y_test_pred_hs)*100000:.0f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "housing-importance",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance (now properly scaled)\n",
        "coef_housing = pd.DataFrame({\n",
        "    'Feature': housing.feature_names,\n",
        "    'Coefficient': model_housing.coef_,\n",
        "    'Abs_Coefficient': np.abs(model_housing.coef_)\n",
        "}).sort_values('Abs_Coefficient', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importance (on standardized features):\")\n",
        "print(coef_housing.to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = ['green' if c > 0 else 'red' for c in coef_housing['Coefficient']]\n",
        "plt.barh(coef_housing['Feature'], coef_housing['Coefficient'], color=colors, alpha=0.7)\n",
        "plt.xlabel('Standardized Coefficient')\n",
        "plt.title('California Housing: Feature Importance')\n",
        "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
        "plt.grid(alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Interpretation (after standardization):\")\n",
        "print(\"   Coefficient magnitudes are now comparable!\")\n",
        "print(f\"   Most important: {coef_housing.iloc[0]['Feature']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "diagnostics",
      "metadata": {},
      "source": [
        "## 6. Model Diagnostics and Assumptions\n",
        "\n",
        "### 6.1 Residual Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "residuals",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate residuals\n",
        "residuals_train = y_train_hs - y_train_pred_hs\n",
        "residuals_test = y_test_hs - y_test_pred_hs\n",
        "\n",
        "# Visualize residuals\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Residuals vs Fitted\n",
        "axes[0, 0].scatter(y_train_pred_hs, residuals_train, alpha=0.5)\n",
        "axes[0, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "axes[0, 0].set_xlabel('Fitted Values')\n",
        "axes[0, 0].set_ylabel('Residuals')\n",
        "axes[0, 0].set_title('Residuals vs Fitted Values')\n",
        "axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "# 2. Histogram of residuals\n",
        "axes[0, 1].hist(residuals_train, bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0, 1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
        "axes[0, 1].set_xlabel('Residuals')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_title('Distribution of Residuals')\n",
        "axes[0, 1].grid(alpha=0.3)\n",
        "\n",
        "# 3. Q-Q plot\n",
        "from scipy import stats\n",
        "stats.probplot(residuals_train, dist=\"norm\", plot=axes[1, 0])\n",
        "axes[1, 0].set_title('Q-Q Plot')\n",
        "axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "# 4. Scale-Location plot\n",
        "standardized_residuals = residuals_train / residuals_train.std()\n",
        "axes[1, 1].scatter(y_train_pred_hs, np.sqrt(np.abs(standardized_residuals)), alpha=0.5)\n",
        "axes[1, 1].set_xlabel('Fitted Values')\n",
        "axes[1, 1].set_ylabel('\u221a|Standardized Residuals|')\n",
        "axes[1, 1].set_title('Scale-Location Plot')\n",
        "axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Residual Analysis:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Mean of residuals: {residuals_train.mean():.6f} (should be \u2248 0)\")\n",
        "print(f\"Std of residuals: {residuals_train.std():.4f}\")\n",
        "print(f\"\\nWhat to look for:\")\n",
        "print(\"  1. Residuals vs Fitted: Random scatter (no pattern)\")\n",
        "print(\"  2. Histogram: Approximately normal distribution\")\n",
        "print(\"  3. Q-Q Plot: Points follow diagonal line\")\n",
        "print(\"  4. Scale-Location: Constant variance (homoscedasticity)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "assumptions",
      "metadata": {},
      "source": [
        "### 6.2 Linear Regression Assumptions\n",
        "\n",
        "Linear regression relies on several key assumptions:\n",
        "\n",
        "1. **Linearity**: Relationship between X and y is linear\n",
        "2. **Independence**: Observations are independent\n",
        "3. **Homoscedasticity**: Constant variance of residuals\n",
        "4. **Normality**: Residuals are normally distributed\n",
        "5. **No multicollinearity**: Features are not highly correlated\n",
        "\n",
        "#### Checking Multicollinearity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "multicollinearity",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate VIF (Variance Inflation Factor)\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# VIF for housing dataset\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = housing.feature_names\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_train_hs_scaled, i) \n",
        "                   for i in range(X_train_hs_scaled.shape[1])]\n",
        "vif_data = vif_data.sort_values('VIF', ascending=False)\n",
        "\n",
        "print(\"Variance Inflation Factor (VIF):\")\n",
        "print(\"=\"*70)\n",
        "print(vif_data.to_string(index=False))\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"  VIF < 5: Low multicollinearity\")\n",
        "print(\"  5 < VIF < 10: Moderate multicollinearity\")\n",
        "print(\"  VIF > 10: High multicollinearity (problematic)\")\n",
        "\n",
        "high_vif = vif_data[vif_data['VIF'] > 10]\n",
        "if len(high_vif) > 0:\n",
        "    print(f\"\\n\u26a0 Warning: {len(high_vif)} feature(s) with VIF > 10\")\n",
        "else:\n",
        "    print(\"\\n\u2713 No severe multicollinearity detected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Summary and Best Practices\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "```python\n",
        "# Basic usage\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Get parameters\n",
        "intercept = model.intercept_\n",
        "coefficients = model.coef_\n",
        "```\n",
        "\n",
        "### When to Use Linear Regression\n",
        "\n",
        "\u2713 **Good for:**\n",
        "- Continuous target variable\n",
        "- Linear or polynomial relationships\n",
        "- Interpretable models\n",
        "- Baseline models\n",
        "- Small to medium datasets\n",
        "\n",
        "\u2717 **Not ideal for:**\n",
        "- Highly non-linear relationships\n",
        "- Classification tasks (use logistic regression)\n",
        "- High-dimensional data without regularization\n",
        "- When assumptions are violated\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. **Always standardize features** when comparing coefficient magnitudes\n",
        "2. **Check assumptions** (linearity, independence, homoscedasticity, normality)\n",
        "3. **Look for multicollinearity** (VIF > 10 is problematic)\n",
        "4. **Analyze residuals** to validate model fit\n",
        "5. **Use polynomial features** for non-linear relationships\n",
        "6. **Apply regularization** (Ridge/Lasso) for high-dimensional data\n",
        "7. **Cross-validate** to assess generalization\n",
        "\n",
        "### Common Metrics\n",
        "\n",
        "- **R\u00b2 Score**: Proportion of variance explained (0-1, higher is better)\n",
        "- **MSE**: Mean squared error (lower is better)\n",
        "- **RMSE**: Root mean squared error (same units as target)\n",
        "- **MAE**: Mean absolute error (robust to outliers)\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Regularization (Ridge, Lasso, ElasticNet) \u2192 Next notebook!\n",
        "- Feature engineering and selection\n",
        "- Gradient descent optimization\n",
        "- Advanced regression techniques"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}