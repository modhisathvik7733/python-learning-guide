{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Polynomial Features and Interaction Terms\n",
        "\n",
        "## Overview\n",
        "\n",
        "Linear models can model **non-linear relationships** by creating polynomial features and interaction terms from the original features. This transforms the feature space while keeping the model linear in its parameters.\n",
        "\n",
        "## Mathematical Foundation\n",
        "\n",
        "### Original Linear Model\n",
        "\\[\n",
        "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n",
        "\\]\n",
        "\n",
        "### Polynomial Features (degree=2)\n",
        "\\[\n",
        "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1^2 + \\beta_4 x_1x_2 + \\beta_5 x_2^2 + \\epsilon\n",
        "\\]\n",
        "\n",
        "**Key Insight**: The model is still linear in coefficients \\(\\beta\\), but non-linear in original features \\(x\\).\n",
        "\n",
        "### Feature Expansion\n",
        "\n",
        "For input \\([x_1, x_2]\\) with degree=2:\n",
        "\\[\n",
        "[x_1, x_2] \\rightarrow [1, x_1, x_2, x_1^2, x_1x_2, x_2^2]\n",
        "\\]\n",
        "\n",
        "**Number of features**: For \\(n\\) original features and degree \\(d\\):\n",
        "\\[\n",
        "\\text{New features} = \\binom{n + d}{d} = \\frac{(n+d)!}{n! \\cdot d!}\n",
        "\\]\n",
        "\n",
        "## Topics Covered\n",
        "\n",
        "1. Polynomial features for single variables\n",
        "2. Interaction terms between multiple features\n",
        "3. Feature explosion and regularization\n",
        "4. Bias-variance tradeoff with polynomial degree\n",
        "5. Real-world applications\n",
        "6. Best practices and pitfalls\n",
        "7. Custom feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, validation_curve\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.datasets import make_regression, load_diabetes\n",
        "from itertools import combinations\n",
        "\n",
        "np.random.seed(42)\n",
        "sns.set_style('whitegrid')\n",
        "print(\"\u2713 Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "single-var",
      "metadata": {},
      "source": [
        "## 1. Polynomial Features: Single Variable\n",
        "\n",
        "### 1.1 Understanding Polynomial Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "poly-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple example to understand transformation\n",
        "X_simple = np.array([[2], [3], [4]])\n",
        "\n",
        "print(\"Polynomial Feature Transformation Demo\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Original features: {X_simple.flatten()}\\n\")\n",
        "\n",
        "for degree in [1, 2, 3]:\n",
        "    poly = PolynomialFeatures(degree=degree, include_bias=True)\n",
        "    X_poly = poly.fit_transform(X_simple)\n",
        "    \n",
        "    print(f\"Degree {degree}:\")\n",
        "    print(f\"  Feature names: {poly.get_feature_names_out(['x'])}\")\n",
        "    print(f\"  Shape: {X_simple.shape} \u2192 {X_poly.shape}\")\n",
        "    print(f\"  Example (x=2): {X_poly[0]}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nonlinear-data",
      "metadata": {},
      "source": [
        "### 1.2 Fitting Non-linear Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nonlinear-fit",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate non-linear data\n",
        "np.random.seed(42)\n",
        "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
        "y = np.sin(X).ravel() + np.random.randn(80) * 0.1\n",
        "\n",
        "print(\"Non-linear Data: y = sin(x) + noise\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Data shape: {X.shape}\")\n",
        "print(f\"X range: [{X.min():.2f}, {X.max():.2f}]\")\n",
        "print(f\"y range: [{y.min():.2f}, {y.max():.2f}]\")\n",
        "\n",
        "# Try different polynomial degrees\n",
        "degrees = [1, 3, 5, 9, 15]\n",
        "X_test = np.linspace(0, 5, 100).reshape(-1, 1)\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "for i, degree in enumerate(degrees, 1):\n",
        "    plt.subplot(2, 3, i)\n",
        "    \n",
        "    # Create polynomial features\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "    X_test_poly = poly.transform(X_test)\n",
        "    \n",
        "    # Fit model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_poly, y)\n",
        "    y_pred = model.predict(X_test_poly)\n",
        "    \n",
        "    # Calculate R\u00b2\n",
        "    y_train_pred = model.predict(X_poly)\n",
        "    r2 = r2_score(y, y_train_pred)\n",
        "    \n",
        "    # Plot\n",
        "    plt.scatter(X, y, alpha=0.5, s=20, label='Data')\n",
        "    plt.plot(X_test, y_pred, 'r-', linewidth=2, label=f'Degree {degree}')\n",
        "    plt.plot(X_test, np.sin(X_test), 'g--', alpha=0.5, linewidth=1.5, label='True function')\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('y')\n",
        "    plt.title(f'Degree {degree} (R\u00b2 = {r2:.3f})')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.ylim(-2, 2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Observations:\")\n",
        "print(\"  - Degree 1 (linear): Underfitting\")\n",
        "print(\"  - Degree 3-5: Good fit\")\n",
        "print(\"  - Degree 15: Overfitting (wild oscillations)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bias-variance",
      "metadata": {},
      "source": [
        "### 1.3 Bias-Variance Tradeoff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bias-variance-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train, X_test_split, y_train, y_test_split = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Evaluate different degrees\n",
        "degrees_eval = range(1, 16)\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "n_features = []\n",
        "\n",
        "for degree in degrees_eval:\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_train_poly = poly.fit_transform(X_train)\n",
        "    X_test_poly = poly.transform(X_test_split)\n",
        "    \n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train_poly, y_train)\n",
        "    \n",
        "    train_scores.append(model.score(X_train_poly, y_train))\n",
        "    test_scores.append(model.score(X_test_poly, y_test_split))\n",
        "    n_features.append(X_train_poly.shape[1])\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Scores vs degree\n",
        "axes[0].plot(degrees_eval, train_scores, 'o-', label='Train R\u00b2', linewidth=2)\n",
        "axes[0].plot(degrees_eval, test_scores, 's-', label='Test R\u00b2', linewidth=2)\n",
        "axes[0].axvline(x=5, color='r', linestyle='--', alpha=0.5, label='Sweet spot')\n",
        "axes[0].set_xlabel('Polynomial Degree')\n",
        "axes[0].set_ylabel('R\u00b2 Score')\n",
        "axes[0].set_title('Bias-Variance Tradeoff')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Number of features\n",
        "axes[1].plot(degrees_eval, n_features, 'o-', color='green', linewidth=2)\n",
        "axes[1].set_xlabel('Polynomial Degree')\n",
        "axes[1].set_ylabel('Number of Features')\n",
        "axes[1].set_title('Feature Explosion')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find optimal degree\n",
        "optimal_idx = np.argmax(test_scores)\n",
        "optimal_degree = list(degrees_eval)[optimal_idx]\n",
        "\n",
        "print(\"\\nBias-Variance Analysis:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Optimal degree: {optimal_degree}\")\n",
        "print(f\"Test R\u00b2 at optimal: {test_scores[optimal_idx]:.4f}\")\n",
        "print(f\"Features at optimal: {n_features[optimal_idx]}\")\n",
        "print(f\"\\n\u26a0\ufe0f At degree 15: {n_features[-1]} features for {len(X_train)} samples!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "multi-var",
      "metadata": {},
      "source": [
        "## 2. Polynomial Features: Multiple Variables\n",
        "\n",
        "### 2.1 Feature Explosion with Multiple Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "multi-var-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate feature explosion\n",
        "print(\"Feature Explosion with Multiple Variables\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'n_features':<12} {'degree':<8} {'new_features':<15} {'ratio':<10}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for n_features_orig in [2, 5, 10, 20]:\n",
        "    for degree in [2, 3, 4]:\n",
        "        poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "        X_dummy = np.random.rand(10, n_features_orig)\n",
        "        X_transformed = poly.fit_transform(X_dummy)\n",
        "        n_new = X_transformed.shape[1]\n",
        "        ratio = n_new / n_features_orig\n",
        "        \n",
        "        print(f\"{n_features_orig:<12} {degree:<8} {n_new:<15} {ratio:<10.1f}x\")\n",
        "\n",
        "print(\"\\n\u26a0\ufe0f Warning: Features grow combinatorially!\")\n",
        "print(\"   Example: 20 features with degree 4 \u2192 10,626 features!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "interactions",
      "metadata": {},
      "source": [
        "### 2.2 Understanding Interaction Terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "interaction-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example with 2 features\n",
        "X_2d = np.array([[2, 3], [4, 5], [6, 7]])\n",
        "\n",
        "print(\"Polynomial Features: 2 Variables\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for degree in [1, 2, 3]:\n",
        "    poly = PolynomialFeatures(degree=degree, include_bias=True)\n",
        "    X_poly = poly.fit_transform(X_2d)\n",
        "    feature_names = poly.get_feature_names_out(['x1', 'x2'])\n",
        "    \n",
        "    print(f\"\\nDegree {degree}:\")\n",
        "    print(f\"  Features: {list(feature_names)}\")\n",
        "    print(f\"  Shape: {X_2d.shape} \u2192 {X_poly.shape}\")\n",
        "    \n",
        "    if degree == 2:\n",
        "        print(f\"\\n  Example transformation for [x1=2, x2=3]:\")\n",
        "        print(f\"  \u2192 [1, 2, 3, 4, 6, 9]\")\n",
        "        print(f\"     \u2191  \u2191  \u2191  \u2191  \u2191   \u2191\")\n",
        "        print(f\"     1  x1 x2 x1\u00b2 x1x2 x2\u00b2\")\n",
        "        print(f\"\\n  \ud83d\udca1 x1*x2 is the INTERACTION term!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "interaction-importance",
      "metadata": {},
      "source": [
        "### 2.3 Interaction Terms in Practice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "interaction-synthetic",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate data with interaction\n",
        "np.random.seed(42)\n",
        "n_samples = 200\n",
        "X1 = np.random.randn(n_samples)\n",
        "X2 = np.random.randn(n_samples)\n",
        "\n",
        "# True relationship includes interaction\n",
        "y_true = 2 + 3*X1 + 1.5*X2 + 2.5*X1*X2 + np.random.randn(n_samples)*0.5\n",
        "\n",
        "X_data = np.column_stack([X1, X2])\n",
        "\n",
        "print(\"Data with Interaction Term\")\n",
        "print(\"=\"*70)\n",
        "print(\"True relationship: y = 2 + 3*x1 + 1.5*x2 + 2.5*x1*x2 + noise\")\n",
        "print(f\"Data shape: {X_data.shape}\\n\")\n",
        "\n",
        "# Split data\n",
        "X_train_int, X_test_int, y_train_int, y_test_int = train_test_split(\n",
        "    X_data, y_true, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Model WITHOUT interaction\n",
        "model_no_int = LinearRegression()\n",
        "model_no_int.fit(X_train_int, y_train_int)\n",
        "r2_no_int = model_no_int.score(X_test_int, y_test_int)\n",
        "\n",
        "print(\"Model WITHOUT interaction (y = \u03b20 + \u03b21*x1 + \u03b22*x2):\")\n",
        "print(f\"  Test R\u00b2: {r2_no_int:.4f}\")\n",
        "print(f\"  Coefficients: {model_no_int.coef_}\")\n",
        "\n",
        "# Model WITH interaction (degree=2)\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_train_poly = poly.fit_transform(X_train_int)\n",
        "X_test_poly = poly.transform(X_test_int)\n",
        "\n",
        "model_with_int = LinearRegression()\n",
        "model_with_int.fit(X_train_poly, y_train_int)\n",
        "r2_with_int = model_with_int.score(X_test_poly, y_test_int)\n",
        "\n",
        "print(f\"\\nModel WITH interaction (degree=2 polynomial):\")\n",
        "print(f\"  Test R\u00b2: {r2_with_int:.4f}\")\n",
        "print(f\"  Feature names: {poly.get_feature_names_out(['x1', 'x2'])}\")\n",
        "print(f\"  Coefficients: {model_with_int.coef_}\")\n",
        "\n",
        "print(f\"\\n\u2713 Improvement: {(r2_with_int - r2_no_int):.4f}\")\n",
        "print(f\"  The interaction term (x1*x2) captures the true relationship!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "interaction-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize interaction effect\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "# 3D scatter plot\n",
        "ax1 = fig.add_subplot(131, projection='3d')\n",
        "ax1.scatter(X1, X2, y_true, alpha=0.5, s=20)\n",
        "ax1.set_xlabel('X1')\n",
        "ax1.set_ylabel('X2')\n",
        "ax1.set_zlabel('y')\n",
        "ax1.set_title('True Data (with interaction)')\n",
        "\n",
        "# Predictions without interaction\n",
        "ax2 = fig.add_subplot(132)\n",
        "y_pred_no_int = model_no_int.predict(X_test_int)\n",
        "ax2.scatter(y_test_int, y_pred_no_int, alpha=0.6)\n",
        "ax2.plot([y_test_int.min(), y_test_int.max()], \n",
        "         [y_test_int.min(), y_test_int.max()], 'r--', lw=2)\n",
        "ax2.set_xlabel('Actual')\n",
        "ax2.set_ylabel('Predicted')\n",
        "ax2.set_title(f'Without Interaction (R\u00b2={r2_no_int:.3f})')\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "# Predictions with interaction\n",
        "ax3 = fig.add_subplot(133)\n",
        "y_pred_with_int = model_with_int.predict(X_test_poly)\n",
        "ax3.scatter(y_test_int, y_pred_with_int, alpha=0.6, color='green')\n",
        "ax3.plot([y_test_int.min(), y_test_int.max()], \n",
        "         [y_test_int.min(), y_test_int.max()], 'r--', lw=2)\n",
        "ax3.set_xlabel('Actual')\n",
        "ax3.set_ylabel('Predicted')\n",
        "ax3.set_title(f'With Interaction (R\u00b2={r2_with_int:.3f})')\n",
        "ax3.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "regularization",
      "metadata": {},
      "source": [
        "## 3. Polynomial Features + Regularization\n",
        "\n",
        "### 3.1 The Need for Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "poly-regularization",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate complex data\n",
        "X_comp, y_comp = make_regression(\n",
        "    n_samples=100, n_features=5, n_informative=3, noise=10, random_state=42\n",
        ")\n",
        "\n",
        "X_train_comp, X_test_comp, y_train_comp, y_test_comp = train_test_split(\n",
        "    X_comp, y_comp, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Polynomial Features + Regularization\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Original features: {X_comp.shape[1]}\")\n",
        "\n",
        "results = []\n",
        "\n",
        "for degree in [1, 2, 3]:\n",
        "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    X_train_poly = poly.fit_transform(X_train_comp)\n",
        "    X_test_poly = poly.transform(X_test_comp)\n",
        "    \n",
        "    # Standardize\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_poly)\n",
        "    X_test_scaled = scaler.transform(X_test_poly)\n",
        "    \n",
        "    print(f\"\\nDegree {degree}: {X_train_poly.shape[1]} features\")\n",
        "    \n",
        "    # Linear Regression (no regularization)\n",
        "    lr = LinearRegression()\n",
        "    lr.fit(X_train_scaled, y_train_comp)\n",
        "    train_r2_lr = lr.score(X_train_scaled, y_train_comp)\n",
        "    test_r2_lr = lr.score(X_test_scaled, y_test_comp)\n",
        "    \n",
        "    # Ridge\n",
        "    ridge = Ridge(alpha=1.0)\n",
        "    ridge.fit(X_train_scaled, y_train_comp)\n",
        "    train_r2_ridge = ridge.score(X_train_scaled, y_train_comp)\n",
        "    test_r2_ridge = ridge.score(X_test_scaled, y_test_comp)\n",
        "    \n",
        "    # Lasso\n",
        "    lasso = Lasso(alpha=0.1, max_iter=10000)\n",
        "    lasso.fit(X_train_scaled, y_train_comp)\n",
        "    train_r2_lasso = lasso.score(X_train_scaled, y_train_comp)\n",
        "    test_r2_lasso = lasso.score(X_test_scaled, y_test_comp)\n",
        "    n_nonzero = np.sum(lasso.coef_ != 0)\n",
        "    \n",
        "    print(f\"  Linear Reg:  Train R\u00b2={train_r2_lr:.3f}, Test R\u00b2={test_r2_lr:.3f}\")\n",
        "    print(f\"  Ridge:       Train R\u00b2={train_r2_ridge:.3f}, Test R\u00b2={test_r2_ridge:.3f}\")\n",
        "    print(f\"  Lasso:       Train R\u00b2={train_r2_lasso:.3f}, Test R\u00b2={test_r2_lasso:.3f}, Features={n_nonzero}\")\n",
        "    \n",
        "    results.append({\n",
        "        'Degree': degree,\n",
        "        'N_Features': X_train_poly.shape[1],\n",
        "        'LR_Test': test_r2_lr,\n",
        "        'Ridge_Test': test_r2_ridge,\n",
        "        'Lasso_Test': test_r2_lasso,\n",
        "        'Lasso_Selected': n_nonzero\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Summary:\")\n",
        "print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "poly-reg-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Test R\u00b2 comparison\n",
        "x_pos = np.arange(len(results_df))\n",
        "width = 0.25\n",
        "\n",
        "axes[0].bar(x_pos - width, results_df['LR_Test'], width, label='Linear Reg', alpha=0.8)\n",
        "axes[0].bar(x_pos, results_df['Ridge_Test'], width, label='Ridge', alpha=0.8)\n",
        "axes[0].bar(x_pos + width, results_df['Lasso_Test'], width, label='Lasso', alpha=0.8)\n",
        "axes[0].set_xticks(x_pos)\n",
        "axes[0].set_xticklabels([f\"Degree {d}\" for d in results_df['Degree']])\n",
        "axes[0].set_ylabel('Test R\u00b2 Score')\n",
        "axes[0].set_title('Regularization Effect with Polynomial Features')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3, axis='y')\n",
        "\n",
        "# Feature selection by Lasso\n",
        "axes[1].bar(results_df['Degree'], results_df['N_Features'], alpha=0.5, label='Total features')\n",
        "axes[1].bar(results_df['Degree'], results_df['Lasso_Selected'], alpha=0.8, label='Lasso selected')\n",
        "axes[1].set_xlabel('Polynomial Degree')\n",
        "axes[1].set_ylabel('Number of Features')\n",
        "axes[1].set_title('Lasso Feature Selection')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Key Insight: Regularization is ESSENTIAL with polynomial features!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pipeline",
      "metadata": {},
      "source": [
        "## 4. Pipeline: Best Practice\n",
        "\n",
        "### 4.1 Creating a Polynomial Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pipeline-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create pipelines with different configurations\n",
        "print(\"Polynomial Regression Pipelines\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Pipeline 1: Polynomial + Linear Regression\n",
        "pipe_lr = make_pipeline(\n",
        "    PolynomialFeatures(degree=2),\n",
        "    StandardScaler(),\n",
        "    LinearRegression()\n",
        ")\n",
        "\n",
        "# Pipeline 2: Polynomial + Ridge\n",
        "pipe_ridge = make_pipeline(\n",
        "    PolynomialFeatures(degree=2),\n",
        "    StandardScaler(),\n",
        "    Ridge(alpha=1.0)\n",
        ")\n",
        "\n",
        "# Pipeline 3: Polynomial + Lasso\n",
        "pipe_lasso = make_pipeline(\n",
        "    PolynomialFeatures(degree=2),\n",
        "    StandardScaler(),\n",
        "    Lasso(alpha=0.1, max_iter=10000)\n",
        ")\n",
        "\n",
        "pipelines = {\n",
        "    'Poly + LR': pipe_lr,\n",
        "    'Poly + Ridge': pipe_ridge,\n",
        "    'Poly + Lasso': pipe_lasso\n",
        "}\n",
        "\n",
        "# Train and evaluate\n",
        "for name, pipeline in pipelines.items():\n",
        "    # Fit\n",
        "    pipeline.fit(X_train_comp, y_train_comp)\n",
        "    \n",
        "    # Evaluate\n",
        "    train_score = pipeline.score(X_train_comp, y_train_comp)\n",
        "    test_score = pipeline.score(X_test_comp, y_test_comp)\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Train R\u00b2: {train_score:.4f}\")\n",
        "    print(f\"  Test R\u00b2: {test_score:.4f}\")\n",
        "    print(f\"  Pipeline steps: {[step[0] for step in pipeline.steps]}\")\n",
        "\n",
        "print(\"\\n\u2713 Pipelines ensure correct preprocessing order!\")\n",
        "print(\"  1. PolynomialFeatures\")\n",
        "print(\"  2. StandardScaler\")\n",
        "print(\"  3. Model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "validation-curve",
      "metadata": {},
      "source": [
        "### 4.2 Tuning Polynomial Degree with Validation Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "validation-curve",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use validation curve to find optimal degree\n",
        "pipeline = Pipeline([\n",
        "    ('poly', PolynomialFeatures()),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('ridge', Ridge(alpha=1.0))\n",
        "])\n",
        "\n",
        "# Test different degrees\n",
        "param_range = range(1, 8)\n",
        "train_scores, test_scores = validation_curve(\n",
        "    pipeline, X_comp, y_comp,\n",
        "    param_name='poly__degree',\n",
        "    param_range=param_range,\n",
        "    cv=5,\n",
        "    scoring='r2'\n",
        ")\n",
        "\n",
        "# Calculate means and stds\n",
        "train_mean = np.mean(train_scores, axis=1)\n",
        "train_std = np.std(train_scores, axis=1)\n",
        "test_mean = np.mean(test_scores, axis=1)\n",
        "test_std = np.std(test_scores, axis=1)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(param_range, train_mean, 'o-', label='Training score', linewidth=2)\n",
        "plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.2)\n",
        "plt.plot(param_range, test_mean, 's-', label='Cross-validation score', linewidth=2)\n",
        "plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, alpha=0.2)\n",
        "\n",
        "# Mark optimal degree\n",
        "optimal_degree = param_range[np.argmax(test_mean)]\n",
        "plt.axvline(x=optimal_degree, color='r', linestyle='--', alpha=0.5, \n",
        "           label=f'Optimal degree: {optimal_degree}')\n",
        "\n",
        "plt.xlabel('Polynomial Degree')\n",
        "plt.ylabel('R\u00b2 Score')\n",
        "plt.title('Validation Curve: Polynomial Degree Selection')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nOptimal polynomial degree: {optimal_degree}\")\n",
        "print(f\"Cross-validation R\u00b2 at optimal: {test_mean[optimal_degree-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "real-dataset",
      "metadata": {},
      "source": [
        "## 5. Real Dataset: Diabetes with Polynomial Features\n",
        "\n",
        "### 5.1 Baseline vs Polynomial Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "diabetes-poly",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X_diabetes = diabetes.data\n",
        "y_diabetes = diabetes.target\n",
        "\n",
        "X_train_db, X_test_db, y_train_db, y_test_db = train_test_split(\n",
        "    X_diabetes, y_diabetes, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Diabetes Dataset: Polynomial Features Analysis\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Original features: {X_diabetes.shape[1]}\")\n",
        "print(f\"Feature names: {diabetes.feature_names}\\n\")\n",
        "\n",
        "# Test different polynomial degrees\n",
        "diabetes_results = []\n",
        "\n",
        "for degree in [1, 2]:\n",
        "    # Create pipeline\n",
        "    pipeline = make_pipeline(\n",
        "        PolynomialFeatures(degree=degree, include_bias=False),\n",
        "        StandardScaler(),\n",
        "        Ridge(alpha=1.0)\n",
        "    )\n",
        "    \n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(pipeline, X_diabetes, y_diabetes, cv=5, scoring='r2')\n",
        "    \n",
        "    # Fit and evaluate\n",
        "    pipeline.fit(X_train_db, y_train_db)\n",
        "    test_score = pipeline.score(X_test_db, y_test_db)\n",
        "    \n",
        "    # Get number of features after transformation\n",
        "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    X_transformed = poly.fit_transform(X_diabetes)\n",
        "    n_features = X_transformed.shape[1]\n",
        "    \n",
        "    diabetes_results.append({\n",
        "        'Degree': degree,\n",
        "        'N_Features': n_features,\n",
        "        'CV_Mean': cv_scores.mean(),\n",
        "        'CV_Std': cv_scores.std(),\n",
        "        'Test_R2': test_score\n",
        "    })\n",
        "    \n",
        "    print(f\"Degree {degree}:\")\n",
        "    print(f\"  Features: {n_features}\")\n",
        "    print(f\"  CV R\u00b2: {cv_scores.mean():.4f} (\u00b1{cv_scores.std():.4f})\")\n",
        "    print(f\"  Test R\u00b2: {test_score:.4f}\\n\")\n",
        "\n",
        "diabetes_df = pd.DataFrame(diabetes_results)\n",
        "print(\"=\"*70)\n",
        "print(diabetes_df.to_string(index=False))\n",
        "\n",
        "improvement = diabetes_df.loc[1, 'Test_R2'] - diabetes_df.loc[0, 'Test_R2']\n",
        "print(f\"\\nImprovement with degree 2: {improvement:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "custom-features",
      "metadata": {},
      "source": [
        "## 6. Custom Feature Engineering\n",
        "\n",
        "### 6.1 Interaction-Only Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "interaction-only",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interaction-only (no polynomial terms)\n",
        "poly_interaction = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
        "\n",
        "X_sample = np.array([[1, 2, 3]])\n",
        "X_full = PolynomialFeatures(degree=2, include_bias=False).fit_transform(X_sample)\n",
        "X_interaction = poly_interaction.fit_transform(X_sample)\n",
        "\n",
        "print(\"Interaction-Only Features\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Input: {X_sample[0]}\\n\")\n",
        "\n",
        "print(\"Full polynomial (degree=2):\")\n",
        "print(f\"  Features: {PolynomialFeatures(degree=2, include_bias=False).fit(X_sample).get_feature_names_out(['x1', 'x2', 'x3'])}\")\n",
        "print(f\"  Output: {X_full[0]}\\n\")\n",
        "\n",
        "print(\"Interaction-only (no x\u00b2):\")\n",
        "print(f\"  Features: {poly_interaction.get_feature_names_out(['x1', 'x2', 'x3'])}\")\n",
        "print(f\"  Output: {X_interaction[0]}\\n\")\n",
        "\n",
        "print(\"\ud83d\udca1 Interaction-only excludes squared terms (x1\u00b2, x2\u00b2, x3\u00b2)\")\n",
        "print(\"   Useful when you only want interactions between different features!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "manual-interactions",
      "metadata": {},
      "source": [
        "### 6.2 Manual Interaction Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "manual-create",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create specific interactions manually\n",
        "def create_custom_interactions(X, interactions):\n",
        "    \"\"\"\n",
        "    Create specific interaction terms.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "    interactions : list of tuples\n",
        "        Each tuple contains indices of features to interact\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    X_with_interactions : array with original features + interactions\n",
        "    \"\"\"\n",
        "    X_interactions = []\n",
        "    \n",
        "    for i, j in interactions:\n",
        "        X_interactions.append((X[:, i] * X[:, j]).reshape(-1, 1))\n",
        "    \n",
        "    if X_interactions:\n",
        "        return np.hstack([X] + X_interactions)\n",
        "    return X\n",
        "\n",
        "# Example\n",
        "X_example = np.random.rand(100, 4)\n",
        "\n",
        "# Only create interactions between features 0&1, and 2&3\n",
        "interactions = [(0, 1), (2, 3)]\n",
        "X_custom = create_custom_interactions(X_example, interactions)\n",
        "\n",
        "print(\"Custom Interaction Creation\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Original shape: {X_example.shape}\")\n",
        "print(f\"With custom interactions: {X_custom.shape}\")\n",
        "print(f\"\\nAdded interactions:\")\n",
        "print(f\"  - Feature 0 \u00d7 Feature 1\")\n",
        "print(f\"  - Feature 2 \u00d7 Feature 3\")\n",
        "print(f\"\\n\ud83d\udca1 This gives you full control over which interactions to include!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Summary and Quick Reference\n",
        "\n",
        "### Quick Start Template\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Best practice pipeline\n",
        "model = make_pipeline(\n",
        "    PolynomialFeatures(degree=2, include_bias=False),\n",
        "    StandardScaler(),\n",
        "    Ridge(alpha=1.0)\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "```\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "**PolynomialFeatures**:\n",
        "- `degree`: Polynomial degree (typically 2-3)\n",
        "- `interaction_only`: Only cross-products, no x\u00b2 terms\n",
        "- `include_bias`: Add constant term (use False in pipelines)\n",
        "\n",
        "### When to Use Polynomial Features\n",
        "\n",
        "\u2713 **Good for:**\n",
        "- Non-linear relationships\n",
        "- Feature interactions matter\n",
        "- Smooth curves needed\n",
        "- Small to moderate number of original features\n",
        "\n",
        "\u2717 **Not ideal for:**\n",
        "- Already high-dimensional data (> 50 features)\n",
        "- Linear relationships\n",
        "- When interpretability is critical\n",
        "- Very limited training data\n",
        "\n",
        "### Feature Count Formula\n",
        "\n",
        "For \\(n\\) features and degree \\(d\\):\n",
        "\n",
        "| n | degree=2 | degree=3 | degree=4 |\n",
        "|---|----------|----------|----------|\n",
        "| 2 | 5 | 9 | 14 |\n",
        "| 5 | 20 | 55 | 125 |\n",
        "| 10 | 65 | 285 | 1,000 |\n",
        "| 20 | 230 | 1,770 | 10,626 |\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. \u2713 Always use pipelines\n",
        "2. \u2713 Standardize after polynomial transform\n",
        "3. \u2713 Use regularization (Ridge/Lasso)\n",
        "4. \u2713 Start with degree=2\n",
        "5. \u2713 Use cross-validation\n",
        "6. \u2713 Check feature explosion\n",
        "7. \u2713 Consider interaction_only\n",
        "8. \u2713 Monitor train vs test gap\n",
        "\n",
        "### Common Pitfalls\n",
        "\n",
        "1. \u274c Not standardizing\n",
        "2. \u274c Using high degrees blindly\n",
        "3. \u274c No regularization\n",
        "4. \u274c Standardizing before polynomial\n",
        "5. \u274c Ignoring feature count\n",
        "6. \u274c Not using cross-validation\n",
        "7. \u274c Including bias in pipeline\n",
        "8. \u274c Overfitting to training data\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Logistic regression with polynomial features\n",
        "- Kernel methods (polynomial kernel in SVM)\n",
        "- Spline regression for smoother curves\n",
        "- Feature selection with high-dimensional polynomial features"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}