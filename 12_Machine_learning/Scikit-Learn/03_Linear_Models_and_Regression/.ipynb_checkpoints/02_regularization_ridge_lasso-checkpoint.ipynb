{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Regularization: Ridge, Lasso, and ElasticNet\n",
        "\n",
        "## Overview\n",
        "\n",
        "**Regularization** adds a penalty term to the loss function to prevent overfitting and improve model generalization. It's particularly useful when:\n",
        "- You have many features (high-dimensional data)\n",
        "- Features are correlated (multicollinearity)\n",
        "- Training data is limited\n",
        "\n",
        "## Mathematical Foundation\n",
        "\n",
        "### Ordinary Least Squares (OLS)\n",
        "\\[\n",
        "\\min_{\\beta} \\sum_{i=1}^{n} (y_i - X_i\\beta)^2\n",
        "\\]\n",
        "\n",
        "### Ridge Regression (L2 Regularization)\n",
        "\\[\n",
        "\\min_{\\beta} \\sum_{i=1}^{n} (y_i - X_i\\beta)^2 + \\alpha \\sum_{j=1}^{p} \\beta_j^2\n",
        "\\]\n",
        "- **Penalty**: L2 norm (sum of squared coefficients)\n",
        "- **Effect**: Shrinks coefficients toward zero\n",
        "- **Result**: All features retained, but with smaller weights\n",
        "\n",
        "### Lasso Regression (L1 Regularization)\n",
        "\\[\n",
        "\\min_{\\beta} \\sum_{i=1}^{n} (y_i - X_i\\beta)^2 + \\alpha \\sum_{j=1}^{p} |\\beta_j|\n",
        "\\]\n",
        "- **Penalty**: L1 norm (sum of absolute coefficients)\n",
        "- **Effect**: Can shrink coefficients to exactly zero\n",
        "- **Result**: Feature selection (sparse model)\n",
        "\n",
        "### ElasticNet (L1 + L2)\n",
        "\\[\n",
        "\\min_{\\beta} \\sum_{i=1}^{n} (y_i - X_i\\beta)^2 + \\alpha \\rho \\sum_{j=1}^{p} |\\beta_j| + \\frac{\\alpha(1-\\rho)}{2} \\sum_{j=1}^{p} \\beta_j^2\n",
        "\\]\n",
        "- **Combines**: L1 and L2 penalties\n",
        "- **Parameter** \\(\\rho\\): Controls L1/L2 ratio (l1_ratio in sklearn)\n",
        "- **Result**: Feature selection + coefficient shrinkage\n",
        "\n",
        "## Topics Covered\n",
        "\n",
        "1. Ridge Regression (L2)\n",
        "2. Lasso Regression (L1)\n",
        "3. ElasticNet (L1 + L2)\n",
        "4. Coefficient paths\n",
        "5. Hyperparameter tuning (\\(\\alpha\\))\n",
        "6. Feature selection with Lasso\n",
        "7. Comparison and when to use each"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, RidgeCV, LassoCV, ElasticNetCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.datasets import make_regression, load_diabetes, fetch_california_housing\n",
        "\n",
        "np.random.seed(42)\n",
        "sns.set_style('whitegrid')\n",
        "print(\"\u2713 Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "problem-demo",
      "metadata": {},
      "source": [
        "## 1. The Problem: Overfitting and Multicollinearity\n",
        "\n",
        "### 1.1 Demonstrating the Need for Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "problem-setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate data with many features\n",
        "X, y = make_regression(\n",
        "    n_samples=100,\n",
        "    n_features=50,\n",
        "    n_informative=10,\n",
        "    n_redundant=20,\n",
        "    noise=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"High-Dimensional Dataset\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X.shape[0]}\")\n",
        "print(f\"Features: {X.shape[1]}\")\n",
        "print(f\"Ratio: {X.shape[1]/X.shape[0]:.2f} features per sample\")\n",
        "print(f\"\\n\u26a0\ufe0f More features than ideal relative to samples!\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"\\nTrain: {X_train.shape[0]} samples\")\n",
        "print(f\"Test: {X_test.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ols-baseline",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train regular linear regression (OLS)\n",
        "ols_model = LinearRegression()\n",
        "ols_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred_ols = ols_model.predict(X_train_scaled)\n",
        "y_test_pred_ols = ols_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate\n",
        "train_r2_ols = r2_score(y_train, y_train_pred_ols)\n",
        "test_r2_ols = r2_score(y_test, y_test_pred_ols)\n",
        "train_mse_ols = mean_squared_error(y_train, y_train_pred_ols)\n",
        "test_mse_ols = mean_squared_error(y_test, y_test_pred_ols)\n",
        "\n",
        "print(\"Ordinary Least Squares (No Regularization)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTrain R\u00b2: {train_r2_ols:.4f}\")\n",
        "print(f\"Test R\u00b2:  {test_r2_ols:.4f}\")\n",
        "print(f\"\\nTrain MSE: {train_mse_ols:.2f}\")\n",
        "print(f\"Test MSE:  {test_mse_ols:.2f}\")\n",
        "print(f\"\\nCoefficient statistics:\")\n",
        "print(f\"  Mean: {np.mean(ols_model.coef_):.2f}\")\n",
        "print(f\"  Std: {np.std(ols_model.coef_):.2f}\")\n",
        "print(f\"  Max: {np.max(np.abs(ols_model.coef_)):.2f}\")\n",
        "\n",
        "if train_r2_ols - test_r2_ols > 0.1:\n",
        "    print(f\"\\n\u26a0\ufe0f Warning: Large gap between train and test R\u00b2 ({train_r2_ols - test_r2_ols:.3f})\")\n",
        "    print(\"   \u2192 Model may be overfitting!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ridge",
      "metadata": {},
      "source": [
        "## 2. Ridge Regression (L2 Regularization)\n",
        "\n",
        "### 2.1 Basic Ridge Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ridge-basic",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Ridge with different alpha values\n",
        "alphas = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "ridge_results = []\n",
        "\n",
        "print(\"Ridge Regression with Different Alpha Values\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for alpha in alphas:\n",
        "    # Train model\n",
        "    ridge = Ridge(alpha=alpha)\n",
        "    ridge.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Predictions\n",
        "    y_train_pred = ridge.predict(X_train_scaled)\n",
        "    y_test_pred = ridge.predict(X_test_scaled)\n",
        "    \n",
        "    # Evaluate\n",
        "    train_r2 = r2_score(y_train, y_train_pred)\n",
        "    test_r2 = r2_score(y_test, y_test_pred)\n",
        "    \n",
        "    ridge_results.append({\n",
        "        'Alpha': alpha,\n",
        "        'Train R\u00b2': train_r2,\n",
        "        'Test R\u00b2': test_r2,\n",
        "        'Coef Mean': np.mean(np.abs(ridge.coef_)),\n",
        "        'Coef Max': np.max(np.abs(ridge.coef_))\n",
        "    })\n",
        "    \n",
        "    print(f\"\\n\u03b1 = {alpha:6.2f}: Train R\u00b2 = {train_r2:.4f}, Test R\u00b2 = {test_r2:.4f}\")\n",
        "\n",
        "ridge_df = pd.DataFrame(ridge_results)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Summary:\")\n",
        "print(ridge_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ridge-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize effect of alpha\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# R\u00b2 scores\n",
        "axes[0].plot(ridge_df['Alpha'], ridge_df['Train R\u00b2'], 'o-', label='Train R\u00b2', linewidth=2)\n",
        "axes[0].plot(ridge_df['Alpha'], ridge_df['Test R\u00b2'], 's-', label='Test R\u00b2', linewidth=2)\n",
        "axes[0].axhline(y=test_r2_ols, color='r', linestyle='--', label='OLS Test R\u00b2', alpha=0.5)\n",
        "axes[0].set_xscale('log')\n",
        "axes[0].set_xlabel('Alpha (log scale)')\n",
        "axes[0].set_ylabel('R\u00b2 Score')\n",
        "axes[0].set_title('Ridge: R\u00b2 vs Alpha')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Coefficient magnitude\n",
        "axes[1].plot(ridge_df['Alpha'], ridge_df['Coef Mean'], 'o-', label='Mean |coef|', linewidth=2)\n",
        "axes[1].plot(ridge_df['Alpha'], ridge_df['Coef Max'], 's-', label='Max |coef|', linewidth=2)\n",
        "axes[1].set_xscale('log')\n",
        "axes[1].set_xlabel('Alpha (log scale)')\n",
        "axes[1].set_ylabel('Coefficient Magnitude')\n",
        "axes[1].set_title('Ridge: Coefficient Shrinkage')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Observations:\")\n",
        "print(\"  - As \u03b1 increases, coefficients shrink toward zero\")\n",
        "print(\"  - Higher \u03b1 = more regularization = lower train R\u00b2 but better test R\u00b2\")\n",
        "print(\"  - Find optimal \u03b1 that balances bias-variance tradeoff\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ridge-cv",
      "metadata": {},
      "source": [
        "### 2.2 Automatic Alpha Selection with RidgeCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ridge-cv",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use RidgeCV to find best alpha automatically\n",
        "alphas_cv = np.logspace(-3, 3, 100)\n",
        "\n",
        "ridge_cv = RidgeCV(alphas=alphas_cv, cv=5, scoring='r2')\n",
        "ridge_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"RidgeCV: Automatic Alpha Selection\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nBest alpha: {ridge_cv.alpha_:.4f}\")\n",
        "print(f\"Best CV score (R\u00b2): {ridge_cv.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "y_test_pred_ridge = ridge_cv.predict(X_test_scaled)\n",
        "test_r2_ridge = r2_score(y_test, y_test_pred_ridge)\n",
        "test_mse_ridge = mean_squared_error(y_test, y_test_pred_ridge)\n",
        "\n",
        "print(f\"\\nTest Performance:\")\n",
        "print(f\"  R\u00b2: {test_r2_ridge:.4f}\")\n",
        "print(f\"  MSE: {test_mse_ridge:.2f}\")\n",
        "\n",
        "print(f\"\\nComparison with OLS:\")\n",
        "print(f\"  OLS Test R\u00b2: {test_r2_ols:.4f}\")\n",
        "print(f\"  Ridge Test R\u00b2: {test_r2_ridge:.4f}\")\n",
        "print(f\"  Improvement: {test_r2_ridge - test_r2_ols:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lasso",
      "metadata": {},
      "source": [
        "## 3. Lasso Regression (L1 Regularization)\n",
        "\n",
        "### 3.1 Basic Lasso Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lasso-basic",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Lasso with different alpha values\n",
        "alphas_lasso = [0.001, 0.01, 0.1, 1.0, 10.0]\n",
        "lasso_results = []\n",
        "\n",
        "print(\"Lasso Regression with Different Alpha Values\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for alpha in alphas_lasso:\n",
        "    # Train model\n",
        "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
        "    lasso.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Predictions\n",
        "    y_train_pred = lasso.predict(X_train_scaled)\n",
        "    y_test_pred = lasso.predict(X_test_scaled)\n",
        "    \n",
        "    # Evaluate\n",
        "    train_r2 = r2_score(y_train, y_train_pred)\n",
        "    test_r2 = r2_score(y_test, y_test_pred)\n",
        "    \n",
        "    # Count non-zero coefficients\n",
        "    n_nonzero = np.sum(lasso.coef_ != 0)\n",
        "    \n",
        "    lasso_results.append({\n",
        "        'Alpha': alpha,\n",
        "        'Train R\u00b2': train_r2,\n",
        "        'Test R\u00b2': test_r2,\n",
        "        'Non-zero Coefs': n_nonzero,\n",
        "        'Zero Coefs': X.shape[1] - n_nonzero\n",
        "    })\n",
        "    \n",
        "    print(f\"\\n\u03b1 = {alpha:6.3f}: Train R\u00b2 = {train_r2:.4f}, Test R\u00b2 = {test_r2:.4f}, \"\n",
        "          f\"Non-zero coefs = {n_nonzero}/{X.shape[1]}\")\n",
        "\n",
        "lasso_df = pd.DataFrame(lasso_results)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Summary:\")\n",
        "print(lasso_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lasso-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Lasso results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# R\u00b2 scores\n",
        "axes[0].plot(lasso_df['Alpha'], lasso_df['Train R\u00b2'], 'o-', label='Train R\u00b2', linewidth=2)\n",
        "axes[0].plot(lasso_df['Alpha'], lasso_df['Test R\u00b2'], 's-', label='Test R\u00b2', linewidth=2)\n",
        "axes[0].set_xscale('log')\n",
        "axes[0].set_xlabel('Alpha (log scale)')\n",
        "axes[0].set_ylabel('R\u00b2 Score')\n",
        "axes[0].set_title('Lasso: R\u00b2 vs Alpha')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Feature selection\n",
        "axes[1].plot(lasso_df['Alpha'], lasso_df['Non-zero Coefs'], 'o-', color='green', \n",
        "            linewidth=2, label='Non-zero coefficients')\n",
        "axes[1].plot(lasso_df['Alpha'], lasso_df['Zero Coefs'], 's-', color='red', \n",
        "            linewidth=2, label='Zero coefficients')\n",
        "axes[1].set_xscale('log')\n",
        "axes[1].set_xlabel('Alpha (log scale)')\n",
        "axes[1].set_ylabel('Number of Coefficients')\n",
        "axes[1].set_title('Lasso: Feature Selection')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Key Difference from Ridge:\")\n",
        "print(\"  - Lasso sets coefficients to EXACTLY zero\")\n",
        "print(\"  - Performs automatic feature selection\")\n",
        "print(\"  - Sparse solutions (fewer features)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lasso-cv",
      "metadata": {},
      "source": [
        "### 3.2 Automatic Alpha Selection with LassoCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lasso-cv",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use LassoCV to find best alpha\n",
        "lasso_cv = LassoCV(alphas=None, cv=5, max_iter=10000, random_state=42)\n",
        "lasso_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"LassoCV: Automatic Alpha Selection\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nBest alpha: {lasso_cv.alpha_:.6f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "y_test_pred_lasso = lasso_cv.predict(X_test_scaled)\n",
        "test_r2_lasso = r2_score(y_test, y_test_pred_lasso)\n",
        "test_mse_lasso = mean_squared_error(y_test, y_test_pred_lasso)\n",
        "\n",
        "# Feature selection\n",
        "n_nonzero = np.sum(lasso_cv.coef_ != 0)\n",
        "selected_features = np.where(lasso_cv.coef_ != 0)[0]\n",
        "\n",
        "print(f\"\\nTest Performance:\")\n",
        "print(f\"  R\u00b2: {test_r2_lasso:.4f}\")\n",
        "print(f\"  MSE: {test_mse_lasso:.2f}\")\n",
        "\n",
        "print(f\"\\nFeature Selection:\")\n",
        "print(f\"  Total features: {X.shape[1]}\")\n",
        "print(f\"  Selected features: {n_nonzero}\")\n",
        "print(f\"  Eliminated features: {X.shape[1] - n_nonzero}\")\n",
        "print(f\"  Sparsity: {(1 - n_nonzero/X.shape[1])*100:.1f}%\")\n",
        "\n",
        "print(f\"\\nSelected feature indices: {selected_features[:10]}...\" if len(selected_features) > 10 else f\"\\nSelected feature indices: {selected_features}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "coefficient-paths",
      "metadata": {},
      "source": [
        "## 4. Coefficient Paths\n",
        "\n",
        "### 4.1 Ridge Coefficient Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ridge-path",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute Ridge coefficients for many alphas\n",
        "alphas_path = np.logspace(-3, 3, 100)\n",
        "coefs_ridge = []\n",
        "\n",
        "for alpha in alphas_path:\n",
        "    ridge = Ridge(alpha=alpha)\n",
        "    ridge.fit(X_train_scaled, y_train)\n",
        "    coefs_ridge.append(ridge.coef_)\n",
        "\n",
        "coefs_ridge = np.array(coefs_ridge)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(min(20, coefs_ridge.shape[1])):  # Plot first 20 features\n",
        "    plt.plot(alphas_path, coefs_ridge[:, i], alpha=0.7)\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Alpha (log scale)', fontsize=12)\n",
        "plt.ylabel('Coefficient Value', fontsize=12)\n",
        "plt.title('Ridge Coefficient Path', fontsize=14)\n",
        "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Ridge Coefficient Path:\")\n",
        "print(\"  - As \u03b1 increases, all coefficients shrink toward zero\")\n",
        "print(\"  - No coefficients reach exactly zero\")\n",
        "print(\"  - Smooth, continuous shrinkage\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lasso-path",
      "metadata": {},
      "source": [
        "### 4.2 Lasso Coefficient Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lasso-path",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute Lasso coefficients for many alphas\n",
        "alphas_path_lasso = np.logspace(-3, 1, 100)\n",
        "coefs_lasso = []\n",
        "\n",
        "for alpha in alphas_path_lasso:\n",
        "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
        "    lasso.fit(X_train_scaled, y_train)\n",
        "    coefs_lasso.append(lasso.coef_)\n",
        "\n",
        "coefs_lasso = np.array(coefs_lasso)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(min(20, coefs_lasso.shape[1])):  # Plot first 20 features\n",
        "    plt.plot(alphas_path_lasso, coefs_lasso[:, i], alpha=0.7)\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Alpha (log scale)', fontsize=12)\n",
        "plt.ylabel('Coefficient Value', fontsize=12)\n",
        "plt.title('Lasso Coefficient Path', fontsize=14)\n",
        "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Lasso Coefficient Path:\")\n",
        "print(\"  - As \u03b1 increases, coefficients drop to zero\")\n",
        "print(\"  - Features eliminated at different \u03b1 values\")\n",
        "print(\"  - Piecewise linear paths (characteristic of L1)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "side-by-side",
      "metadata": {},
      "source": [
        "### 4.3 Side-by-Side Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comparison-paths",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare Ridge and Lasso paths\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Ridge\n",
        "for i in range(min(15, coefs_ridge.shape[1])):\n",
        "    axes[0].plot(alphas_path, coefs_ridge[:, i], alpha=0.7)\n",
        "axes[0].set_xscale('log')\n",
        "axes[0].set_xlabel('Alpha (log scale)', fontsize=12)\n",
        "axes[0].set_ylabel('Coefficient Value', fontsize=12)\n",
        "axes[0].set_title('Ridge: Continuous Shrinkage', fontsize=14)\n",
        "axes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Lasso\n",
        "for i in range(min(15, coefs_lasso.shape[1])):\n",
        "    axes[1].plot(alphas_path_lasso, coefs_lasso[:, i], alpha=0.7)\n",
        "axes[1].set_xscale('log')\n",
        "axes[1].set_xlabel('Alpha (log scale)', fontsize=12)\n",
        "axes[1].set_ylabel('Coefficient Value', fontsize=12)\n",
        "axes[1].set_title('Lasso: Sparse Solutions', fontsize=14)\n",
        "axes[1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "elasticnet",
      "metadata": {},
      "source": [
        "## 5. ElasticNet (L1 + L2)\n",
        "\n",
        "### 5.1 Basic ElasticNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "elasticnet-basic",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ElasticNet with different l1_ratio values\n",
        "l1_ratios = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "alpha_en = 0.1\n",
        "\n",
        "en_results = []\n",
        "\n",
        "print(\"ElasticNet with Different L1 Ratios (\u03b1 = 0.1)\")\n",
        "print(\"=\"*70)\n",
        "print(\"l1_ratio = 0.0 \u2192 Pure Ridge\")\n",
        "print(\"l1_ratio = 1.0 \u2192 Pure Lasso\\n\")\n",
        "\n",
        "for l1_ratio in l1_ratios:\n",
        "    # Train model\n",
        "    en = ElasticNet(alpha=alpha_en, l1_ratio=l1_ratio, max_iter=10000)\n",
        "    en.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Evaluate\n",
        "    y_test_pred = en.predict(X_test_scaled)\n",
        "    test_r2 = r2_score(y_test, y_test_pred)\n",
        "    n_nonzero = np.sum(en.coef_ != 0)\n",
        "    \n",
        "    en_results.append({\n",
        "        'L1 Ratio': l1_ratio,\n",
        "        'Test R\u00b2': test_r2,\n",
        "        'Non-zero Coefs': n_nonzero,\n",
        "        'Sparsity %': (1 - n_nonzero/X.shape[1])*100\n",
        "    })\n",
        "    \n",
        "    print(f\"l1_ratio = {l1_ratio:.1f}: Test R\u00b2 = {test_r2:.4f}, \"\n",
        "          f\"Non-zero = {n_nonzero}, Sparsity = {(1 - n_nonzero/X.shape[1])*100:.1f}%\")\n",
        "\n",
        "en_df = pd.DataFrame(en_results)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Summary:\")\n",
        "print(en_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\ud83d\udca1 ElasticNet Benefits:\")\n",
        "print(\"  - Combines Ridge and Lasso advantages\")\n",
        "print(\"  - More stable than Lasso when features are correlated\")\n",
        "print(\"  - Still performs feature selection\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "elasticnet-cv",
      "metadata": {},
      "source": [
        "### 5.2 ElasticNetCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "elasticnet-cv",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use ElasticNetCV\n",
        "en_cv = ElasticNetCV(l1_ratio=[0.1, 0.3, 0.5, 0.7, 0.9], \n",
        "                     cv=5, max_iter=10000, random_state=42)\n",
        "en_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"ElasticNetCV: Automatic Hyperparameter Selection\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nBest alpha: {en_cv.alpha_:.6f}\")\n",
        "print(f\"Best l1_ratio: {en_cv.l1_ratio_:.2f}\")\n",
        "\n",
        "# Evaluate\n",
        "y_test_pred_en = en_cv.predict(X_test_scaled)\n",
        "test_r2_en = r2_score(y_test, y_test_pred_en)\n",
        "test_mse_en = mean_squared_error(y_test, y_test_pred_en)\n",
        "n_nonzero_en = np.sum(en_cv.coef_ != 0)\n",
        "\n",
        "print(f\"\\nTest Performance:\")\n",
        "print(f\"  R\u00b2: {test_r2_en:.4f}\")\n",
        "print(f\"  MSE: {test_mse_en:.2f}\")\n",
        "print(f\"\\nFeature Selection:\")\n",
        "print(f\"  Selected: {n_nonzero_en}/{X.shape[1]} features\")\n",
        "print(f\"  Sparsity: {(1 - n_nonzero_en/X.shape[1])*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comparison",
      "metadata": {},
      "source": [
        "## 6. Complete Comparison\n",
        "\n",
        "### 6.1 Performance Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "final-comparison",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all methods\n",
        "comparison = pd.DataFrame([\n",
        "    {\n",
        "        'Method': 'OLS (No Regularization)',\n",
        "        'Train R\u00b2': train_r2_ols,\n",
        "        'Test R\u00b2': test_r2_ols,\n",
        "        'Test MSE': test_mse_ols,\n",
        "        'Non-zero Coefs': X.shape[1],\n",
        "        'Alpha': '-'\n",
        "    },\n",
        "    {\n",
        "        'Method': 'Ridge',\n",
        "        'Train R\u00b2': ridge_cv.best_score_,\n",
        "        'Test R\u00b2': test_r2_ridge,\n",
        "        'Test MSE': test_mse_ridge,\n",
        "        'Non-zero Coefs': X.shape[1],\n",
        "        'Alpha': f\"{ridge_cv.alpha_:.4f}\"\n",
        "    },\n",
        "    {\n",
        "        'Method': 'Lasso',\n",
        "        'Train R\u00b2': r2_score(y_train, lasso_cv.predict(X_train_scaled)),\n",
        "        'Test R\u00b2': test_r2_lasso,\n",
        "        'Test MSE': test_mse_lasso,\n",
        "        'Non-zero Coefs': n_nonzero,\n",
        "        'Alpha': f\"{lasso_cv.alpha_:.6f}\"\n",
        "    },\n",
        "    {\n",
        "        'Method': 'ElasticNet',\n",
        "        'Train R\u00b2': r2_score(y_train, en_cv.predict(X_train_scaled)),\n",
        "        'Test R\u00b2': test_r2_en,\n",
        "        'Test MSE': test_mse_en,\n",
        "        'Non-zero Coefs': n_nonzero_en,\n",
        "        'Alpha': f\"{en_cv.alpha_:.6f}\"\n",
        "    }\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL COMPARISON: All Regularization Methods\")\n",
        "print(\"=\"*80)\n",
        "print(comparison.to_string(index=False))\n",
        "\n",
        "# Find best method\n",
        "best_idx = comparison['Test R\u00b2'].idxmax()\n",
        "best_method = comparison.loc[best_idx]\n",
        "\n",
        "print(f\"\\n\ud83c\udfc6 Best Method: {best_method['Method']}\")\n",
        "print(f\"   Test R\u00b2: {best_method['Test R\u00b2']:.4f}\")\n",
        "print(f\"   Features used: {int(best_method['Non-zero Coefs'])}/{X.shape[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comparison-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# R\u00b2 scores\n",
        "methods = comparison['Method']\n",
        "x_pos = np.arange(len(methods))\n",
        "\n",
        "axes[0].bar(x_pos - 0.2, comparison['Train R\u00b2'], 0.4, label='Train R\u00b2', alpha=0.8)\n",
        "axes[0].bar(x_pos + 0.2, comparison['Test R\u00b2'], 0.4, label='Test R\u00b2', alpha=0.8)\n",
        "axes[0].set_xticks(x_pos)\n",
        "axes[0].set_xticklabels(methods, rotation=15, ha='right')\n",
        "axes[0].set_ylabel('R\u00b2 Score')\n",
        "axes[0].set_title('Performance Comparison')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3, axis='y')\n",
        "\n",
        "# Feature usage\n",
        "axes[1].bar(x_pos, comparison['Non-zero Coefs'], alpha=0.8)\n",
        "axes[1].axhline(y=X.shape[1], color='r', linestyle='--', label=f'Total features ({X.shape[1]})')\n",
        "axes[1].set_xticks(x_pos)\n",
        "axes[1].set_xticklabels(methods, rotation=15, ha='right')\n",
        "axes[1].set_ylabel('Number of Features')\n",
        "axes[1].set_title('Feature Selection')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "real-dataset",
      "metadata": {},
      "source": [
        "## 7. Real Dataset Example: Diabetes\n",
        "\n",
        "### 7.1 Apply All Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "diabetes-all",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X_db = diabetes.data\n",
        "y_db = diabetes.target\n",
        "\n",
        "# Split and scale\n",
        "X_train_db, X_test_db, y_train_db, y_test_db = train_test_split(\n",
        "    X_db, y_db, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "scaler_db = StandardScaler()\n",
        "X_train_db_scaled = scaler_db.fit_transform(X_train_db)\n",
        "X_test_db_scaled = scaler_db.transform(X_test_db)\n",
        "\n",
        "print(\"Diabetes Dataset Regularization Comparison\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Features: {diabetes.feature_names}\")\n",
        "print(f\"Samples: {X_db.shape[0]}, Features: {X_db.shape[1]}\")\n",
        "\n",
        "# Train all models\n",
        "models = {\n",
        "    'OLS': LinearRegression(),\n",
        "    'Ridge': RidgeCV(alphas=np.logspace(-3, 3, 100), cv=5),\n",
        "    'Lasso': LassoCV(cv=5, max_iter=10000, random_state=42),\n",
        "    'ElasticNet': ElasticNetCV(l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99], \n",
        "                               cv=5, max_iter=10000, random_state=42)\n",
        "}\n",
        "\n",
        "diabetes_results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_db_scaled, y_train_db)\n",
        "    y_pred = model.predict(X_test_db_scaled)\n",
        "    \n",
        "    r2 = r2_score(y_test_db, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_db, y_pred))\n",
        "    mae = mean_absolute_error(y_test_db, y_pred)\n",
        "    \n",
        "    n_nonzero = np.sum(model.coef_ != 0) if hasattr(model, 'coef_') else X_db.shape[1]\n",
        "    \n",
        "    diabetes_results.append({\n",
        "        'Method': name,\n",
        "        'R\u00b2': r2,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'Features': n_nonzero\n",
        "    })\n",
        "\n",
        "diabetes_df = pd.DataFrame(diabetes_results)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(diabetes_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "diabetes-features",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare feature selection\n",
        "feature_comparison = pd.DataFrame({\n",
        "    'Feature': diabetes.feature_names,\n",
        "    'OLS': models['OLS'].coef_,\n",
        "    'Ridge': models['Ridge'].coef_,\n",
        "    'Lasso': models['Lasso'].coef_,\n",
        "    'ElasticNet': models['ElasticNet'].coef_\n",
        "})\n",
        "\n",
        "print(\"\\nFeature Coefficients Comparison:\")\n",
        "print(\"=\"*70)\n",
        "print(feature_comparison.to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "x_pos = np.arange(len(diabetes.feature_names))\n",
        "width = 0.2\n",
        "\n",
        "ax.bar(x_pos - 1.5*width, feature_comparison['OLS'], width, label='OLS', alpha=0.8)\n",
        "ax.bar(x_pos - 0.5*width, feature_comparison['Ridge'], width, label='Ridge', alpha=0.8)\n",
        "ax.bar(x_pos + 0.5*width, feature_comparison['Lasso'], width, label='Lasso', alpha=0.8)\n",
        "ax.bar(x_pos + 1.5*width, feature_comparison['ElasticNet'], width, label='ElasticNet', alpha=0.8)\n",
        "\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(diabetes.feature_names, rotation=45, ha='right')\n",
        "ax.set_ylabel('Coefficient Value')\n",
        "ax.set_title('Diabetes: Coefficient Comparison Across Methods')\n",
        "ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Notice how Lasso/ElasticNet set some coefficients to zero!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Summary and Decision Guide\n",
        "\n",
        "### Quick Reference\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet, RidgeCV, LassoCV, ElasticNetCV\n",
        "\n",
        "# Ridge (L2) - shrinks all coefficients\n",
        "ridge = RidgeCV(alphas=np.logspace(-3, 3, 100), cv=5)\n",
        "ridge.fit(X_train, y_train)\n",
        "\n",
        "# Lasso (L1) - feature selection\n",
        "lasso = LassoCV(cv=5, max_iter=10000)\n",
        "lasso.fit(X_train, y_train)\n",
        "\n",
        "# ElasticNet (L1 + L2) - best of both\n",
        "elastic = ElasticNetCV(l1_ratio=[0.1, 0.5, 0.9], cv=5, max_iter=10000)\n",
        "elastic.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "### When to Use Each Method\n",
        "\n",
        "| Scenario | Recommended Method | Reason |\n",
        "|----------|-------------------|--------|\n",
        "| Many features, all relevant | **Ridge** | Keeps all features, reduces coefficients |\n",
        "| Many features, some irrelevant | **Lasso** | Automatic feature selection |\n",
        "| Correlated features | **ElasticNet** | More stable than Lasso |\n",
        "| Small dataset, many features | **Ridge or ElasticNet** | Prevents overfitting |\n",
        "| Need interpretable model | **Lasso** | Sparse solution, clear feature importance |\n",
        "| Multicollinearity present | **Ridge** | Handles correlated features well |\n",
        "| Feature selection + stability | **ElasticNet** | Combines advantages of both |\n",
        "\n",
        "### Key Differences\n",
        "\n",
        "| Property | Ridge (L2) | Lasso (L1) | ElasticNet |\n",
        "|----------|------------|------------|------------|\n",
        "| Penalty | \\(\\sum \\beta_j^2\\) | \\(\\sum |\\beta_j|\\) | L1 + L2 |\n",
        "| Sparsity | No | Yes | Yes |\n",
        "| Feature selection | No | Yes | Yes |\n",
        "| Correlated features | Keeps all | Picks one | Compromise |\n",
        "| Solution | Closed-form | Iterative | Iterative |\n",
        "| Interpretability | Moderate | High | High |\n",
        "\n",
        "### Hyperparameter Tuning\n",
        "\n",
        "**Alpha (\u03b1)**: Controls regularization strength\n",
        "- \u03b1 = 0: No regularization (OLS)\n",
        "- Small \u03b1: Weak regularization\n",
        "- Large \u03b1: Strong regularization\n",
        "- Use CV methods (RidgeCV, LassoCV) for automatic selection\n",
        "\n",
        "**L1 Ratio** (ElasticNet only): Balance between L1 and L2\n",
        "- l1_ratio = 0: Pure Ridge\n",
        "- l1_ratio = 1: Pure Lasso  \n",
        "- 0 < l1_ratio < 1: Combination\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. **Always standardize features** before regularization\n",
        "2. **Use CV methods** (RidgeCV, LassoCV) for automatic alpha selection\n",
        "3. **Plot coefficient paths** to understand feature importance\n",
        "4. **Start with ElasticNet** if unsure (combines both advantages)\n",
        "5. **Check feature selection** - Lasso/ElasticNet show which features matter\n",
        "6. **Compare with OLS** to see if regularization helps\n",
        "7. **Use appropriate metrics** - R\u00b2, MSE, MAE for regression\n",
        "\n",
        "### Common Pitfalls\n",
        "\n",
        "- \u274c Not standardizing features before regularization\n",
        "- \u274c Using too large alpha (underfitting)\n",
        "- \u274c Using too small alpha (overfitting)\n",
        "- \u274c Not using CV for alpha selection\n",
        "- \u274c Expecting Lasso to work well with highly correlated features\n",
        "- \u274c Interpreting coefficients without standardization\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Logistic Regression (classification)\n",
        "- Polynomial regression with regularization\n",
        "- Feature engineering\n",
        "- Advanced regularization techniques"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}