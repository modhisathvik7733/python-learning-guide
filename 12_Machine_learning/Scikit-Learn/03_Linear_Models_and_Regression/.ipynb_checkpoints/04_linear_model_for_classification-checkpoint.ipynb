{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Linear Models for Classification: Logistic Regression\n",
        "\n",
        "## Overview\n",
        "\n",
        "**Logistic Regression** is a fundamental classification algorithm that models the probability of an instance belonging to a particular class. Despite its name, it's a **classification** algorithm, not regression.\n",
        "\n",
        "## Mathematical Foundation\n",
        "\n",
        "### Binary Classification\n",
        "\n",
        "**Linear Combination**:\n",
        "\\[\n",
        "z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p = \\beta^T x\n",
        "\\]\n",
        "\n",
        "**Sigmoid Function** (maps to probability [0, 1]):\n",
        "\\[\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "\\]\n",
        "\n",
        "**Predicted Probability**:\n",
        "\\[\n",
        "P(y=1 | x) = \\sigma(\\beta^T x) = \\frac{1}{1 + e^{-\\beta^T x}}\n",
        "\\]\n",
        "\n",
        "**Decision Rule**:\n",
        "\\[\n",
        "\\hat{y} = \\begin{cases} \n",
        "1 & \\text{if } P(y=1|x) \\geq 0.5 \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "\\]\n",
        "\n",
        "### Loss Function (Log Loss / Cross-Entropy)\n",
        "\n",
        "\\[\n",
        "L(\\beta) = -\\frac{1}{n}\\sum_{i=1}^{n} [y_i \\log(p_i) + (1-y_i)\\log(1-p_i)]\n",
        "\\]\n",
        "\n",
        "where \\(p_i = P(y_i=1|x_i)\\).\n",
        "\n",
        "### Regularization\n",
        "\n",
        "**L2 (Ridge)**:\n",
        "\\[\n",
        "L(\\beta) = -\\frac{1}{n}\\sum_{i=1}^{n} [y_i \\log(p_i) + (1-y_i)\\log(1-p_i)] + \\frac{1}{2C}||\\beta||^2\n",
        "\\]\n",
        "\n",
        "**L1 (Lasso)**:\n",
        "\\[\n",
        "L(\\beta) = -\\frac{1}{n}\\sum_{i=1}^{n} [y_i \\log(p_i) + (1-y_i)\\log(1-p_i)] + \\frac{1}{C}||\\beta||_1\n",
        "\\]\n",
        "\n",
        "Note: In sklearn, `C` is the **inverse** of regularization strength (larger C = less regularization).\n",
        "\n",
        "## Topics Covered\n",
        "\n",
        "1. Binary classification basics\n",
        "2. Sigmoid function and probability interpretation\n",
        "3. Decision boundaries\n",
        "4. Multiclass classification (One-vs-Rest, Multinomial)\n",
        "5. Regularization (L1, L2)\n",
        "6. Model evaluation metrics\n",
        "7. Real-world datasets\n",
        "8. Probability calibration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_curve, auc,\n",
        "    roc_auc_score, log_loss\n",
        ")\n",
        "from sklearn.datasets import make_classification, load_breast_cancer, load_iris, load_wine\n",
        "\n",
        "np.random.seed(42)\n",
        "sns.set_style('whitegrid')\n",
        "print(\"\u2713 Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sigmoid",
      "metadata": {},
      "source": [
        "## 1. Understanding the Sigmoid Function\n",
        "\n",
        "### 1.1 Sigmoid Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sigmoid-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Plot sigmoid\n",
        "z = np.linspace(-10, 10, 200)\n",
        "probs = sigmoid(z)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(z, probs, linewidth=3, label='\u03c3(z) = 1/(1 + e\u207b\u1dbb)')\n",
        "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Decision threshold (0.5)')\n",
        "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.3)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.xlabel('z (linear combination of features)', fontsize=12)\n",
        "plt.ylabel('P(y=1|x) - Probability', fontsize=12)\n",
        "plt.title('Sigmoid (Logistic) Function', fontsize=14)\n",
        "plt.legend(fontsize=11)\n",
        "plt.ylim(-0.1, 1.1)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Sigmoid Function Properties\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\u03c3(-\u221e) = {sigmoid(-100):.10f} \u2248 0\")\n",
        "print(f\"\u03c3(0)   = {sigmoid(0):.10f}\")\n",
        "print(f\"\u03c3(+\u221e) = {sigmoid(100):.10f} \u2248 1\")\n",
        "print(f\"\\n\u03c3(-5) = {sigmoid(-5):.4f}  (very unlikely)\")\n",
        "print(f\"\u03c3(5)  = {sigmoid(5):.4f}   (very likely)\")\n",
        "print(f\"\\n\ud83d\udca1 Sigmoid squashes any real number to [0, 1] range\")\n",
        "print(\"   This makes it perfect for probability estimation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "binary-classification",
      "metadata": {},
      "source": [
        "## 2. Binary Classification\n",
        "\n",
        "### 2.1 Simple 2D Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "binary-synthetic",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate binary classification data\n",
        "X, y = make_classification(\n",
        "    n_samples=200,\n",
        "    n_features=2,\n",
        "    n_redundant=0,\n",
        "    n_informative=2,\n",
        "    n_clusters_per_class=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Binary Classification Dataset\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X.shape[0]}\")\n",
        "print(f\"Features: {X.shape[1]}\")\n",
        "print(f\"Classes: {np.unique(y)}\")\n",
        "print(f\"Class distribution: {np.bincount(y)}\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "binary-train",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train logistic regression\n",
        "log_reg = LogisticRegression(random_state=42)\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Logistic Regression Model\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nCoefficients (\u03b2): {log_reg.coef_[0]}\")\n",
        "print(f\"Intercept (\u03b2\u2080): {log_reg.intercept_[0]:.4f}\")\n",
        "\n",
        "# Predictions\n",
        "y_pred = log_reg.predict(X_test_scaled)\n",
        "y_proba = log_reg.predict_proba(X_test_scaled)\n",
        "\n",
        "print(f\"\\nExample predictions (first 5 test samples):\")\n",
        "for i in range(5):\n",
        "    print(f\"  Sample {i}: True={y_test[i]}, Predicted={y_pred[i]}, \"\n",
        "          f\"P(class 0)={y_proba[i][0]:.3f}, P(class 1)={y_proba[i][1]:.3f}\")\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nModel Performance:\")\n",
        "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"  Precision: {precision:.4f}\")\n",
        "print(f\"  Recall:    {recall:.4f}\")\n",
        "print(f\"  F1-Score:  {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "decision-boundary",
      "metadata": {},
      "source": [
        "### 2.2 Decision Boundary Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decision-boundary-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot decision boundary\n",
        "def plot_decision_boundary(X, y, model, title=\"Decision Boundary\"):\n",
        "    h = 0.02  # step size in the mesh\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='k', s=50)\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title(title)\n",
        "    plt.colorbar(label='Predicted Class')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot\n",
        "plot_decision_boundary(X_train_scaled, y_train, log_reg, \n",
        "                      \"Logistic Regression Decision Boundary\")\n",
        "\n",
        "print(\"Decision Boundary:\")\n",
        "print(\"  The line separating the two colored regions is where P(y=1) = 0.5\")\n",
        "print(\"  This corresponds to \u03b2\u2080 + \u03b2\u2081x\u2081 + \u03b2\u2082x\u2082 = 0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "confusion-matrix",
      "metadata": {},
      "source": [
        "### 2.3 Confusion Matrix and Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "confusion-matrix-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
        "            xticklabels=['Class 0', 'Class 1'],\n",
        "            yticklabels=['Class 0', 'Class 1'])\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nConfusion Matrix Breakdown:\")\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(f\"  True Negatives (TN):  {tn}\")\n",
        "print(f\"  False Positives (FP): {fp}\")\n",
        "print(f\"  False Negatives (FN): {fn}\")\n",
        "print(f\"  True Positives (TP):  {tp}\")\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "roc-curve",
      "metadata": {},
      "source": [
        "### 2.4 ROC Curve and AUC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "roc-curve-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get probability predictions\n",
        "y_proba_pos = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_proba_pos)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ROC-AUC Analysis:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"\\nInterpretation:\")\n",
        "print(f\"  AUC = 0.5: Random classifier (no discrimination)\")\n",
        "print(f\"  AUC = 1.0: Perfect classifier\")\n",
        "print(f\"  Our model: {roc_auc:.3f} - {('Excellent' if roc_auc > 0.9 else 'Good' if roc_auc > 0.8 else 'Fair')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "regularization-logistic",
      "metadata": {},
      "source": [
        "## 3. Regularization in Logistic Regression\n",
        "\n",
        "### 3.1 Effect of Regularization (C parameter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "regularization-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different C values (note: larger C = less regularization)\n",
        "C_values = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "penalties = ['l2', 'l1']  # Ridge and Lasso\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"Regularization Analysis\")\n",
        "print(\"=\"*70)\n",
        "print(\"Note: Larger C = Less regularization\\n\")\n",
        "\n",
        "for penalty in penalties:\n",
        "    print(f\"\\n{penalty.upper()} Penalty:\")\n",
        "    for C in C_values:\n",
        "        # Train model\n",
        "        if penalty == 'l1':\n",
        "            model = LogisticRegression(C=C, penalty=penalty, solver='liblinear', random_state=42)\n",
        "        else:\n",
        "            model = LogisticRegression(C=C, penalty=penalty, random_state=42)\n",
        "        \n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        \n",
        "        # Evaluate\n",
        "        train_acc = model.score(X_train_scaled, y_train)\n",
        "        test_acc = model.score(X_test_scaled, y_test)\n",
        "        \n",
        "        # Coefficient magnitudes\n",
        "        coef_magnitude = np.linalg.norm(model.coef_)\n",
        "        n_nonzero = np.sum(model.coef_[0] != 0) if penalty == 'l1' else X.shape[1]\n",
        "        \n",
        "        results.append({\n",
        "            'Penalty': penalty,\n",
        "            'C': C,\n",
        "            'Train_Acc': train_acc,\n",
        "            'Test_Acc': test_acc,\n",
        "            'Coef_Norm': coef_magnitude,\n",
        "            'Nonzero_Coefs': n_nonzero\n",
        "        })\n",
        "        \n",
        "        print(f\"  C={C:6.2f}: Train={train_acc:.3f}, Test={test_acc:.3f}, \"\n",
        "              f\"||\u03b2||={coef_magnitude:.3f}, Features={n_nonzero}\")\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Summary:\")\n",
        "print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "regularization-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize regularization effect\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# L2 regularization\n",
        "l2_results = results_df[results_df['Penalty'] == 'l2']\n",
        "axes[0].semilogx(l2_results['C'], l2_results['Train_Acc'], 'o-', label='Train Accuracy', linewidth=2)\n",
        "axes[0].semilogx(l2_results['C'], l2_results['Test_Acc'], 's-', label='Test Accuracy', linewidth=2)\n",
        "axes[0].set_xlabel('C (inverse regularization strength)')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('L2 Regularization (Ridge)')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# L1 regularization\n",
        "l1_results = results_df[results_df['Penalty'] == 'l1']\n",
        "axes[1].semilogx(l1_results['C'], l1_results['Train_Acc'], 'o-', label='Train Accuracy', linewidth=2)\n",
        "axes[1].semilogx(l1_results['C'], l1_results['Test_Acc'], 's-', label='Test Accuracy', linewidth=2)\n",
        "axes[1].set_xlabel('C (inverse regularization strength)')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_title('L1 Regularization (Lasso)')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Observations:\")\n",
        "print(\"  - Low C (strong regularization): May underfit\")\n",
        "print(\"  - High C (weak regularization): May overfit\")\n",
        "print(\"  - L1 can set coefficients to zero (feature selection)\")\n",
        "print(\"  - L2 shrinks coefficients but keeps all features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "multiclass",
      "metadata": {},
      "source": [
        "## 4. Multiclass Classification\n",
        "\n",
        "### 4.1 Iris Dataset (3 classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "multiclass-iris",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target\n",
        "\n",
        "print(\"Iris Dataset - Multiclass Classification\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_iris.shape[0]}\")\n",
        "print(f\"Features: {X_iris.shape[1]}\")\n",
        "print(f\"Feature names: {iris.feature_names}\")\n",
        "print(f\"Classes: {iris.target_names}\")\n",
        "print(f\"Class distribution: {np.bincount(y_iris)}\")\n",
        "\n",
        "# Split and scale\n",
        "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
        "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
        ")\n",
        "\n",
        "scaler_iris = StandardScaler()\n",
        "X_train_iris_scaled = scaler_iris.fit_transform(X_train_iris)\n",
        "X_test_iris_scaled = scaler_iris.transform(X_test_iris)\n",
        "\n",
        "print(f\"\\nTrain set: {X_train_iris.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test_iris.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "multiclass-strategies",
      "metadata": {},
      "source": [
        "### 4.2 Multiclass Strategies: OvR vs Multinomial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "multiclass-comparison",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare multiclass strategies\n",
        "strategies = {\n",
        "    'One-vs-Rest (OvR)': LogisticRegression(multi_class='ovr', random_state=42),\n",
        "    'Multinomial': LogisticRegression(multi_class='multinomial', random_state=42)\n",
        "}\n",
        "\n",
        "print(\"Multiclass Strategy Comparison\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for name, model in strategies.items():\n",
        "    # Train\n",
        "    model.fit(X_train_iris_scaled, y_train_iris)\n",
        "    \n",
        "    # Evaluate\n",
        "    train_acc = model.score(X_train_iris_scaled, y_train_iris)\n",
        "    test_acc = model.score(X_test_iris_scaled, y_test_iris)\n",
        "    \n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test_iris_scaled)\n",
        "    y_proba = model.predict_proba(X_test_iris_scaled)\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Train Accuracy: {train_acc:.4f}\")\n",
        "    print(f\"  Test Accuracy:  {test_acc:.4f}\")\n",
        "    print(f\"  Coefficients shape: {model.coef_.shape}\")\n",
        "    print(f\"  (rows = classes, columns = features)\")\n",
        "    \n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test_iris, y_pred)\n",
        "    print(f\"\\n  Confusion Matrix:\")\n",
        "    print(f\"  {cm}\")\n",
        "    \n",
        "    # Classification report\n",
        "    print(f\"\\n  Classification Report:\")\n",
        "    print(classification_report(y_test_iris, y_pred, target_names=iris.target_names))\n",
        "\n",
        "print(\"\\n\ud83d\udca1 OvR vs Multinomial:\")\n",
        "print(\"  - OvR: Trains 3 binary classifiers (one per class)\")\n",
        "print(\"  - Multinomial: Trains a single multiclass classifier\")\n",
        "print(\"  - Multinomial often performs better for multiclass problems\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "multiclass-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize confusion matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "for idx, (name, model) in enumerate(strategies.items()):\n",
        "    y_pred = model.predict(X_test_iris_scaled)\n",
        "    cm = confusion_matrix(y_test_iris, y_pred)\n",
        "    \n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=iris.target_names,\n",
        "                yticklabels=iris.target_names,\n",
        "                ax=axes[idx])\n",
        "    axes[idx].set_ylabel('True Label')\n",
        "    axes[idx].set_xlabel('Predicted Label')\n",
        "    axes[idx].set_title(f'{name}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "breast-cancer",
      "metadata": {},
      "source": [
        "## 5. Real-World Example: Breast Cancer Detection\n",
        "\n",
        "### 5.1 Dataset Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "breast-cancer-load",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load breast cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X_cancer = cancer.data\n",
        "y_cancer = cancer.target\n",
        "\n",
        "print(\"Breast Cancer Wisconsin Dataset\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_cancer.shape[0]}\")\n",
        "print(f\"Features: {X_cancer.shape[1]}\")\n",
        "print(f\"\\nTarget classes:\")\n",
        "print(f\"  0: Malignant (cancerous)\")\n",
        "print(f\"  1: Benign (non-cancerous)\")\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(f\"  Malignant: {np.sum(y_cancer == 0)}\")\n",
        "print(f\"  Benign: {np.sum(y_cancer == 1)}\")\n",
        "print(f\"\\nFeatures (first 10): {cancer.feature_names[:10].tolist()}...\")\n",
        "\n",
        "# Split and scale\n",
        "X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = train_test_split(\n",
        "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
        ")\n",
        "\n",
        "scaler_cancer = StandardScaler()\n",
        "X_train_cancer_scaled = scaler_cancer.fit_transform(X_train_cancer)\n",
        "X_test_cancer_scaled = scaler_cancer.transform(X_test_cancer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "breast-cancer-model",
      "metadata": {},
      "source": [
        "### 5.2 Model Training and Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "breast-cancer-grid",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grid search for best hyperparameters\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    LogisticRegression(random_state=42, max_iter=10000),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"Hyperparameter Tuning with GridSearchCV\")\n",
        "print(\"=\"*70)\n",
        "print(\"Searching for best parameters...\\n\")\n",
        "\n",
        "grid_search.fit(X_train_cancer_scaled, y_train_cancer)\n",
        "\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best CV AUC score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Get best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred_cancer = best_model.predict(X_test_cancer_scaled)\n",
        "y_proba_cancer = best_model.predict_proba(X_test_cancer_scaled)[:, 1]\n",
        "\n",
        "test_acc = accuracy_score(y_test_cancer, y_pred_cancer)\n",
        "test_auc = roc_auc_score(y_test_cancer, y_proba_cancer)\n",
        "test_f1 = f1_score(y_test_cancer, y_pred_cancer)\n",
        "\n",
        "print(f\"\\nTest Set Performance:\")\n",
        "print(f\"  Accuracy:  {test_acc:.4f}\")\n",
        "print(f\"  AUC:       {test_auc:.4f}\")\n",
        "print(f\"  F1-Score:  {test_f1:.4f}\")\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_cancer, y_pred_cancer, \n",
        "                          target_names=['Malignant', 'Benign']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "breast-cancer-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test_cancer, y_pred_cancer)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Malignant', 'Benign'],\n",
        "            yticklabels=['Malignant', 'Benign'],\n",
        "            ax=axes[0])\n",
        "axes[0].set_ylabel('True Label')\n",
        "axes[0].set_xlabel('Predicted Label')\n",
        "axes[0].set_title('Confusion Matrix')\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test_cancer, y_proba_cancer)\n",
        "axes[1].plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {test_auc:.3f}')\n",
        "axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "axes[1].set_xlabel('False Positive Rate')\n",
        "axes[1].set_ylabel('True Positive Rate')\n",
        "axes[1].set_title('ROC Curve')\n",
        "axes[1].legend(loc='lower right')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Medical Application Note:\")\n",
        "print(\"  - High recall (sensitivity) is crucial to avoid missing cancer cases\")\n",
        "print(\"  - False negatives (cancer classified as benign) are more serious\")\n",
        "print(\"  - Threshold tuning may be needed based on clinical requirements\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feature-importance",
      "metadata": {},
      "source": [
        "### 5.3 Feature Importance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feature-importance-analysis",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze feature importance from coefficients\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': cancer.feature_names,\n",
        "    'Coefficient': best_model.coef_[0],\n",
        "    'Abs_Coefficient': np.abs(best_model.coef_[0])\n",
        "}).sort_values('Abs_Coefficient', ascending=False)\n",
        "\n",
        "print(\"Top 10 Most Important Features\")\n",
        "print(\"=\"*70)\n",
        "print(feature_importance.head(10).to_string(index=False))\n",
        "\n",
        "# Plot top 15 features\n",
        "top_features = feature_importance.head(15)\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = ['green' if c > 0 else 'red' for c in top_features['Coefficient']]\n",
        "plt.barh(top_features['Feature'], top_features['Coefficient'], color=colors, alpha=0.7)\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.title('Top 15 Feature Coefficients (Benign Prediction)')\n",
        "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
        "plt.grid(alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"  - Positive coefficients \u2192 Higher values increase probability of Benign\")\n",
        "print(\"  - Negative coefficients \u2192 Higher values increase probability of Malignant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "probability-calibration",
      "metadata": {},
      "source": [
        "## 6. Probability Calibration\n",
        "\n",
        "### 6.1 Checking Calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "calibration-check",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calibration curve\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "prob_true, prob_pred = calibration_curve(y_test_cancer, y_proba_cancer, n_bins=10)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(prob_pred, prob_true, 's-', label='Logistic Regression', linewidth=2)\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
        "plt.xlabel('Mean Predicted Probability')\n",
        "plt.ylabel('Fraction of Positives')\n",
        "plt.title('Calibration Curve')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Probability Calibration:\")\n",
        "print(\"=\"*70)\n",
        "print(\"A well-calibrated model's predicted probabilities match actual frequencies.\")\n",
        "print(\"\\nExample: If model predicts 70% probability for 100 samples,\")\n",
        "print(\"         ~70 of them should actually be positive.\")\n",
        "print(\"\\nLogistic Regression is generally well-calibrated by default!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Summary and Best Practices\n",
        "\n",
        "### Quick Reference\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Binary classification\n",
        "model = LogisticRegression(\n",
        "    C=1.0,              # Inverse of regularization strength\n",
        "    penalty='l2',       # 'l1', 'l2', 'elasticnet', 'none'\n",
        "    solver='lbfgs',     # Optimization algorithm\n",
        "    max_iter=1000,      # Maximum iterations\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Multiclass\n",
        "model = LogisticRegression(\n",
        "    multi_class='multinomial',  # or 'ovr'\n",
        "    C=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Always standardize features!\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_proba = model.predict_proba(X_test_scaled)\n",
        "```\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "| Parameter | Description | Typical Values |\n",
        "|-----------|-------------|----------------|\n",
        "| `C` | Inverse regularization strength | 0.01 to 100 |\n",
        "| `penalty` | Regularization type | 'l1', 'l2', 'elasticnet' |\n",
        "| `solver` | Optimization algorithm | 'lbfgs', 'liblinear', 'saga' |\n",
        "| `multi_class` | Multiclass strategy | 'ovr', 'multinomial' |\n",
        "| `class_weight` | Handle imbalanced data | 'balanced', None |\n",
        "\n",
        "### Solver Selection\n",
        "\n",
        "| Solver | Penalty | Use Case |\n",
        "|--------|---------|----------|\n",
        "| `lbfgs` | L2 | Default, good for most cases |\n",
        "| `liblinear` | L1, L2 | Small datasets, binary classification |\n",
        "| `saga` | L1, L2, ElasticNet | Large datasets, all penalties |\n",
        "| `newton-cg` | L2 | Large datasets, L2 only |\n",
        "\n",
        "### Evaluation Metrics\n",
        "\n",
        "**Binary Classification:**\n",
        "- **Accuracy**: Overall correctness\n",
        "- **Precision**: Of predicted positives, how many are correct\n",
        "- **Recall (Sensitivity)**: Of actual positives, how many are found\n",
        "- **F1-Score**: Harmonic mean of precision and recall\n",
        "- **ROC-AUC**: Classifier's discrimination ability\n",
        "\n",
        "**Multiclass:**\n",
        "- Weighted/Macro/Micro averaged metrics\n",
        "- Confusion matrix\n",
        "- Per-class precision/recall\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. \u2713 **Always standardize features** - Essential for regularization\n",
        "2. \u2713 **Use cross-validation** - For hyperparameter tuning\n",
        "3. \u2713 **Check class imbalance** - Use `class_weight='balanced'` if needed\n",
        "4. \u2713 **Start with default parameters** - Then tune C\n",
        "5. \u2713 **Monitor multiple metrics** - Don't rely on accuracy alone\n",
        "6. \u2713 **Use stratified splits** - Preserve class ratios\n",
        "7. \u2713 **Check probability calibration** - Important for probability estimates\n",
        "8. \u2713 **Visualize decision boundaries** - Understand model behavior\n",
        "\n",
        "### Common Pitfalls\n",
        "\n",
        "1. \u274c Not standardizing features\n",
        "2. \u274c Using wrong solver for penalty\n",
        "3. \u274c Ignoring class imbalance\n",
        "4. \u274c Not checking convergence (increase `max_iter` if needed)\n",
        "5. \u274c Using accuracy for imbalanced data\n",
        "6. \u274c Not using stratified splits\n",
        "7. \u274c Forgetting to scale test data with training scaler\n",
        "8. \u274c Misinterpreting coefficients without standardization\n",
        "\n",
        "### When to Use Logistic Regression\n",
        "\n",
        "\u2713 **Good for:**\n",
        "- Binary and multiclass classification\n",
        "- Interpretable models (coefficients show feature impact)\n",
        "- Probability estimates needed\n",
        "- Baseline models\n",
        "- Linear decision boundaries\n",
        "- Fast training and prediction\n",
        "\n",
        "\u2717 **Not ideal for:**\n",
        "- Highly non-linear problems (try polynomial features or kernel methods)\n",
        "- Complex decision boundaries\n",
        "- Image/text with raw pixels/words (use CNNs/RNNs)\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Support Vector Machines (SVM)\n",
        "- Decision Trees and Random Forests\n",
        "- Gradient Boosting (XGBoost, LightGBM)\n",
        "- Neural Networks for classification\n",
        "- Advanced feature engineering\n",
        "- Handling imbalanced datasets"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}