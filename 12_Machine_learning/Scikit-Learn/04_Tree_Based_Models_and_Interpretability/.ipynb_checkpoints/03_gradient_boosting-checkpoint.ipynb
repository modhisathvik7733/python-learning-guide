{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Gradient Boosting: Advanced Ensemble Methods\n",
        "\n",
        "## Overview\n",
        "\n",
        "**Gradient Boosting** is a powerful ensemble technique that builds trees **sequentially**, where each tree corrects errors of previous trees. Unlike Random Forest (parallel bagging), Gradient Boosting uses **boosting** - adding weak learners iteratively to minimize a loss function.\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "### Boosting Algorithm\n",
        "\n",
        "**Sequential Learning**:\n",
        "\\[\n",
        "F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)\n",
        "\\]\n",
        "\n",
        "where:\n",
        "- \\(F_m(x)\\): Ensemble after \\(m\\) trees\n",
        "- \\(\\eta\\): Learning rate (shrinkage)\n",
        "- \\(h_m(x)\\): New tree fitted to residuals\n",
        "\n",
        "### Gradient Descent in Function Space\n",
        "\n",
        "**Loss Function**:\n",
        "\\[\n",
        "L(y, F(x)) = \\sum_{i=1}^{n} l(y_i, F(x_i))\n",
        "\\]\n",
        "\n",
        "**Negative Gradient** (pseudo-residuals):\n",
        "\\[\n",
        "r_{im} = -\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F=F_{m-1}}\n",
        "\\]\n",
        "\n",
        "**Tree Update**:\n",
        "\\[\n",
        "F_m(x) = F_{m-1}(x) + \\eta \\sum_{j=1}^{J_m} \\gamma_{jm} I(x \\in R_{jm})\n",
        "\\]\n",
        "\n",
        "### Key Hyperparameters\n",
        "\n",
        "- **n_estimators**: Number of boosting stages\n",
        "- **learning_rate** (\\(\\eta\\)): Shrinkage parameter\n",
        "- **max_depth**: Depth of individual trees (usually 3-8)\n",
        "- **subsample**: Fraction of samples for each tree (stochastic GB)\n",
        "- **min_samples_split/leaf**: Regularization\n",
        "\n",
        "## Topics Covered\n",
        "\n",
        "1. GradientBoostingClassifier / Regressor\n",
        "2. HistGradientBoostingClassifier / Regressor (faster)\n",
        "3. Learning rate and n_estimators trade-off\n",
        "4. Early stopping\n",
        "5. Feature importance\n",
        "6. Comparison with Random Forest\n",
        "7. Real-world applications\n",
        "8. Introduction to XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import (\n",
        "    GradientBoostingClassifier, GradientBoostingRegressor,\n",
        "    HistGradientBoostingClassifier, HistGradientBoostingRegressor,\n",
        "    RandomForestClassifier, RandomForestRegressor\n",
        ")\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, cross_val_score, GridSearchCV, learning_curve\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, confusion_matrix,\n",
        "    mean_squared_error, r2_score, roc_auc_score, roc_curve, log_loss\n",
        ")\n",
        "from sklearn.datasets import (\n",
        "    make_classification, make_regression,\n",
        "    load_breast_cancer, load_diabetes\n",
        ")\n",
        "import time\n",
        "\n",
        "np.random.seed(42)\n",
        "sns.set_style('whitegrid')\n",
        "print(\"\u2713 Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gradient-boosting-basics",
      "metadata": {},
      "source": [
        "## 1. Gradient Boosting Basics\n",
        "\n",
        "### 1.1 Classification Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "basic-classification",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=15,\n",
        "    n_redundant=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Classification Dataset\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X.shape[0]}\")\n",
        "print(f\"Features: {X.shape[1]}\")\n",
        "print(f\"Classes: {np.unique(y)}\")\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain: {X_train.shape[0]}, Test: {X_test.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "train-gb-classifier",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Gradient Boosting Classifier\n",
        "gb_clf = GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training Gradient Boosting Classifier...\")\n",
        "start_time = time.time()\n",
        "gb_clf.fit(X_train, y_train)\n",
        "train_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nTraining completed in {train_time:.2f} seconds\")\n",
        "\n",
        "# Evaluate\n",
        "train_acc = gb_clf.score(X_train, y_train)\n",
        "test_acc = gb_clf.score(X_test, y_test)\n",
        "\n",
        "print(f\"\\nPerformance:\")\n",
        "print(f\"  Train Accuracy: {train_acc:.4f}\")\n",
        "print(f\"  Test Accuracy:  {test_acc:.4f}\")\n",
        "print(f\"  Overfitting Gap: {train_acc - test_acc:.4f}\")\n",
        "\n",
        "# Predictions\n",
        "y_pred = gb_clf.predict(X_test)\n",
        "y_proba = gb_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comparison",
      "metadata": {},
      "source": [
        "### 1.2 Comparison: Random Forest vs Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rf-vs-gb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Random Forest for comparison\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Compare\n",
        "models = {\n",
        "    'Random Forest': rf_clf,\n",
        "    'Gradient Boosting': gb_clf\n",
        "}\n",
        "\n",
        "print(\"Random Forest vs Gradient Boosting\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    train_acc = model.score(X_train, y_train)\n",
        "    test_acc = model.score(X_test, y_test)\n",
        "    \n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "    auc = roc_auc_score(y_test, y_proba)\n",
        "    \n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Train_Acc': train_acc,\n",
        "        'Test_Acc': test_acc,\n",
        "        'AUC': auc\n",
        "    })\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Train: {train_acc:.4f}, Test: {test_acc:.4f}\")\n",
        "    print(f\"  AUC: {auc:.4f}\")\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Key Differences:\")\n",
        "print(\"  RF: Parallel training, less prone to overfitting\")\n",
        "print(\"  GB: Sequential training, often higher accuracy with tuning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "learning-rate",
      "metadata": {},
      "source": [
        "## 2. Learning Rate and N_Estimators Trade-off\n",
        "\n",
        "### 2.1 Effect of Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "learning-rate-analysis",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different learning rates\n",
        "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.5, 1.0]\n",
        "n_estimators = 100\n",
        "\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    gb = GradientBoostingClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        learning_rate=lr,\n",
        "        max_depth=3,\n",
        "        random_state=42\n",
        "    )\n",
        "    gb.fit(X_train, y_train)\n",
        "    \n",
        "    train_scores.append(gb.score(X_train, y_train))\n",
        "    test_scores.append(gb.score(X_test, y_test))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(learning_rates, train_scores, 'o-', label='Train', linewidth=2)\n",
        "plt.plot(learning_rates, test_scores, 's-', label='Test', linewidth=2)\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title(f'Effect of Learning Rate (n_estimators={n_estimators})')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.xscale('log')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Learning Rate Analysis\")\n",
        "print(\"=\"*70)\n",
        "for lr, train_acc, test_acc in zip(learning_rates, train_scores, test_scores):\n",
        "    print(f\"LR={lr:4.2f}: Train={train_acc:.4f}, Test={test_acc:.4f}, Gap={train_acc-test_acc:.4f}\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Observations:\")\n",
        "print(\"  - Low LR: Underfitting (needs more trees)\")\n",
        "print(\"  - High LR: Overfitting risk\")\n",
        "print(\"  - Sweet spot: 0.05-0.1 typically\")\n",
        "print(\"  - Lower LR needs more estimators (better generalization)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "staged-prediction",
      "metadata": {},
      "source": [
        "### 2.2 Staged Predictions (Learning Curve)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "staged-predictions",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model and track performance at each stage\n",
        "gb_staged = GradientBoostingClassifier(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "gb_staged.fit(X_train, y_train)\n",
        "\n",
        "# Get staged predictions\n",
        "train_scores_staged = []\n",
        "test_scores_staged = []\n",
        "\n",
        "for y_pred_train in gb_staged.staged_predict(X_train):\n",
        "    train_scores_staged.append(accuracy_score(y_train, y_pred_train))\n",
        "\n",
        "for y_pred_test in gb_staged.staged_predict(X_test):\n",
        "    test_scores_staged.append(accuracy_score(y_test, y_pred_test))\n",
        "\n",
        "# Plot learning curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_scores_staged, label='Train', linewidth=2)\n",
        "plt.plot(test_scores_staged, label='Test', linewidth=2)\n",
        "plt.xlabel('Number of Trees')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Gradient Boosting: Learning Curve (LR=0.1)')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find optimal number of trees\n",
        "optimal_n = np.argmax(test_scores_staged) + 1\n",
        "print(f\"\\nOptimal number of trees: {optimal_n}\")\n",
        "print(f\"Best test accuracy: {max(test_scores_staged):.4f}\")\n",
        "print(f\"\\n\ud83d\udca1 After {optimal_n} trees, model starts overfitting\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hist-gradient-boosting",
      "metadata": {},
      "source": [
        "## 3. HistGradientBoosting (Faster Implementation)\n",
        "\n",
        "### 3.1 Speed and Performance Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hist-gb-comparison",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create larger dataset\n",
        "X_large, y_large = make_classification(\n",
        "    n_samples=10000,\n",
        "    n_features=50,\n",
        "    n_informative=40,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train_large, X_test_large, y_train_large, y_test_large = train_test_split(\n",
        "    X_large, y_large, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Large Dataset Comparison\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Train samples: {X_train_large.shape[0]}\")\n",
        "print(f\"Features: {X_train_large.shape[1]}\")\n",
        "\n",
        "# Traditional GradientBoosting\n",
        "print(\"\\nTraining GradientBoostingClassifier...\")\n",
        "start = time.time()\n",
        "gb_traditional = GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "gb_traditional.fit(X_train_large, y_train_large)\n",
        "time_traditional = time.time() - start\n",
        "acc_traditional = gb_traditional.score(X_test_large, y_test_large)\n",
        "\n",
        "# HistGradientBoosting\n",
        "print(\"Training HistGradientBoostingClassifier...\")\n",
        "start = time.time()\n",
        "hist_gb = HistGradientBoostingClassifier(\n",
        "    max_iter=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "hist_gb.fit(X_train_large, y_train_large)\n",
        "time_hist = time.time() - start\n",
        "acc_hist = hist_gb.score(X_test_large, y_test_large)\n",
        "\n",
        "# Results\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Results:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nGradientBoostingClassifier:\")\n",
        "print(f\"  Time: {time_traditional:.2f}s\")\n",
        "print(f\"  Accuracy: {acc_traditional:.4f}\")\n",
        "\n",
        "print(f\"\\nHistGradientBoostingClassifier:\")\n",
        "print(f\"  Time: {time_hist:.2f}s\")\n",
        "print(f\"  Accuracy: {acc_hist:.4f}\")\n",
        "\n",
        "speedup = time_traditional / time_hist\n",
        "print(f\"\\n\ud83d\udca1 Speedup: {speedup:.1f}x faster!\")\n",
        "print(f\"   Accuracy difference: {abs(acc_traditional - acc_hist):.4f}\")\n",
        "\n",
        "print(\"\\nKey Differences:\")\n",
        "print(\"  - HistGB uses histogram-based splitting (bins features)\")\n",
        "print(\"  - Much faster on large datasets (>10K samples)\")\n",
        "print(\"  - Similar or better accuracy\")\n",
        "print(\"  - Native missing value support\")\n",
        "print(\"  - Parameter: max_iter instead of n_estimators\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "early-stopping",
      "metadata": {},
      "source": [
        "### 3.2 Early Stopping with HistGradientBoosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "early-stopping",
      "metadata": {},
      "outputs": [],
      "source": [
        "# HistGB with early stopping\n",
        "hist_gb_es = HistGradientBoostingClassifier(\n",
        "    max_iter=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    early_stopping=True,\n",
        "    n_iter_no_change=10,\n",
        "    validation_fraction=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"HistGradientBoosting with Early Stopping\")\n",
        "print(\"=\"*70)\n",
        "hist_gb_es.fit(X_train_large, y_train_large)\n",
        "\n",
        "print(f\"\\nStopped at iteration: {hist_gb_es.n_iter_}\")\n",
        "print(f\"Test accuracy: {hist_gb_es.score(X_test_large, y_test_large):.4f}\")\n",
        "\n",
        "# Plot training history\n",
        "train_scores_hist = hist_gb_es.train_score_\n",
        "val_scores_hist = hist_gb_es.validation_score_\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_scores_hist, label='Train Loss', linewidth=2)\n",
        "plt.plot(val_scores_hist, label='Validation Loss', linewidth=2)\n",
        "plt.axvline(x=hist_gb_es.n_iter_, color='r', linestyle='--', \n",
        "           alpha=0.5, label=f'Early Stop (iter={hist_gb_es.n_iter_})')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Early Stopping: Training History')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Early Stopping Benefits:\")\n",
        "print(\"  - Prevents overfitting automatically\")\n",
        "print(\"  - Saves computation time\")\n",
        "print(\"  - No need to tune n_estimators precisely\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "regression",
      "metadata": {},
      "source": [
        "## 4. Gradient Boosting Regression\n",
        "\n",
        "### 4.1 Diabetes Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gb-regression",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load diabetes\n",
        "diabetes = load_diabetes()\n",
        "X_diabetes = diabetes.data\n",
        "y_diabetes = diabetes.target\n",
        "\n",
        "print(\"Gradient Boosting Regression: Diabetes Dataset\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_diabetes.shape[0]}\")\n",
        "print(f\"Features: {X_diabetes.shape[1]}\")\n",
        "\n",
        "# Split\n",
        "X_train_db, X_test_db, y_train_db, y_test_db = train_test_split(\n",
        "    X_diabetes, y_diabetes, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train models\n",
        "models_reg = {\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    'GradientBoosting': GradientBoostingRegressor(\n",
        "        n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42\n",
        "    ),\n",
        "    'HistGradientBoosting': HistGradientBoostingRegressor(\n",
        "        max_iter=100, learning_rate=0.1, max_depth=3, random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "results_reg = []\n",
        "for name, model in models_reg.items():\n",
        "    model.fit(X_train_db, y_train_db)\n",
        "    \n",
        "    y_train_pred = model.predict(X_train_db)\n",
        "    y_test_pred = model.predict(X_test_db)\n",
        "    \n",
        "    train_r2 = r2_score(y_train_db, y_train_pred)\n",
        "    test_r2 = r2_score(y_test_db, y_test_pred)\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test_db, y_test_pred))\n",
        "    \n",
        "    results_reg.append({\n",
        "        'Model': name,\n",
        "        'Train_R2': train_r2,\n",
        "        'Test_R2': test_r2,\n",
        "        'Test_RMSE': test_rmse\n",
        "    })\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Train R\u00b2: {train_r2:.4f}, Test R\u00b2: {test_r2:.4f}\")\n",
        "    print(f\"  Test RMSE: {test_rmse:.2f}\")\n",
        "\n",
        "results_reg_df = pd.DataFrame(results_reg)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(results_reg_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "regression-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize best model (GradientBoosting)\n",
        "gb_reg = models_reg['GradientBoosting']\n",
        "y_pred_db = gb_reg.predict(X_test_db)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test_db, y_pred_db, alpha=0.6)\n",
        "plt.plot([y_test_db.min(), y_test_db.max()], \n",
        "         [y_test_db.min(), y_test_db.max()], 'r--', lw=2)\n",
        "plt.xlabel('Actual Disease Progression')\n",
        "plt.ylabel('Predicted Disease Progression')\n",
        "plt.title(f'Gradient Boosting Regression (R\u00b2 = {test_r2:.3f})')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Feature importance\n",
        "importances = gb_reg.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': diabetes.feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importance: Diabetes Progression\")\n",
        "print(\"=\"*70)\n",
        "print(feature_importance_df.to_string(index=False))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'], \n",
        "        feature_importance_df['Importance'], alpha=0.8)\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importance: Gradient Boosting Regression')\n",
        "plt.grid(alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "breast-cancer",
      "metadata": {},
      "source": [
        "## 5. Real-World Application: Breast Cancer Detection\n",
        "\n",
        "### 5.1 Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "breast-cancer-gb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load breast cancer\n",
        "cancer = load_breast_cancer()\n",
        "X_cancer = cancer.data\n",
        "y_cancer = cancer.target\n",
        "\n",
        "print(\"Breast Cancer Detection with Gradient Boosting\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_cancer.shape[0]}\")\n",
        "print(f\"Features: {X_cancer.shape[1]}\")\n",
        "\n",
        "# Split\n",
        "X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = train_test_split(\n",
        "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
        ")\n",
        "\n",
        "# Grid search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    GradientBoostingClassifier(random_state=42),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nPerforming Grid Search...\")\n",
        "grid_search.fit(X_train_cancer, y_train_cancer)\n",
        "\n",
        "print(\"\\nBest Parameters:\")\n",
        "for param, value in grid_search.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "print(f\"\\nBest CV AUC: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate\n",
        "best_gb = grid_search.best_estimator_\n",
        "y_pred_cancer = best_gb.predict(X_test_cancer)\n",
        "y_proba_cancer = best_gb.predict_proba(X_test_cancer)[:, 1]\n",
        "\n",
        "test_acc = accuracy_score(y_test_cancer, y_pred_cancer)\n",
        "test_auc = roc_auc_score(y_test_cancer, y_proba_cancer)\n",
        "\n",
        "print(f\"\\nTest Performance:\")\n",
        "print(f\"  Accuracy: {test_acc:.4f}\")\n",
        "print(f\"  AUC: {test_auc:.4f}\")\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_cancer, y_pred_cancer,\n",
        "                          target_names=cancer.target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cancer-visualizations",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizations\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test_cancer, y_pred_cancer)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=cancer.target_names,\n",
        "            yticklabels=cancer.target_names,\n",
        "            ax=axes[0])\n",
        "axes[0].set_ylabel('True Label')\n",
        "axes[0].set_xlabel('Predicted Label')\n",
        "axes[0].set_title('Gradient Boosting: Confusion Matrix')\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test_cancer, y_proba_cancer)\n",
        "axes[1].plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {test_auc:.3f}')\n",
        "axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "axes[1].set_xlabel('False Positive Rate')\n",
        "axes[1].set_ylabel('True Positive Rate')\n",
        "axes[1].set_title('ROC Curve')\n",
        "axes[1].legend(loc='lower right')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Feature importance\n",
        "importances_cancer = best_gb.feature_importances_\n",
        "feature_importance_cancer = pd.DataFrame({\n",
        "    'Feature': cancer.feature_names,\n",
        "    'Importance': importances_cancer\n",
        "}).sort_values('Importance', ascending=False).head(15)\n",
        "\n",
        "print(\"\\nTop 15 Features:\")\n",
        "print(feature_importance_cancer.to_string(index=False))\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.barh(feature_importance_cancer['Feature'], \n",
        "        feature_importance_cancer['Importance'], alpha=0.8)\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Top 15 Features: Breast Cancer Detection (Gradient Boosting)')\n",
        "plt.grid(alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xgboost",
      "metadata": {},
      "source": [
        "## 6. Introduction to XGBoost (Optional)\n",
        "\n",
        "### 6.1 XGBoost vs Sklearn GradientBoosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xgboost-intro",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if XGBoost is available\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    xgboost_available = True\n",
        "    print(\"XGBoost is installed!\")\n",
        "except ImportError:\n",
        "    xgboost_available = False\n",
        "    print(\"XGBoost not installed. Install with: pip install xgboost\")\n",
        "\n",
        "if xgboost_available:\n",
        "    print(\"\\nXGBoost vs Sklearn GradientBoosting\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Train XGBoost\n",
        "    xgb_clf = xgb.XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=3,\n",
        "        random_state=42,\n",
        "        eval_metric='logloss'\n",
        "    )\n",
        "    \n",
        "    start = time.time()\n",
        "    xgb_clf.fit(X_train_cancer, y_train_cancer)\n",
        "    time_xgb = time.time() - start\n",
        "    \n",
        "    # Train Sklearn GB\n",
        "    gb_sklearn = GradientBoostingClassifier(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=3,\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    start = time.time()\n",
        "    gb_sklearn.fit(X_train_cancer, y_train_cancer)\n",
        "    time_sklearn = time.time() - start\n",
        "    \n",
        "    # Compare\n",
        "    acc_xgb = xgb_clf.score(X_test_cancer, y_test_cancer)\n",
        "    acc_sklearn = gb_sklearn.score(X_test_cancer, y_test_cancer)\n",
        "    \n",
        "    print(f\"\\nXGBoost:\")\n",
        "    print(f\"  Time: {time_xgb:.2f}s\")\n",
        "    print(f\"  Accuracy: {acc_xgb:.4f}\")\n",
        "    \n",
        "    print(f\"\\nSklearn GradientBoosting:\")\n",
        "    print(f\"  Time: {time_sklearn:.2f}s\")\n",
        "    print(f\"  Accuracy: {acc_sklearn:.4f}\")\n",
        "    \n",
        "    print(\"\\n\ud83d\udca1 XGBoost Advantages:\")\n",
        "    print(\"  - Often faster (especially on large datasets)\")\n",
        "    print(\"  - Regularization (L1, L2)\")\n",
        "    print(\"  - Handling missing values\")\n",
        "    print(\"  - Built-in cross-validation\")\n",
        "    print(\"  - Feature importance types\")\n",
        "    print(\"  - GPU support\")\n",
        "else:\n",
        "    print(\"\\nXGBoost Key Features:\")\n",
        "    print(\"  - Extreme Gradient Boosting\")\n",
        "    print(\"  - Regularization (L1, L2) to prevent overfitting\")\n",
        "    print(\"  - Parallel processing\")\n",
        "    print(\"  - Tree pruning using max_depth\")\n",
        "    print(\"  - Built-in cross-validation\")\n",
        "    print(\"  - Native handling of missing values\")\n",
        "    print(\"  - Multiple objective functions\")\n",
        "    print(\"\\n  Install: pip install xgboost\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Summary and Best Practices\n",
        "\n",
        "### Quick Reference\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import (\n",
        "    GradientBoostingClassifier, GradientBoostingRegressor,\n",
        "    HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
        ")\n",
        "\n",
        "# Standard Gradient Boosting\n",
        "gb_clf = GradientBoostingClassifier(\n",
        "    n_estimators=100,       # Number of boosting stages\n",
        "    learning_rate=0.1,      # Shrinkage (0.01-0.3)\n",
        "    max_depth=3,            # Tree depth (3-8 typical)\n",
        "    subsample=1.0,          # Fraction of samples (0.5-1.0)\n",
        "    min_samples_split=2,    # Min samples to split\n",
        "    min_samples_leaf=1,     # Min samples per leaf\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Histogram-based (faster for large datasets)\n",
        "hist_gb = HistGradientBoostingClassifier(\n",
        "    max_iter=100,           # Number of iterations\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    early_stopping=True,    # Enable early stopping\n",
        "    n_iter_no_change=10,    # Patience for early stopping\n",
        "    validation_fraction=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "```\n",
        "\n",
        "### Advantages\n",
        "\n",
        "\u2713 **High accuracy**: Often best performance on tabular data\n",
        "\u2713 **Handles non-linearity**: Captures complex patterns\n",
        "\u2713 **Feature interactions**: Automatically captures interactions\n",
        "\u2713 **Robust to outliers**: (with appropriate loss functions)\n",
        "\u2713 **No scaling needed**: Tree-based\n",
        "\u2713 **Feature importance**: Built-in\n",
        "\n",
        "### Disadvantages\n",
        "\n",
        "\u274c **Sequential training**: Slower than Random Forest\n",
        "\u274c **Overfitting risk**: Needs careful tuning\n",
        "\u274c **Many hyperparameters**: Requires tuning\n",
        "\u274c **Less interpretable**: Ensemble of trees\n",
        "\u274c **Sensitive to noisy data**: Can overfit noise\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. \u2713 **Start with low learning rate** (0.01-0.1) + many trees (100-1000)\n",
        "2. \u2713 **Shallow trees** (max_depth=3-8) work best\n",
        "3. \u2713 **Use early stopping** to find optimal n_estimators\n",
        "4. \u2713 **Monitor train vs validation** to prevent overfitting\n",
        "5. \u2713 **Use HistGB for large datasets** (>10K samples)\n",
        "6. \u2713 **Tune learning_rate and n_estimators together**\n",
        "7. \u2713 **Try subsample < 1.0** (stochastic GB) for regularization\n",
        "8. \u2713 **Cross-validate** for reliable performance estimates\n",
        "\n",
        "### Key Hyperparameters\n",
        "\n",
        "| Parameter | Typical Range | Effect |\n",
        "|-----------|---------------|--------|\n",
        "| `learning_rate` | 0.01-0.3 | Lower = better generalization (need more trees) |\n",
        "| `n_estimators` | 100-1000 | More trees (with low LR) = better |\n",
        "| `max_depth` | 3-8 | Deeper = more complex (risk overfitting) |\n",
        "| `subsample` | 0.5-1.0 | < 1.0 adds randomness (regularization) |\n",
        "| `min_samples_split` | 2-20 | Higher = simpler trees |\n",
        "| `min_samples_leaf` | 1-10 | Higher = smoother predictions |\n",
        "\n",
        "### Hyperparameter Tuning Strategy\n",
        "\n",
        "**Step 1**: Fix learning_rate=0.1, tune tree structure\n",
        "- max_depth, min_samples_split, min_samples_leaf\n",
        "\n",
        "**Step 2**: Tune n_estimators with early stopping\n",
        "- Find optimal number of trees\n",
        "\n",
        "**Step 3**: Lower learning_rate, increase n_estimators\n",
        "- E.g., LR=0.05 with 2x trees, or LR=0.01 with 10x trees\n",
        "\n",
        "**Step 4**: Tune subsample for stochastic GB\n",
        "- Try 0.8 or 0.5 for additional regularization\n",
        "\n",
        "### When to Use Gradient Boosting\n",
        "\n",
        "\u2713 **Good for:**\n",
        "- Structured/tabular data\n",
        "- Kaggle competitions (often wins)\n",
        "- Need highest accuracy\n",
        "- Medium-sized datasets\n",
        "- Can invest time in tuning\n",
        "\n",
        "\u2717 **Consider alternatives:**\n",
        "- Need fast prototyping \u2192 Random Forest\n",
        "- Very large datasets \u2192 LightGBM, CatBoost\n",
        "- Need interpretability \u2192 Linear models, single tree\n",
        "- Real-time predictions \u2192 Simpler models\n",
        "- High-dimensional sparse data \u2192 Linear models\n",
        "\n",
        "### GradientBoosting vs HistGradientBoosting\n",
        "\n",
        "| Aspect | GradientBoosting | HistGradientBoosting |\n",
        "|--------|------------------|----------------------|\n",
        "| Speed | Slower | 5-10x faster |\n",
        "| Dataset size | < 10K samples | > 10K samples |\n",
        "| Missing values | Manual handling | Native support |\n",
        "| Memory | Higher | Lower |\n",
        "| Accuracy | Similar | Similar |\n",
        "| Scikit-learn | Traditional | Modern (v0.21+) |\n",
        "\n",
        "### Popular Gradient Boosting Libraries\n",
        "\n",
        "1. **Scikit-learn**: GradientBoosting, HistGradientBoosting\n",
        "2. **XGBoost**: Extreme Gradient Boosting (most popular)\n",
        "3. **LightGBM**: Microsoft's fast implementation\n",
        "4. **CatBoost**: Yandex's categorical-friendly version\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- SHAP values for model interpretation\n",
        "- XGBoost, LightGBM, CatBoost\n",
        "- Handling categorical features\n",
        "- Stacking and blending\n",
        "- Hyperparameter optimization (Optuna, Hyperopt)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}