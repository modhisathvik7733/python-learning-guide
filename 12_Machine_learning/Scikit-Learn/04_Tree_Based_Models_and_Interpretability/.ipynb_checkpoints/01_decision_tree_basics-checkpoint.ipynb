{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Decision Trees: Fundamentals\n",
        "\n",
        "## Overview\n",
        "\n",
        "**Decision Trees** are non-parametric supervised learning algorithms used for both classification and regression. They learn simple decision rules inferred from data features, creating a tree-like model of decisions.\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "### Tree Structure\n",
        "\n",
        "- **Root Node**: Top of the tree, represents entire dataset\n",
        "- **Internal Nodes**: Decision points (splits)\n",
        "- **Branches**: Outcomes of decisions\n",
        "- **Leaf Nodes**: Final predictions (terminal nodes)\n",
        "\n",
        "### Splitting Criteria\n",
        "\n",
        "**Classification (Impurity Measures)**:\n",
        "\n",
        "**Gini Impurity**:\n",
        "\\[\n",
        "Gini = 1 - \\sum_{i=1}^{C} p_i^2\n",
        "\\]\n",
        "where \\(p_i\\) is probability of class \\(i\\), \\(C\\) is number of classes.\n",
        "\n",
        "**Entropy (Information Gain)**:\n",
        "\\[\n",
        "Entropy = -\\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
        "\\]\n",
        "\n",
        "**Regression (Variance Reduction)**:\n",
        "\n",
        "**Mean Squared Error**:\n",
        "\\[\n",
        "MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\bar{y})^2\n",
        "\\]\n",
        "\n",
        "### Decision Rule\n",
        "\n",
        "At each node, find the best split:\n",
        "\\[\n",
        "\\text{Best split} = \\arg\\min_{feature, threshold} [Impurity_{left} + Impurity_{right}]\n",
        "\\]\n",
        "\n",
        "## Topics Covered\n",
        "\n",
        "1. Decision Tree Classification\n",
        "2. Decision Tree Regression\n",
        "3. Tree visualization and interpretation\n",
        "4. Hyperparameter tuning (depth, samples, impurity)\n",
        "5. Overfitting and pruning\n",
        "6. Feature importance\n",
        "7. Real-world applications"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree, export_text\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, cross_val_score, GridSearchCV, validation_curve\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, confusion_matrix,\n",
        "    mean_squared_error, r2_score, mean_absolute_error\n",
        ")\n",
        "from sklearn.datasets import (\n",
        "    make_classification, make_regression, load_iris, load_wine,\n",
        "    load_breast_cancer, load_diabetes\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "np.random.seed(42)\n",
        "sns.set_style('whitegrid')\n",
        "print(\"\u2713 Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "classification",
      "metadata": {},
      "source": [
        "## 1. Decision Tree Classification\n",
        "\n",
        "### 1.1 Simple Binary Classification Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "binary-classification",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic data\n",
        "X, y = make_classification(\n",
        "    n_samples=200,\n",
        "    n_features=2,\n",
        "    n_redundant=0,\n",
        "    n_informative=2,\n",
        "    n_clusters_per_class=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Binary Classification Dataset\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X.shape[0]}\")\n",
        "print(f\"Features: {X.shape[1]}\")\n",
        "print(f\"Classes: {np.unique(y)}\")\n",
        "print(f\"Class distribution: {np.bincount(y)}\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain: {X_train.shape[0]}, Test: {X_test.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "train-tree-classifier",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train decision tree\n",
        "tree_clf = DecisionTreeClassifier(\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "tree_clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Decision Tree Classifier\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Tree depth: {tree_clf.get_depth()}\")\n",
        "print(f\"Number of leaves: {tree_clf.get_n_leaves()}\")\n",
        "print(f\"Number of features: {tree_clf.n_features_in_}\")\n",
        "\n",
        "# Predictions\n",
        "y_pred = tree_clf.predict(X_test)\n",
        "y_proba = tree_clf.predict_proba(X_test)\n",
        "\n",
        "# Evaluate\n",
        "train_acc = tree_clf.score(X_train, y_train)\n",
        "test_acc = tree_clf.score(X_test, y_test)\n",
        "\n",
        "print(f\"\\nPerformance:\")\n",
        "print(f\"  Train Accuracy: {train_acc:.4f}\")\n",
        "print(f\"  Test Accuracy:  {test_acc:.4f}\")\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tree-visualization",
      "metadata": {},
      "source": [
        "### 1.2 Tree Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "visualize-tree",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the tree\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(\n",
        "    tree_clf,\n",
        "    feature_names=['Feature 0', 'Feature 1'],\n",
        "    class_names=['Class 0', 'Class 1'],\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    fontsize=10\n",
        ")\n",
        "plt.title('Decision Tree Visualization', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nHow to Read the Tree:\")\n",
        "print(\"=\"*70)\n",
        "print(\"Each node shows:\")\n",
        "print(\"  - Splitting condition (e.g., 'Feature 0 <= 0.5')\")\n",
        "print(\"  - Gini impurity (lower = purer node)\")\n",
        "print(\"  - Samples: number of samples in node\")\n",
        "print(\"  - Value: [class 0 count, class 1 count]\")\n",
        "print(\"  - Class: predicted class (majority vote)\")\n",
        "print(\"\\nColor intensity indicates class confidence (purity)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "text-representation",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text representation\n",
        "text_rules = export_text(\n",
        "    tree_clf,\n",
        "    feature_names=['Feature 0', 'Feature 1']\n",
        ")\n",
        "\n",
        "print(\"Decision Tree Rules (Text Format):\")\n",
        "print(\"=\"*70)\n",
        "print(text_rules)\n",
        "\n",
        "print(\"\\n\ud83d\udca1 This shows the exact decision path the tree follows!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "decision-boundary",
      "metadata": {},
      "source": [
        "### 1.3 Decision Boundary Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decision-boundary-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot decision boundary\n",
        "def plot_decision_boundary(X, y, model, title=\"Decision Boundary\"):\n",
        "    h = 0.02\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='k', s=50)\n",
        "    plt.xlabel('Feature 0')\n",
        "    plt.ylabel('Feature 1')\n",
        "    plt.title(title)\n",
        "    plt.colorbar(label='Predicted Class')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(X_train, y_train, tree_clf, \n",
        "                      \"Decision Tree: Axis-Parallel Splits\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Key Observation:\")\n",
        "print(\"  Decision trees create RECTANGULAR decision boundaries\")\n",
        "print(\"  (parallel to feature axes) due to axis-aligned splits.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iris-dataset",
      "metadata": {},
      "source": [
        "## 2. Multiclass Classification: Iris Dataset\n",
        "\n",
        "### 2.1 Train and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iris-tree",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target\n",
        "\n",
        "print(\"Iris Dataset - Multiclass Classification\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_iris.shape[0]}\")\n",
        "print(f\"Features: {X_iris.shape[1]}\")\n",
        "print(f\"Feature names: {iris.feature_names}\")\n",
        "print(f\"Classes: {iris.target_names}\")\n",
        "print(f\"Class distribution: {np.bincount(y_iris)}\")\n",
        "\n",
        "# Split\n",
        "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
        "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
        ")\n",
        "\n",
        "# Train tree\n",
        "iris_tree = DecisionTreeClassifier(\n",
        "    max_depth=3,\n",
        "    min_samples_split=5,\n",
        "    random_state=42\n",
        ")\n",
        "iris_tree.fit(X_train_iris, y_train_iris)\n",
        "\n",
        "# Evaluate\n",
        "train_acc = iris_tree.score(X_train_iris, y_train_iris)\n",
        "test_acc = iris_tree.score(X_test_iris, y_test_iris)\n",
        "\n",
        "print(f\"\\nModel Performance:\")\n",
        "print(f\"  Train Accuracy: {train_acc:.4f}\")\n",
        "print(f\"  Test Accuracy:  {test_acc:.4f}\")\n",
        "print(f\"  Tree depth: {iris_tree.get_depth()}\")\n",
        "print(f\"  Number of leaves: {iris_tree.get_n_leaves()}\")\n",
        "\n",
        "# Predictions\n",
        "y_pred_iris = iris_tree.predict(X_test_iris)\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_iris, y_pred_iris, target_names=iris.target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iris-tree-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize iris tree\n",
        "plt.figure(figsize=(25, 12))\n",
        "plot_tree(\n",
        "    iris_tree,\n",
        "    feature_names=iris.feature_names,\n",
        "    class_names=iris.target_names,\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    fontsize=11\n",
        ")\n",
        "plt.title('Iris Decision Tree', fontsize=18)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test_iris, y_pred_iris)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=iris.target_names,\n",
        "            yticklabels=iris.target_names)\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feature-importance",
      "metadata": {},
      "source": [
        "### 2.2 Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feature-importance-analysis",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance\n",
        "importances = iris_tree.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': iris.feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"Feature Importance\")\n",
        "print(\"=\"*70)\n",
        "print(feature_importance_df.to_string(index=False))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], alpha=0.8)\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importance in Decision Tree')\n",
        "plt.grid(alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Feature Importance measures:\")\n",
        "print(\"  How much each feature contributes to reducing impurity\")\n",
        "print(\"  Sum of all importances = 1.0\")\n",
        "print(\"  Higher value = more important for classification\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "regression",
      "metadata": {},
      "source": [
        "## 3. Decision Tree Regression\n",
        "\n",
        "### 3.1 Simple 1D Regression Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "regression-1d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate non-linear data\n",
        "np.random.seed(42)\n",
        "X_reg = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
        "y_reg = np.sin(X_reg).ravel() + np.random.randn(80) * 0.1\n",
        "\n",
        "print(\"1D Regression Data\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_reg.shape[0]}\")\n",
        "print(f\"Features: {X_reg.shape[1]}\")\n",
        "print(f\"Target range: [{y_reg.min():.2f}, {y_reg.max():.2f}]\")\n",
        "\n",
        "# Train trees with different depths\n",
        "depths = [2, 5, 10, None]\n",
        "X_test_reg = np.linspace(0, 5, 200).reshape(-1, 1)\n",
        "\n",
        "plt.figure(figsize=(16, 10))\n",
        "\n",
        "for i, depth in enumerate(depths, 1):\n",
        "    plt.subplot(2, 2, i)\n",
        "    \n",
        "    # Train tree\n",
        "    tree_reg = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
        "    tree_reg.fit(X_reg, y_reg)\n",
        "    \n",
        "    # Predict\n",
        "    y_pred_reg = tree_reg.predict(X_test_reg)\n",
        "    \n",
        "    # Calculate R\u00b2\n",
        "    y_train_pred = tree_reg.predict(X_reg)\n",
        "    r2 = r2_score(y_reg, y_train_pred)\n",
        "    \n",
        "    # Plot\n",
        "    plt.scatter(X_reg, y_reg, s=20, alpha=0.6, label='Training data')\n",
        "    plt.plot(X_test_reg, y_pred_reg, 'r-', linewidth=2, \n",
        "            label=f'Tree (depth={depth})')\n",
        "    plt.plot(X_test_reg, np.sin(X_test_reg), 'g--', alpha=0.5, \n",
        "            linewidth=1.5, label='True function')\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('y')\n",
        "    plt.title(f'Max Depth = {depth} (R\u00b2 = {r2:.3f})')\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Observations:\")\n",
        "print(\"  - Depth 2: Underfitting (too simple)\")\n",
        "print(\"  - Depth 5: Good fit\")\n",
        "print(\"  - Depth 10: Starting to overfit\")\n",
        "print(\"  - Depth None: Severe overfitting (memorizing data)\")\n",
        "print(\"\\n  Note: Predictions are PIECEWISE CONSTANT (step functions)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "diabetes-regression",
      "metadata": {},
      "source": [
        "### 3.2 Real Dataset: Diabetes Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "diabetes-tree",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X_diabetes = diabetes.data\n",
        "y_diabetes = diabetes.target\n",
        "\n",
        "print(\"Diabetes Dataset\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_diabetes.shape[0]}\")\n",
        "print(f\"Features: {X_diabetes.shape[1]}\")\n",
        "print(f\"Feature names: {diabetes.feature_names}\")\n",
        "print(f\"Target: Disease progression (one year after baseline)\")\n",
        "\n",
        "# Split\n",
        "X_train_db, X_test_db, y_train_db, y_test_db = train_test_split(\n",
        "    X_diabetes, y_diabetes, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train tree\n",
        "tree_reg_db = DecisionTreeRegressor(\n",
        "    max_depth=5,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=5,\n",
        "    random_state=42\n",
        ")\n",
        "tree_reg_db.fit(X_train_db, y_train_db)\n",
        "\n",
        "# Evaluate\n",
        "y_train_pred_db = tree_reg_db.predict(X_train_db)\n",
        "y_test_pred_db = tree_reg_db.predict(X_test_db)\n",
        "\n",
        "train_r2 = r2_score(y_train_db, y_train_pred_db)\n",
        "test_r2 = r2_score(y_test_db, y_test_pred_db)\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train_db, y_train_pred_db))\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test_db, y_test_pred_db))\n",
        "\n",
        "print(f\"\\nModel Performance:\")\n",
        "print(f\"  Train R\u00b2: {train_r2:.4f}\")\n",
        "print(f\"  Test R\u00b2:  {test_r2:.4f}\")\n",
        "print(f\"  Train RMSE: {train_rmse:.2f}\")\n",
        "print(f\"  Test RMSE:  {test_rmse:.2f}\")\n",
        "print(f\"\\n  Tree depth: {tree_reg_db.get_depth()}\")\n",
        "print(f\"  Number of leaves: {tree_reg_db.get_n_leaves()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "diabetes-feature-importance",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance\n",
        "importances_db = tree_reg_db.feature_importances_\n",
        "feature_importance_db = pd.DataFrame({\n",
        "    'Feature': diabetes.feature_names,\n",
        "    'Importance': importances_db\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"Feature Importance (Diabetes Regression)\")\n",
        "print(\"=\"*70)\n",
        "print(feature_importance_db.to_string(index=False))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_db['Feature'], feature_importance_db['Importance'], alpha=0.8)\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importance: Diabetes Progression Prediction')\n",
        "plt.grid(alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Predictions vs Actual\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test_db, y_test_pred_db, alpha=0.6)\n",
        "plt.plot([y_test_db.min(), y_test_db.max()], \n",
        "         [y_test_db.min(), y_test_db.max()], 'r--', lw=2)\n",
        "plt.xlabel('Actual Disease Progression')\n",
        "plt.ylabel('Predicted Disease Progression')\n",
        "plt.title(f'Decision Tree Regression (Test R\u00b2 = {test_r2:.3f})')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "overfitting",
      "metadata": {},
      "source": [
        "## 4. Overfitting and Pruning\n",
        "\n",
        "### 4.1 Demonstrating Overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "overfitting-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different tree depths\n",
        "depths_to_test = range(1, 16)\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "n_leaves = []\n",
        "\n",
        "for depth in depths_to_test:\n",
        "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "    tree.fit(X_train, y_train)\n",
        "    \n",
        "    train_scores.append(tree.score(X_train, y_train))\n",
        "    test_scores.append(tree.score(X_test, y_test))\n",
        "    n_leaves.append(tree.get_n_leaves())\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy vs depth\n",
        "axes[0].plot(depths_to_test, train_scores, 'o-', label='Train Accuracy', linewidth=2)\n",
        "axes[0].plot(depths_to_test, test_scores, 's-', label='Test Accuracy', linewidth=2)\n",
        "optimal_depth = depths_to_test[np.argmax(test_scores)]\n",
        "axes[0].axvline(x=optimal_depth, color='r', linestyle='--', alpha=0.5,\n",
        "               label=f'Optimal depth: {optimal_depth}')\n",
        "axes[0].set_xlabel('Max Depth')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('Overfitting Analysis')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Number of leaves\n",
        "axes[1].plot(depths_to_test, n_leaves, 'o-', color='green', linewidth=2)\n",
        "axes[1].set_xlabel('Max Depth')\n",
        "axes[1].set_ylabel('Number of Leaves')\n",
        "axes[1].set_title('Tree Complexity')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Overfitting Analysis\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Optimal depth: {optimal_depth}\")\n",
        "print(f\"Test accuracy at optimal: {max(test_scores):.4f}\")\n",
        "print(f\"\\nAt max depth (15):\")\n",
        "print(f\"  Train accuracy: {train_scores[-1]:.4f}\")\n",
        "print(f\"  Test accuracy:  {test_scores[-1]:.4f}\")\n",
        "print(f\"  Gap: {train_scores[-1] - test_scores[-1]:.4f} (overfitting!)\")\n",
        "print(f\"  Number of leaves: {n_leaves[-1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hyperparameters",
      "metadata": {},
      "source": [
        "### 4.2 Pruning via Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pruning-params",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Key Hyperparameters for Pruning\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "hyperparams = [\n",
        "    (\"max_depth\", \"Maximum depth of tree\", \"Lower = simpler tree\"),\n",
        "    (\"min_samples_split\", \"Min samples to split a node\", \"Higher = simpler tree\"),\n",
        "    (\"min_samples_leaf\", \"Min samples in leaf node\", \"Higher = simpler tree\"),\n",
        "    (\"max_leaf_nodes\", \"Max number of leaf nodes\", \"Lower = simpler tree\"),\n",
        "    (\"min_impurity_decrease\", \"Min impurity decrease for split\", \"Higher = simpler tree\"),\n",
        "]\n",
        "\n",
        "for param, description, effect in hyperparams:\n",
        "    print(f\"\\n{param}:\")\n",
        "    print(f\"  Description: {description}\")\n",
        "    print(f\"  Effect: {effect}\")\n",
        "\n",
        "# Compare different pruning strategies\n",
        "pruning_strategies = {\n",
        "    'No Pruning': DecisionTreeClassifier(random_state=42),\n",
        "    'max_depth=5': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
        "    'min_samples_split=20': DecisionTreeClassifier(min_samples_split=20, random_state=42),\n",
        "    'min_samples_leaf=10': DecisionTreeClassifier(min_samples_leaf=10, random_state=42),\n",
        "    'Combined': DecisionTreeClassifier(\n",
        "        max_depth=5, min_samples_split=20, min_samples_leaf=5, random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Pruning Strategy Comparison\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "results = []\n",
        "for name, tree in pruning_strategies.items():\n",
        "    tree.fit(X_train, y_train)\n",
        "    train_acc = tree.score(X_train, y_train)\n",
        "    test_acc = tree.score(X_test, y_test)\n",
        "    depth = tree.get_depth()\n",
        "    leaves = tree.get_n_leaves()\n",
        "    \n",
        "    results.append({\n",
        "        'Strategy': name,\n",
        "        'Train_Acc': train_acc,\n",
        "        'Test_Acc': test_acc,\n",
        "        'Depth': depth,\n",
        "        'Leaves': leaves\n",
        "    })\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Train: {train_acc:.4f}, Test: {test_acc:.4f}\")\n",
        "    print(f\"  Depth: {depth}, Leaves: {leaves}\")\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Summary:\")\n",
        "print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "grid-search",
      "metadata": {},
      "source": [
        "## 5. Hyperparameter Tuning with GridSearchCV\n",
        "\n",
        "### 5.1 Comprehensive Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "grid-search-tree",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grid search for best hyperparameters\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7, 10, None],\n",
        "    'min_samples_split': [2, 5, 10, 20],\n",
        "    'min_samples_leaf': [1, 2, 5, 10],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"GridSearchCV: Finding Best Hyperparameters\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Parameter grid: {len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf']) * len(param_grid['criterion'])} combinations\")\n",
        "print(f\"Cross-validation folds: 5\")\n",
        "print(\"\\nSearching...\\n\")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nBest Parameters:\")\n",
        "for param, value in grid_search.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "print(f\"\\nBest CV Score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "best_tree = grid_search.best_estimator_\n",
        "test_acc = best_tree.score(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "print(f\"\\nBest Tree Properties:\")\n",
        "print(f\"  Depth: {best_tree.get_depth()}\")\n",
        "print(f\"  Leaves: {best_tree.get_n_leaves()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "breast-cancer",
      "metadata": {},
      "source": [
        "## 6. Real-World Application: Breast Cancer Detection\n",
        "\n",
        "### 6.1 Complete Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "breast-cancer-tree",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load breast cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X_cancer = cancer.data\n",
        "y_cancer = cancer.target\n",
        "\n",
        "print(\"Breast Cancer Detection with Decision Trees\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_cancer.shape[0]}\")\n",
        "print(f\"Features: {X_cancer.shape[1]}\")\n",
        "print(f\"Classes: {cancer.target_names}\")\n",
        "print(f\"Class distribution: Malignant={np.sum(y_cancer==0)}, Benign={np.sum(y_cancer==1)}\")\n",
        "\n",
        "# Split\n",
        "X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = train_test_split(\n",
        "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
        ")\n",
        "\n",
        "# Simplified grid search\n",
        "param_grid_cancer = {\n",
        "    'max_depth': [3, 5, 7, 10],\n",
        "    'min_samples_split': [5, 10, 20],\n",
        "    'min_samples_leaf': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid_cancer = GridSearchCV(\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    param_grid_cancer,\n",
        "    cv=5,\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_cancer.fit(X_train_cancer, y_train_cancer)\n",
        "\n",
        "print(f\"\\nBest Parameters: {grid_cancer.best_params_}\")\n",
        "print(f\"Best CV AUC: {grid_cancer.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate\n",
        "best_cancer_tree = grid_cancer.best_estimator_\n",
        "y_pred_cancer = best_cancer_tree.predict(X_test_cancer)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "y_proba_cancer = best_cancer_tree.predict_proba(X_test_cancer)[:, 1]\n",
        "test_auc = roc_auc_score(y_test_cancer, y_proba_cancer)\n",
        "\n",
        "print(f\"\\nTest Performance:\")\n",
        "print(f\"  Accuracy: {accuracy_score(y_test_cancer, y_pred_cancer):.4f}\")\n",
        "print(f\"  AUC: {test_auc:.4f}\")\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_cancer, y_pred_cancer, \n",
        "                          target_names=cancer.target_names))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test_cancer, y_pred_cancer)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=cancer.target_names,\n",
        "            yticklabels=cancer.target_names)\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.title('Breast Cancer Detection: Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cancer-feature-importance",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Top features for cancer detection\n",
        "importances_cancer = best_cancer_tree.feature_importances_\n",
        "feature_importance_cancer = pd.DataFrame({\n",
        "    'Feature': cancer.feature_names,\n",
        "    'Importance': importances_cancer\n",
        "}).sort_values('Importance', ascending=False).head(15)\n",
        "\n",
        "print(\"Top 15 Features for Cancer Detection\")\n",
        "print(\"=\"*70)\n",
        "print(feature_importance_cancer.to_string(index=False))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.barh(feature_importance_cancer['Feature'], \n",
        "        feature_importance_cancer['Importance'], alpha=0.8)\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Top 15 Features: Breast Cancer Detection')\n",
        "plt.grid(alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Summary and Best Practices\n",
        "\n",
        "### Quick Reference\n",
        "\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "\n",
        "# Classification\n",
        "clf = DecisionTreeClassifier(\n",
        "    criterion='gini',       # or 'entropy'\n",
        "    max_depth=5,           # Limit tree depth\n",
        "    min_samples_split=10,  # Min samples to split\n",
        "    min_samples_leaf=5,    # Min samples in leaf\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Regression\n",
        "reg = DecisionTreeRegressor(\n",
        "    criterion='squared_error',  # or 'absolute_error'\n",
        "    max_depth=5,\n",
        "    min_samples_split=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "predictions = clf.predict(X_test)\n",
        "```\n",
        "\n",
        "### Advantages\n",
        "\n",
        "\u2713 **Interpretable**: Easy to visualize and understand\n",
        "\u2713 **No scaling needed**: Handles features at different scales\n",
        "\u2713 **Handles non-linearity**: Captures complex patterns\n",
        "\u2713 **Mixed data types**: Works with numerical and categorical\n",
        "\u2713 **Feature importance**: Built-in feature selection\n",
        "\u2713 **Fast prediction**: O(log n) complexity\n",
        "\n",
        "### Disadvantages\n",
        "\n",
        "\u274c **Overfitting**: Tends to overfit without pruning\n",
        "\u274c **Instability**: Small data changes = different tree\n",
        "\u274c **Bias**: Biased toward features with more levels\n",
        "\u274c **Poor extrapolation**: Cannot predict beyond training range\n",
        "\u274c **Axis-parallel splits**: Cannot learn diagonal boundaries well\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. \u2713 **Always limit depth** (`max_depth=5-10` is often good)\n",
        "2. \u2713 **Use pruning parameters** to prevent overfitting\n",
        "3. \u2713 **Cross-validate** to find optimal hyperparameters\n",
        "4. \u2713 **Visualize trees** for interpretability\n",
        "5. \u2713 **Check feature importance** to understand model\n",
        "6. \u2713 **Use ensembles** (Random Forest, Gradient Boosting) for better performance\n",
        "7. \u2713 **Monitor train vs test** to detect overfitting\n",
        "\n",
        "### Key Hyperparameters\n",
        "\n",
        "| Parameter | Description | Typical Values |\n",
        "|-----------|-------------|----------------|\n",
        "| `max_depth` | Maximum tree depth | 3-10 |\n",
        "| `min_samples_split` | Min samples to split | 2-20 |\n",
        "| `min_samples_leaf` | Min samples per leaf | 1-10 |\n",
        "| `max_leaf_nodes` | Max leaf nodes | 10-100 |\n",
        "| `criterion` | Split quality measure | 'gini', 'entropy' |\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Random Forests (ensemble of trees)\n",
        "- Gradient Boosting (sequential trees)\n",
        "- XGBoost, LightGBM, CatBoost\n",
        "- Feature engineering for trees\n",
        "- Model interpretation (SHAP values)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}