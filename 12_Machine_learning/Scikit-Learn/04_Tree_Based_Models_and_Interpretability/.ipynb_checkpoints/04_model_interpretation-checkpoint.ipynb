{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Model Interpretation and Explainability\n",
        "\n",
        "## Overview\n",
        "\n",
        "**Model interpretability** answers the question: *Why did the model make this prediction?*\n",
        "\n",
        "Understanding model decisions is crucial for:\n",
        "- **Trust**: Stakeholders need to trust predictions\n",
        "- **Debugging**: Identify when models fail and why\n",
        "- **Compliance**: Regulatory requirements (GDPR, Fair Lending)\n",
        "- **Discovery**: Learn new insights from data patterns\n",
        "- **Fairness**: Detect and mitigate bias\n",
        "\n",
        "## Interpretability Spectrum\n",
        "\n",
        "```\n",
        "Inherently Interpretable          Black Box (Need Explanation Tools)\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "Linear Models   Decision Trees   Random Forests   Neural Networks\n",
        "Logistic Reg    (shallow)        Gradient Boost   Deep Learning\n",
        "```\n",
        "\n",
        "## Topics Covered\n",
        "\n",
        "### 1. Permutation Importance\n",
        "- Feature importance by shuffling\n",
        "- Model-agnostic approach\n",
        "- sklearn implementation\n",
        "\n",
        "### 2. SHAP (SHapley Additive exPlanations)\n",
        "- Game theory foundations\n",
        "- Shapley values explained\n",
        "- Global and local interpretability\n",
        "- Different SHAP explainers\n",
        "\n",
        "### 3. Practical Applications\n",
        "- Real-world datasets\n",
        "- Comparing interpretation methods\n",
        "- Feature engineering insights\n",
        "- Model debugging"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sklearn models and utilities\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, roc_auc_score\n",
        "from sklearn.datasets import load_diabetes, load_breast_cancer, load_boston\n",
        "\n",
        "# Interpretation tools\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# SHAP (install if needed: pip install shap)\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "    print(f\"\u2713 SHAP version {shap.__version__} loaded\")\n",
        "except ImportError:\n",
        "    SHAP_AVAILABLE = False\n",
        "    print(\"\u26a0 SHAP not installed. Install with: pip install shap\")\n",
        "    print(\"  Some sections will be skipped.\")\n",
        "\n",
        "np.random.seed(42)\n",
        "sns.set_style('whitegrid')\n",
        "print(\"\\n\u2713 Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "interpretability-intro",
      "metadata": {},
      "source": [
        "## 1. Why Model Interpretation Matters\n",
        "\n",
        "### 1.1 The Interpretability-Accuracy Tradeoff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "interpretability-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Model Interpretability vs Accuracy Tradeoff\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "models_spectrum = [\n",
        "    {\"Model\": \"Linear Regression\", \"Interpretability\": \"High\", \"Typical Accuracy\": \"Low-Medium\",\n",
        "     \"Interpretation\": \"Coefficients directly show feature impact\"},\n",
        "    {\"Model\": \"Logistic Regression\", \"Interpretability\": \"High\", \"Typical Accuracy\": \"Low-Medium\",\n",
        "     \"Interpretation\": \"Log-odds coefficients, probability interpretation\"},\n",
        "    {\"Model\": \"Decision Tree\", \"Interpretability\": \"High\", \"Typical Accuracy\": \"Medium\",\n",
        "     \"Interpretation\": \"Visual tree structure, if-then rules\"},\n",
        "    {\"Model\": \"Random Forest\", \"Interpretability\": \"Medium\", \"Typical Accuracy\": \"High\",\n",
        "     \"Interpretation\": \"Feature importances, partial dependence\"},\n",
        "    {\"Model\": \"Gradient Boosting\", \"Interpretability\": \"Medium\", \"Typical Accuracy\": \"High\",\n",
        "     \"Interpretation\": \"Feature importances, SHAP values\"},\n",
        "    {\"Model\": \"Neural Networks\", \"Interpretability\": \"Low\", \"Typical Accuracy\": \"Very High\",\n",
        "     \"Interpretation\": \"Requires advanced techniques (attention, saliency)\"},\n",
        "]\n",
        "\n",
        "df_spectrum = pd.DataFrame(models_spectrum)\n",
        "print(df_spectrum.to_string(index=False))\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Key Insight:\")\n",
        "print(\"   More complex models often perform better but are harder to interpret.\")\n",
        "print(\"   Model interpretation tools bridge this gap!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "permutation-importance",
      "metadata": {},
      "source": [
        "## 2. Permutation Importance\n",
        "\n",
        "### Concept\n",
        "\n",
        "**Permutation Importance** measures feature importance by observing how much the model's performance degrades when a feature's values are randomly shuffled.\n",
        "\n",
        "**Algorithm**:\n",
        "1. Train model and calculate baseline performance\n",
        "2. For each feature:\n",
        "   - Randomly shuffle the feature's values\n",
        "   - Calculate performance with shuffled feature\n",
        "   - Importance = baseline_score - shuffled_score\n",
        "3. Repeat shuffling multiple times for stability\n",
        "\n",
        "**Advantages**:\n",
        "- Model-agnostic (works with any model)\n",
        "- Intuitive interpretation\n",
        "- Captures feature interactions\n",
        "\n",
        "**Disadvantages**:\n",
        "- Computationally expensive\n",
        "- Can be misleading with correlated features\n",
        "- Only shows importance, not direction of effect\n",
        "\n",
        "### 2.1 Simple Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "permutation-simple",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X_diabetes = diabetes.data\n",
        "y_diabetes = diabetes.target\n",
        "feature_names = diabetes.feature_names\n",
        "\n",
        "print(\"Permutation Importance Demo - Diabetes Dataset\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_diabetes.shape[0]}\")\n",
        "print(f\"Features: {feature_names}\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_diabetes, y_diabetes, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train a random forest\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Baseline performance\n",
        "y_pred = rf_model.predict(X_test)\n",
        "baseline_r2 = r2_score(y_test, y_pred)\n",
        "baseline_mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nBaseline Model Performance:\")\n",
        "print(f\"  R\u00b2 Score: {baseline_r2:.4f}\")\n",
        "print(f\"  MSE: {baseline_mse:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "permutation-compute",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate permutation importance\n",
        "perm_importance = permutation_importance(\n",
        "    rf_model, X_test, y_test,\n",
        "    n_repeats=30,  # Number of times to shuffle each feature\n",
        "    random_state=42,\n",
        "    scoring='r2'\n",
        ")\n",
        "\n",
        "print(\"\\nPermutation Importance Results\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create results dataframe\n",
        "perm_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': perm_importance.importances_mean,\n",
        "    'Std': perm_importance.importances_std\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(perm_df.to_string(index=False))\n",
        "\n",
        "print(f\"\\nInterpretation:\")\n",
        "print(f\"  - Higher importance = shuffling causes larger performance drop\")\n",
        "print(f\"  - Most important: {perm_df.iloc[0]['Feature']}\")\n",
        "print(f\"  - Least important: {perm_df.iloc[-1]['Feature']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "permutation-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize permutation importance\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Bar plot\n",
        "axes[0].barh(perm_df['Feature'], perm_df['Importance'], xerr=perm_df['Std'], alpha=0.8)\n",
        "axes[0].set_xlabel('Permutation Importance (R\u00b2 decrease)')\n",
        "axes[0].set_title('Feature Importance with Standard Deviation')\n",
        "axes[0].grid(alpha=0.3, axis='x')\n",
        "\n",
        "# Box plot showing distribution\n",
        "perm_importance_array = perm_importance.importances.T\n",
        "axes[1].boxplot(perm_importance_array, labels=feature_names, vert=False)\n",
        "axes[1].set_xlabel('Permutation Importance')\n",
        "axes[1].set_title('Importance Distribution (30 shuffles per feature)')\n",
        "axes[1].grid(alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Error bars show variability across shuffles\")\n",
        "print(\"   Larger bars = less stable importance estimate\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "permutation-vs-builtin",
      "metadata": {},
      "source": [
        "### 2.2 Permutation Importance vs Built-in Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comparison-importances",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare with built-in feature importance\n",
        "builtin_importance = rf_model.feature_importances_\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Permutation': perm_importance.importances_mean,\n",
        "    'Built-in (Gini)': builtin_importance\n",
        "}).sort_values('Permutation', ascending=False)\n",
        "\n",
        "print(\"Permutation vs Built-in Feature Importance\")\n",
        "print(\"=\"*70)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualize comparison\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "x = np.arange(len(feature_names))\n",
        "width = 0.35\n",
        "\n",
        "# Sort by permutation importance\n",
        "sorted_indices = np.argsort(perm_importance.importances_mean)[::-1]\n",
        "sorted_features = [feature_names[i] for i in sorted_indices]\n",
        "sorted_perm = perm_importance.importances_mean[sorted_indices]\n",
        "sorted_builtin = builtin_importance[sorted_indices]\n",
        "\n",
        "ax.barh(x - width/2, sorted_perm, width, label='Permutation', alpha=0.8)\n",
        "ax.barh(x + width/2, sorted_builtin, width, label='Built-in (Gini)', alpha=0.8)\n",
        "ax.set_yticks(x)\n",
        "ax.set_yticklabels(sorted_features)\n",
        "ax.set_xlabel('Importance')\n",
        "ax.set_title('Permutation vs Built-in Feature Importance')\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Differences between methods:\")\n",
        "print(\"   Built-in (Gini): Based on impurity decrease during training\")\n",
        "print(\"   Permutation: Based on actual prediction performance\")\n",
        "print(\"   Permutation is often more reliable!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "shap-intro",
      "metadata": {},
      "source": [
        "## 3. SHAP (SHapley Additive exPlanations)\n",
        "\n",
        "### Mathematical Foundation\n",
        "\n",
        "**Shapley Values** come from cooperative game theory. They answer:\n",
        "*How much does each player (feature) contribute to the payoff (prediction)?*\n",
        "\n",
        "**Key Properties**:\n",
        "1. **Additivity**: Sum of all SHAP values = prediction - baseline\n",
        "2. **Consistency**: If a feature helps more, its value increases\n",
        "3. **Missingness**: Features not used get zero value\n",
        "4. **Symmetry**: Identical features get identical values\n",
        "\n",
        "**Formula** (simplified):\n",
        "\\[\n",
        "\\phi_j = \\sum_{S \\subseteq F \\setminus \\{j\\}} \\frac{|S|!(|F|-|S|-1)!}{|F|!} [f(S \\cup \\{j\\}) - f(S)]\n",
        "\\]\n",
        "\n",
        "where:\n",
        "- \\(\\phi_j\\) = SHAP value for feature j\n",
        "- \\(F\\) = set of all features\n",
        "- \\(S\\) = subset of features\n",
        "- \\(f(S)\\) = prediction using only features in S\n",
        "\n",
        "**Intuition**: Average marginal contribution of a feature across all possible feature combinations.\n",
        "\n",
        "### SHAP Explainer Types\n",
        "\n",
        "| Explainer | Best For | Speed | Accuracy |\n",
        "|-----------|----------|-------|----------|\n",
        "| TreeExplainer | Tree-based models (RF, XGBoost) | Fast | Exact |\n",
        "| LinearExplainer | Linear models | Very Fast | Exact |\n",
        "| KernelExplainer | Any model (model-agnostic) | Slow | Approximate |\n",
        "| DeepExplainer | Neural networks | Medium | Approximate |\n",
        "| GradientExplainer | Differentiable models | Medium | Approximate |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shap-check",
      "metadata": {},
      "outputs": [],
      "source": [
        "if not SHAP_AVAILABLE:\n",
        "    print(\"\u26a0 SHAP sections require 'shap' package.\")\n",
        "    print(\"  Install with: pip install shap\")\n",
        "    print(\"  Skipping SHAP demonstrations...\")\n",
        "else:\n",
        "    print(\"\u2713 SHAP is available. Proceeding with demonstrations...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "shap-regression",
      "metadata": {},
      "source": [
        "### 3.1 SHAP for Regression (Tree-based Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shap-regression-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "if SHAP_AVAILABLE:\n",
        "    print(\"SHAP Analysis - Diabetes Regression\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Create SHAP explainer for tree model\n",
        "    explainer = shap.TreeExplainer(rf_model)\n",
        "    \n",
        "    # Calculate SHAP values for test set\n",
        "    shap_values = explainer.shap_values(X_test)\n",
        "    \n",
        "    print(f\"SHAP values shape: {shap_values.shape}\")\n",
        "    print(f\"  (samples \u00d7 features) = ({shap_values.shape[0]} \u00d7 {shap_values.shape[1]})\")\n",
        "    print(f\"\\nExpected value (baseline): {explainer.expected_value:.2f}\")\n",
        "    print(f\"  This is the average prediction across training data\")\n",
        "    \n",
        "    # Show one prediction breakdown\n",
        "    sample_idx = 0\n",
        "    print(f\"\\nExample: Sample {sample_idx}\")\n",
        "    print(f\"  Actual prediction: {y_pred[sample_idx]:.2f}\")\n",
        "    print(f\"  Baseline: {explainer.expected_value:.2f}\")\n",
        "    print(f\"  Sum of SHAP values: {shap_values[sample_idx].sum():.2f}\")\n",
        "    print(f\"  Baseline + SHAP sum: {explainer.expected_value + shap_values[sample_idx].sum():.2f}\")\n",
        "    print(f\"\\n  \u2713 Additivity property satisfied!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "shap-waterfall",
      "metadata": {},
      "source": [
        "### 3.2 SHAP Waterfall Plot (Single Prediction Explanation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shap-waterfall-plot",
      "metadata": {},
      "outputs": [],
      "source": [
        "if SHAP_AVAILABLE:\n",
        "    print(\"\\nWaterfall Plot: Explaining a Single Prediction\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Select a sample to explain\n",
        "    sample_idx = 5\n",
        "    \n",
        "    # Create explanation object\n",
        "    explanation = shap.Explanation(\n",
        "        values=shap_values[sample_idx],\n",
        "        base_values=explainer.expected_value,\n",
        "        data=X_test[sample_idx],\n",
        "        feature_names=feature_names\n",
        "    )\n",
        "    \n",
        "    # Waterfall plot\n",
        "    shap.plots.waterfall(explanation, max_display=10)\n",
        "    \n",
        "    print(f\"\\nInterpretation:\")\n",
        "    print(f\"  - Red bars: Features pushing prediction higher\")\n",
        "    print(f\"  - Blue bars: Features pushing prediction lower\")\n",
        "    print(f\"  - Starts from baseline (expected value)\")\n",
        "    print(f\"  - Each feature adds/subtracts to reach final prediction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "shap-force-plot",
      "metadata": {},
      "source": [
        "### 3.3 SHAP Force Plot (Alternative Visualization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shap-force-plot",
      "metadata": {},
      "outputs": [],
      "source": [
        "if SHAP_AVAILABLE:\n",
        "    print(\"\\nForce Plot: Interactive Visualization\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Force plot for single prediction\n",
        "    shap.initjs()  # Initialize JavaScript for notebook\n",
        "    \n",
        "    sample_idx = 5\n",
        "    shap.force_plot(\n",
        "        explainer.expected_value,\n",
        "        shap_values[sample_idx],\n",
        "        X_test[sample_idx],\n",
        "        feature_names=feature_names\n",
        "    )\n",
        "    \n",
        "    print(\"\\nForce Plot shows:\")\n",
        "    print(\"  - Features pushing prediction higher (red)\")\n",
        "    print(\"  - Features pushing prediction lower (blue)\")\n",
        "    print(\"  - Width of each bar = magnitude of impact\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "shap-summary",
      "metadata": {},
      "source": [
        "### 3.4 SHAP Summary Plot (Global Feature Importance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shap-summary-plot",
      "metadata": {},
      "outputs": [],
      "source": [
        "if SHAP_AVAILABLE:\n",
        "    print(\"\\nSHAP Summary Plot: Global Feature Importance\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Summary plot (beeswarm)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    shap.summary_plot(shap_values, X_test, feature_names=feature_names, show=False)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nHow to read this plot:\")\n",
        "    print(\"  - Y-axis: Features sorted by importance\")\n",
        "    print(\"  - X-axis: SHAP value (impact on prediction)\")\n",
        "    print(\"  - Color: Feature value (red=high, blue=low)\")\n",
        "    print(\"  - Each dot: One sample\")\n",
        "    print(\"\\nExample interpretation:\")\n",
        "    print(\"  - If red dots are on the right \u2192 high feature value increases prediction\")\n",
        "    print(\"  - If blue dots are on the left \u2192 low feature value decreases prediction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "shap-bar",
      "metadata": {},
      "source": [
        "### 3.5 SHAP Bar Plot (Mean Absolute SHAP Values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shap-bar-plot",
      "metadata": {},
      "outputs": [],
      "source": [
        "if SHAP_AVAILABLE:\n",
        "    print(\"\\nSHAP Bar Plot: Average Feature Impact\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Bar plot of mean absolute SHAP values\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    shap.summary_plot(shap_values, X_test, feature_names=feature_names, \n",
        "                     plot_type=\"bar\", show=False)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Calculate mean absolute SHAP values\n",
        "    mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
        "    shap_importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Mean |SHAP|': mean_abs_shap\n",
        "    }).sort_values('Mean |SHAP|', ascending=False)\n",
        "    \n",
        "    print(\"\\nMean Absolute SHAP Values:\")\n",
        "    print(shap_importance_df.to_string(index=False))\n",
        "    \n",
        "    print(\"\\n\ud83d\udca1 This shows average magnitude of feature impact across all samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "shap-dependence",
      "metadata": {},
      "source": [
        "### 3.6 SHAP Dependence Plot (Feature Effects)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shap-dependence-plot",
      "metadata": {},
      "outputs": [],
      "source": [
        "if SHAP_AVAILABLE:\n",
        "    print(\"\\nSHAP Dependence Plot: How Feature Values Affect Predictions\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Get most important features\n",
        "    top_features = shap_importance_df['Feature'].head(2).tolist()\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    for idx, feature in enumerate(top_features):\n",
        "        plt.sca(axes[idx])\n",
        "        shap.dependence_plot(\n",
        "            feature, shap_values, X_test,\n",
        "            feature_names=feature_names,\n",
        "            show=False\n",
        "        )\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nDependence Plot shows:\")\n",
        "    print(\"  - X-axis: Feature value\")\n",
        "    print(\"  - Y-axis: SHAP value (impact on prediction)\")\n",
        "    print(\"  - Color: Interaction with another feature\")\n",
        "    print(\"  - Reveals non-linear effects and interactions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "shap-classification",
      "metadata": {},
      "source": [
        "## 4. SHAP for Classification\n",
        "\n",
        "### 4.1 Binary Classification Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shap-classification-setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "if SHAP_AVAILABLE:\n",
        "    # Load breast cancer dataset\n",
        "    cancer = load_breast_cancer()\n",
        "    X_cancer = cancer.data\n",
        "    y_cancer = cancer.target\n",
        "    \n",
        "    print(\"SHAP for Classification - Breast Cancer Dataset\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Samples: {X_cancer.shape[0]}\")\n",
        "    print(f\"Features: {X_cancer.shape[1]}\")\n",
        "    print(f\"Classes: {cancer.target_names}\")\n",
        "    \n",
        "    # Split data\n",
        "    X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = train_test_split(\n",
        "        X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
        "    )\n",
        "    \n",
        "    # Train classifier\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf_classifier.fit(X_train_cancer, y_train_cancer)\n",
        "    \n",
        "    # Evaluate\n",
        "    y_pred_cancer = rf_classifier.predict(X_test_cancer)\n",
        "    accuracy = accuracy_score(y_test_cancer, y_pred_cancer)\n",
        "    auc = roc_auc_score(y_test_cancer, rf_classifier.predict_proba(X_test_cancer)[:, 1])\n",
        "    \n",
        "    print(f\"\\nModel Performance:\")\n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  AUC: {auc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shap-classification-explain",
      "metadata": {},
      "outputs": [],
      "source": [
        "if SHAP_AVAILABLE:\n",
        "    print(\"\\nCalculating SHAP values for classifier...\")\n",
        "    \n",
        "    # Create explainer\n",
        "    explainer_cancer = shap.TreeExplainer(rf_classifier)\n",
        "    \n",
        "    # Calculate SHAP values\n",
        "    shap_values_cancer = explainer_cancer.shap_values(X_test_cancer)\n",
        "    \n",
        "    print(f\"\\nSHAP values shape: {len(shap_values_cancer)} classes\")\n",
        "    print(f\"  Class 0 (malignant): {shap_values_cancer[0].shape}\")\n",
        "    print(f\"  Class 1 (benign): {shap_values_cancer[1].shape}\")\n",
        "    print(f\"\\n\ud83d\udca1 For binary classification, we typically use class 1 SHAP values\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shap-classification-summary",
      "metadata": {},
      "outputs": [],
      "source": [
        "if SHAP_AVAILABLE:\n",
        "    print(\"\\nSHAP Summary for Binary Classification\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Summary plot for benign class (class 1)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    shap.summary_plot(\n",
        "        shap_values_cancer[1], \n",
        "        X_test_cancer,\n",
        "        feature_names=cancer.feature_names,\n",
        "        max_display=15,\n",
        "        show=False\n",
        "    )\n",
        "    plt.title('SHAP Summary: Predicting Benign (class 1)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nInterpretation for benign prediction:\")\n",
        "    print(\"  - Positive SHAP \u2192 Increases probability of benign\")\n",
        "    print(\"  - Negative SHAP \u2192 Decreases probability of benign (= increases malignant)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "shap-single-prediction",
      "metadata": {},
      "source": [
        "### 4.2 Explaining Individual Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shap-individual-classification",
      "metadata": {},
      "outputs": [],
      "source": [
        "if SHAP_AVAILABLE:\n",
        "    print(\"\\nExplaining Individual Cancer Predictions\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Find interesting samples\n",
        "    proba = rf_classifier.predict_proba(X_test_cancer)\n",
        "    \n",
        "    # High confidence benign\n",
        "    confident_benign = np.where((proba[:, 1] > 0.95) & (y_test_cancer == 1))[0][0]\n",
        "    # High confidence malignant\n",
        "    confident_malignant = np.where((proba[:, 0] > 0.95) & (y_test_cancer == 0))[0][0]\n",
        "    \n",
        "    print(f\"\\nSample {confident_benign}:\")\n",
        "    print(f\"  True label: Benign\")\n",
        "    print(f\"  Predicted probability: {proba[confident_benign, 1]:.3f} (Benign)\")\n",
        "    \n",
        "    # Waterfall plot\n",
        "    explanation_benign = shap.Explanation(\n",
        "        values=shap_values_cancer[1][confident_benign],\n",
        "        base_values=explainer_cancer.expected_value[1],\n",
        "        data=X_test_cancer[confident_benign],\n",
        "        feature_names=cancer.feature_names\n",
        "    )\n",
        "    shap.plots.waterfall(explanation_benign, max_display=15)\n",
        "    \n",
        "    print(f\"\\n\\nSample {confident_malignant}:\")\n",
        "    print(f\"  True label: Malignant\")\n",
        "    print(f\"  Predicted probability: {proba[confident_malignant, 0]:.3f} (Malignant)\")\n",
        "    \n",
        "    # For malignant, show class 0 SHAP values\n",
        "    explanation_malignant = shap.Explanation(\n",
        "        values=shap_values_cancer[0][confident_malignant],\n",
        "        base_values=explainer_cancer.expected_value[0],\n",
        "        data=X_test_cancer[confident_malignant],\n",
        "        feature_names=cancer.feature_names\n",
        "    )\n",
        "    shap.plots.waterfall(explanation_malignant, max_display=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comparison",
      "metadata": {},
      "source": [
        "## 5. Comparing Interpretation Methods\n",
        "\n",
        "### 5.1 Side-by-Side Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comparison-methods",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Comparison of Interpretation Methods\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Using diabetes dataset and random forest from earlier\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Built-in': rf_model.feature_importances_,\n",
        "    'Permutation': perm_importance.importances_mean,\n",
        "})\n",
        "\n",
        "if SHAP_AVAILABLE:\n",
        "    comparison_df['SHAP (mean |value|)'] = np.abs(shap_values).mean(axis=0)\n",
        "\n",
        "# Normalize for comparison\n",
        "for col in comparison_df.columns[1:]:\n",
        "    comparison_df[f'{col}_norm'] = comparison_df[col] / comparison_df[col].sum()\n",
        "\n",
        "# Sort by SHAP if available, else permutation\n",
        "sort_col = 'SHAP (mean |value|)' if SHAP_AVAILABLE else 'Permutation'\n",
        "comparison_df = comparison_df.sort_values(sort_col, ascending=False)\n",
        "\n",
        "print(\"\\nNormalized Importance Scores:\")\n",
        "print(comparison_df[['Feature', 'Built-in', 'Permutation'] + \n",
        "                   (['SHAP (mean |value|)'] if SHAP_AVAILABLE else [])].to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comparison-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize comparison\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "x = np.arange(len(feature_names))\n",
        "width = 0.25 if SHAP_AVAILABLE else 0.35\n",
        "\n",
        "methods = ['Built-in', 'Permutation']\n",
        "if SHAP_AVAILABLE:\n",
        "    methods.append('SHAP (mean |value|)')\n",
        "\n",
        "for idx, method in enumerate(methods):\n",
        "    offset = (idx - len(methods)/2 + 0.5) * width\n",
        "    ax.barh(x + offset, comparison_df[method], width, label=method, alpha=0.8)\n",
        "\n",
        "ax.set_yticks(x)\n",
        "ax.set_yticklabels(comparison_df['Feature'])\n",
        "ax.set_xlabel('Importance')\n",
        "ax.set_title('Feature Importance: Method Comparison')\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Notice the differences:\")\n",
        "print(\"   - Rankings may differ between methods\")\n",
        "print(\"   - Each method measures importance differently\")\n",
        "print(\"   - Use multiple methods for robust conclusions!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "linear-model-shap",
      "metadata": {},
      "source": [
        "## 6. SHAP for Linear Models\n",
        "\n",
        "### 6.1 Linear Model SHAP Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shap-linear",
      "metadata": {},
      "outputs": [],
      "source": [
        "if SHAP_AVAILABLE:\n",
        "    print(\"SHAP for Linear Models\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Train linear model\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    \n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    linear_model = LinearRegression()\n",
        "    linear_model.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Linear model performance\n",
        "    y_pred_linear = linear_model.predict(X_test_scaled)\n",
        "    r2_linear = r2_score(y_test, y_pred_linear)\n",
        "    print(f\"Linear Model R\u00b2: {r2_linear:.4f}\")\n",
        "    print(f\"Random Forest R\u00b2: {baseline_r2:.4f}\")\n",
        "    \n",
        "    # SHAP for linear model\n",
        "    explainer_linear = shap.LinearExplainer(linear_model, X_train_scaled)\n",
        "    shap_values_linear = explainer_linear.shap_values(X_test_scaled)\n",
        "    \n",
        "    print(f\"\\n\ud83d\udca1 For linear models, SHAP values are exact and fast to compute!\")\n",
        "    print(f\"   SHAP value = coefficient \u00d7 (feature_value - feature_mean)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shap-linear-viz",
      "metadata": {},
      "outputs": [],
      "source": [
        "if SHAP_AVAILABLE:\n",
        "    print(\"\\nSHAP Summary for Linear Model\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    shap.summary_plot(shap_values_linear, X_test_scaled, \n",
        "                     feature_names=feature_names, show=False)\n",
        "    plt.title('SHAP Summary: Linear Regression')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Compare linear coefficients with SHAP importance\n",
        "    mean_abs_shap_linear = np.abs(shap_values_linear).mean(axis=0)\n",
        "    \n",
        "    linear_comparison = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Coefficient': linear_model.coef_,\n",
        "        'Abs Coefficient': np.abs(linear_model.coef_),\n",
        "        'Mean |SHAP|': mean_abs_shap_linear\n",
        "    }).sort_values('Mean |SHAP|', ascending=False)\n",
        "    \n",
        "    print(\"\\nLinear Model: Coefficients vs SHAP:\")\n",
        "    print(linear_comparison.to_string(index=False))\n",
        "    \n",
        "    print(\"\\n\ud83d\udca1 For linear models:\")\n",
        "    print(\"   - SHAP importance \u221d |coefficient| \u00d7 feature_std\")\n",
        "    print(\"   - Accounts for both coefficient size and feature variance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "practical-tips",
      "metadata": {},
      "source": [
        "## 7. Practical Tips and Best Practices\n",
        "\n",
        "### 7.1 Method Selection Guide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "method-guide",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Interpretation Method Selection Guide\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "guide = [\n",
        "    {\n",
        "        'Method': 'Model Coefficients',\n",
        "        'When to Use': 'Linear models only',\n",
        "        'Pros': 'Fast, exact, easy to understand',\n",
        "        'Cons': 'Only for linear models',\n",
        "        'Scope': 'Global'\n",
        "    },\n",
        "    {\n",
        "        'Method': 'Built-in Feature Importance',\n",
        "        'When to Use': 'Tree-based models (quick check)',\n",
        "        'Pros': 'Very fast, no extra computation',\n",
        "        'Cons': 'Can be biased, less reliable',\n",
        "        'Scope': 'Global'\n",
        "    },\n",
        "    {\n",
        "        'Method': 'Permutation Importance',\n",
        "        'When to Use': 'Any model, small datasets',\n",
        "        'Pros': 'Model-agnostic, reliable',\n",
        "        'Cons': 'Slow, issues with correlated features',\n",
        "        'Scope': 'Global'\n",
        "    },\n",
        "    {\n",
        "        'Method': 'SHAP (TreeExplainer)',\n",
        "        'When to Use': 'Tree models, need local explanations',\n",
        "        'Pros': 'Fast, exact, local + global',\n",
        "        'Cons': 'Only for tree models',\n",
        "        'Scope': 'Both'\n",
        "    },\n",
        "    {\n",
        "        'Method': 'SHAP (LinearExplainer)',\n",
        "        'When to Use': 'Linear models',\n",
        "        'Pros': 'Very fast, exact',\n",
        "        'Cons': 'Only for linear models',\n",
        "        'Scope': 'Both'\n",
        "    },\n",
        "    {\n",
        "        'Method': 'SHAP (KernelExplainer)',\n",
        "        'When to Use': 'Any model (last resort)',\n",
        "        'Pros': 'Model-agnostic, local explanations',\n",
        "        'Cons': 'Very slow, approximate',\n",
        "        'Scope': 'Both'\n",
        "    },\n",
        "]\n",
        "\n",
        "guide_df = pd.DataFrame(guide)\n",
        "print(guide_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "best-practices",
      "metadata": {},
      "source": [
        "### 7.2 Common Pitfalls and Solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pitfalls",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nCommon Pitfalls in Model Interpretation\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "pitfalls = [\n",
        "    (\"\u274c Pitfall\", \"\u2713 Solution\"),\n",
        "    (\"-\" * 35, \"-\" * 35),\n",
        "    (\"Trusting single interpretation method\", \"Use multiple methods and compare\"),\n",
        "    (\"Interpreting coefficients without scaling\", \"Always standardize features first\"),\n",
        "    (\"Using built-in importance blindly\", \"Verify with permutation/SHAP\"),\n",
        "    (\"Ignoring feature correlations\", \"Check correlations, use SHAP for interactions\"),\n",
        "    (\"Not checking model performance first\", \"Only interpret well-performing models\"),\n",
        "    (\"Overfitting interpretation to test set\", \"Use separate validation set\"),\n",
        "    (\"Mistaking correlation for causation\", \"Remember: models show associations, not causes\"),\n",
        "    (\"Using wrong SHAP explainer\", \"TreeExplainer for trees, LinearExplainer for linear\"),\n",
        "]\n",
        "\n",
        "for pitfall, solution in pitfalls:\n",
        "    print(f\"{pitfall:<40} {solution}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Summary and Decision Tree\n",
        "\n",
        "### Quick Reference\n",
        "\n",
        "```python\n",
        "# Permutation Importance (sklearn)\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "perm_imp = permutation_importance(\n",
        "    model, X_test, y_test,\n",
        "    n_repeats=30,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# SHAP for Tree Models\n",
        "import shap\n",
        "\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "shap.summary_plot(shap_values, X_test)\n",
        "\n",
        "# SHAP for Linear Models\n",
        "explainer = shap.LinearExplainer(model, X_train)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# SHAP for Any Model (slow)\n",
        "explainer = shap.KernelExplainer(model.predict, X_train_sample)\n",
        "shap_values = explainer.shap_values(X_test_sample)\n",
        "```\n",
        "\n",
        "### Decision Tree: Which Method to Use?\n",
        "\n",
        "```\n",
        "Start Here\n",
        "    |\n",
        "    \u251c\u2500 Need local explanations? (individual predictions)\n",
        "    \u2502   |\n",
        "    \u2502   YES \u2192 Use SHAP\n",
        "    \u2502   \u2502       |\n",
        "    \u2502   \u2502       \u251c\u2500 Tree model \u2192 TreeExplainer\n",
        "    \u2502   \u2502       \u251c\u2500 Linear model \u2192 LinearExplainer\n",
        "    \u2502   \u2502       \u2514\u2500 Other model \u2192 KernelExplainer (slow)\n",
        "    \u2502   \u2502\n",
        "    \u2502   NO \u2192 Need global explanations only\n",
        "    \u2502           |\n",
        "    \u2502           \u251c\u2500 Linear model \u2192 Coefficients\n",
        "    \u2502           \u251c\u2500 Tree model \u2192 Built-in + Permutation\n",
        "    \u2502           \u2514\u2500 Any model \u2192 Permutation Importance\n",
        "    \u2502\n",
        "    \u2514\u2500 Very large dataset? (>100k samples)\n",
        "        |\n",
        "        YES \u2192 Sample data for SHAP, use permutation on sample\n",
        "        NO \u2192 Use any method above\n",
        "```\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Always use multiple interpretation methods** - Different methods reveal different insights\n",
        "\n",
        "2. **SHAP is often the best choice** when you need:\n",
        "   - Local explanations (why this prediction?)\n",
        "   - Consistent, theoretically grounded values\n",
        "   - Both global and local interpretability\n",
        "\n",
        "3. **Permutation importance** is great for:\n",
        "   - Quick model-agnostic feature ranking\n",
        "   - When you don't need local explanations\n",
        "   - Validating built-in importance\n",
        "\n",
        "4. **Watch out for**:\n",
        "   - Correlated features (can mislead all methods)\n",
        "   - Small sample sizes (unstable importance estimates)\n",
        "   - Overfitting (interpret well-validated models only)\n",
        "\n",
        "### Further Reading\n",
        "\n",
        "- **SHAP Paper**: Lundberg & Lee (2017) - \"A Unified Approach to Interpreting Model Predictions\"\n",
        "- **Interpretable ML Book**: Christoph Molnar - https://christophm.github.io/interpretable-ml-book/\n",
        "- **SHAP Documentation**: https://shap.readthedocs.io/\n",
        "- **Sklearn Inspection**: https://scikit-learn.org/stable/modules/permutation_importance.html\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Partial Dependence Plots (PDPs)\n",
        "- Individual Conditional Expectation (ICE) plots\n",
        "- LIME (Local Interpretable Model-agnostic Explanations)\n",
        "- Accumulated Local Effects (ALE) plots\n",
        "- Counterfactual explanations\n",
        "- Fairness and bias detection"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}