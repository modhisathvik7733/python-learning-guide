{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Random Forest: Ensemble Learning\n",
        "\n",
        "## Overview\n",
        "\n",
        "**Random Forest** is an ensemble learning method that combines multiple decision trees to create a more robust and accurate model. It uses **bagging** (bootstrap aggregating) and **feature randomness** to reduce overfitting and improve generalization.\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "### Ensemble Learning\n",
        "\n",
        "**Wisdom of the Crowd**:\n",
        "\\[\n",
        "\\text{Final Prediction} = \\text{Aggregate}(\\text{Tree}_1, \\text{Tree}_2, \\ldots, \\text{Tree}_n)\n",
        "\\]\n",
        "\n",
        "**Classification**: Majority voting\n",
        "\\[\n",
        "\\hat{y} = \\text{mode}(h_1(x), h_2(x), \\ldots, h_n(x))\n",
        "\\]\n",
        "\n",
        "**Regression**: Average prediction\n",
        "\\[\n",
        "\\hat{y} = \\frac{1}{n}\\sum_{i=1}^{n} h_i(x)\n",
        "\\]\n",
        "\n",
        "### Bootstrap Aggregating (Bagging)\n",
        "\n",
        "1. Create \\(n\\) bootstrap samples (sampling with replacement)\n",
        "2. Train one tree on each sample\n",
        "3. Aggregate predictions\n",
        "\n",
        "Each bootstrap sample contains ~63.2% unique samples from original dataset.\n",
        "\n",
        "### Feature Randomness\n",
        "\n",
        "At each split, consider only a **random subset** of features:\n",
        "- Classification: \\(\\sqrt{p}\\) features (default)\n",
        "- Regression: \\(p/3\\) features (default)\n",
        "\n",
        "where \\(p\\) is total number of features.\n",
        "\n",
        "### Out-of-Bag (OOB) Score\n",
        "\n",
        "- Each tree is trained on ~63% of data\n",
        "- Remaining ~37% (out-of-bag) used for validation\n",
        "- OOB score approximates test performance without separate validation set\n",
        "\n",
        "## Topics Covered\n",
        "\n",
        "1. Random Forest Classification\n",
        "2. Random Forest Regression\n",
        "3. Out-of-Bag evaluation\n",
        "4. Feature importance (impurity-based and permutation)\n",
        "5. Hyperparameter tuning\n",
        "6. Comparison with single decision trees\n",
        "7. Real-world applications\n",
        "8. Partial dependence plots"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, confusion_matrix,\n",
        "    mean_squared_error, r2_score, roc_auc_score, roc_curve\n",
        ")\n",
        "from sklearn.datasets import (\n",
        "    make_classification, make_regression, load_iris, load_wine,\n",
        "    load_breast_cancer, load_diabetes, fetch_california_housing\n",
        ")\n",
        "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
        "\n",
        "np.random.seed(42)\n",
        "sns.set_style('whitegrid')\n",
        "print(\"\u2713 Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "classification",
      "metadata": {},
      "source": [
        "## 1. Random Forest Classification\n",
        "\n",
        "### 1.1 Comparison with Single Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comparison-setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=500,\n",
        "    n_features=10,\n",
        "    n_informative=8,\n",
        "    n_redundant=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Classification Dataset\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X.shape[0]}\")\n",
        "print(f\"Features: {X.shape[1]}\")\n",
        "print(f\"Classes: {np.unique(y)}\")\n",
        "print(f\"Class distribution: {np.bincount(y)}\")\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain: {X_train.shape[0]}, Test: {X_test.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "single-tree-vs-forest",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train single decision tree\n",
        "single_tree = DecisionTreeClassifier(random_state=42)\n",
        "single_tree.fit(X_train, y_train)\n",
        "\n",
        "# Train random forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Compare performance\n",
        "print(\"Single Decision Tree vs Random Forest\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "models = {\n",
        "    'Single Decision Tree': single_tree,\n",
        "    'Random Forest (100 trees)': rf\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    train_acc = model.score(X_train, y_train)\n",
        "    test_acc = model.score(X_test, y_test)\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Train Accuracy: {train_acc:.4f}\")\n",
        "    print(f\"  Test Accuracy:  {test_acc:.4f}\")\n",
        "    print(f\"  Overfitting Gap: {train_acc - test_acc:.4f}\")\n",
        "    \n",
        "    if hasattr(model, 'get_depth'):\n",
        "        print(f\"  Tree depth: {model.get_depth()}\")\n",
        "        print(f\"  Leaves: {model.get_n_leaves()}\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Key Observations:\")\n",
        "print(\"  - Random Forest reduces overfitting compared to single tree\")\n",
        "print(\"  - Better generalization (higher test accuracy)\")\n",
        "print(\"  - More stable predictions through averaging\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oob",
      "metadata": {},
      "source": [
        "### 1.2 Out-of-Bag (OOB) Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oob-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train with OOB score\n",
        "rf_oob = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    oob_score=True,\n",
        "    random_state=42\n",
        ")\n",
        "rf_oob.fit(X_train, y_train)\n",
        "\n",
        "print(\"Out-of-Bag (OOB) Evaluation\")\n",
        "print(\"=\"*70)\n",
        "print(f\"OOB Score: {rf_oob.oob_score_:.4f}\")\n",
        "print(f\"Test Score: {rf_oob.score(X_test, y_test):.4f}\")\n",
        "\n",
        "print(\"\\nWhat is OOB?\")\n",
        "print(\"  - Each tree trained on ~63% of data (bootstrap sample)\")\n",
        "print(\"  - Remaining ~37% (out-of-bag) used for validation\")\n",
        "print(\"  - OOB score approximates cross-validation without extra computation\")\n",
        "print(\"  - Usually close to test performance\")\n",
        "\n",
        "# Compare OOB vs cross-validation\n",
        "cv_scores = cross_val_score(rf_oob, X_train, y_train, cv=5)\n",
        "print(f\"\\n5-Fold CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
        "print(f\"OOB Score:       {rf_oob.oob_score_:.4f}\")\n",
        "print(\"\\n\u2713 OOB provides a good estimate without cross-validation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feature-importance",
      "metadata": {},
      "source": [
        "## 2. Feature Importance\n",
        "\n",
        "### 2.1 Impurity-Based Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "impurity-importance",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance\n",
        "importances = rf.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
        "\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': [f'Feature {i}' for i in range(X.shape[1])],\n",
        "    'Importance': importances,\n",
        "    'Std': std\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"Feature Importance (Impurity-Based)\")\n",
        "print(\"=\"*70)\n",
        "print(feature_importance_df.to_string(index=False))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'], \n",
        "        feature_importance_df['Importance'],\n",
        "        xerr=feature_importance_df['Std'],\n",
        "        alpha=0.8)\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importance with Error Bars')\n",
        "plt.grid(alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 How it works:\")\n",
        "print(\"  - Measures average impurity decrease across all trees\")\n",
        "print(\"  - Higher value = feature contributes more to predictions\")\n",
        "print(\"  - Sum of all importances = 1.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iris-example",
      "metadata": {},
      "source": [
        "## 3. Real Example: Iris Classification\n",
        "\n",
        "### 3.1 Train and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iris-rf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load iris\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target\n",
        "\n",
        "print(\"Iris Dataset with Random Forest\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_iris.shape[0]}\")\n",
        "print(f\"Features: {iris.feature_names}\")\n",
        "print(f\"Classes: {iris.target_names}\")\n",
        "\n",
        "# Split\n",
        "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
        "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
        ")\n",
        "\n",
        "# Train random forest\n",
        "rf_iris = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=5,\n",
        "    oob_score=True,\n",
        "    random_state=42\n",
        ")\n",
        "rf_iris.fit(X_train_iris, y_train_iris)\n",
        "\n",
        "# Evaluate\n",
        "train_acc = rf_iris.score(X_train_iris, y_train_iris)\n",
        "test_acc = rf_iris.score(X_test_iris, y_test_iris)\n",
        "oob_score = rf_iris.oob_score_\n",
        "\n",
        "print(f\"\\nPerformance:\")\n",
        "print(f\"  Train Accuracy: {train_acc:.4f}\")\n",
        "print(f\"  Test Accuracy:  {test_acc:.4f}\")\n",
        "print(f\"  OOB Score:      {oob_score:.4f}\")\n",
        "\n",
        "# Predictions\n",
        "y_pred_iris = rf_iris.predict(X_test_iris)\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_iris, y_pred_iris, target_names=iris.target_names))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test_iris, y_pred_iris)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=iris.target_names,\n",
        "            yticklabels=iris.target_names)\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.title('Random Forest: Iris Classification')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iris-feature-importance",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance for iris\n",
        "importances_iris = rf_iris.feature_importances_\n",
        "std_iris = np.std([tree.feature_importances_ for tree in rf_iris.estimators_], axis=0)\n",
        "\n",
        "feature_importance_iris = pd.DataFrame({\n",
        "    'Feature': iris.feature_names,\n",
        "    'Importance': importances_iris,\n",
        "    'Std': std_iris\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"Feature Importance: Iris Classification\")\n",
        "print(\"=\"*70)\n",
        "print(feature_importance_iris.to_string(index=False))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_iris['Feature'], \n",
        "        feature_importance_iris['Importance'],\n",
        "        xerr=feature_importance_iris['Std'],\n",
        "        alpha=0.8, color='skyblue')\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importance: Iris Dataset')\n",
        "plt.grid(alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Insights:\")\n",
        "print(f\"  Most important: {feature_importance_iris.iloc[0]['Feature']}\")\n",
        "print(f\"  Least important: {feature_importance_iris.iloc[-1]['Feature']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "regression",
      "metadata": {},
      "source": [
        "## 4. Random Forest Regression\n",
        "\n",
        "### 4.1 Diabetes Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "diabetes-rf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X_diabetes = diabetes.data\n",
        "y_diabetes = diabetes.target\n",
        "\n",
        "print(\"Diabetes Dataset - Random Forest Regression\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_diabetes.shape[0]}\")\n",
        "print(f\"Features: {X_diabetes.shape[1]}\")\n",
        "print(f\"Feature names: {diabetes.feature_names}\")\n",
        "\n",
        "# Split\n",
        "X_train_db, X_test_db, y_train_db, y_test_db = train_test_split(\n",
        "    X_diabetes, y_diabetes, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Compare single tree vs random forest\n",
        "models_reg = {\n",
        "    'Single Decision Tree': DecisionTreeRegressor(random_state=42),\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, oob_score=True)\n",
        "}\n",
        "\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for name, model in models_reg.items():\n",
        "    model.fit(X_train_db, y_train_db)\n",
        "    \n",
        "    y_train_pred = model.predict(X_train_db)\n",
        "    y_test_pred = model.predict(X_test_db)\n",
        "    \n",
        "    train_r2 = r2_score(y_train_db, y_train_pred)\n",
        "    test_r2 = r2_score(y_test_db, y_test_pred)\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train_db, y_train_pred))\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test_db, y_test_pred))\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Train R\u00b2: {train_r2:.4f}, Test R\u00b2: {test_r2:.4f}\")\n",
        "    print(f\"  Train RMSE: {train_rmse:.2f}, Test RMSE: {test_rmse:.2f}\")\n",
        "    \n",
        "    if hasattr(model, 'oob_score_'):\n",
        "        print(f\"  OOB R\u00b2: {model.oob_score_:.4f}\")\n",
        "\n",
        "# Keep RF model for further analysis\n",
        "rf_diabetes = models_reg['Random Forest']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "diabetes-predictions",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions\n",
        "y_pred_db = rf_diabetes.predict(X_test_db)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test_db, y_pred_db, alpha=0.6)\n",
        "plt.plot([y_test_db.min(), y_test_db.max()], \n",
        "         [y_test_db.min(), y_test_db.max()], 'r--', lw=2)\n",
        "plt.xlabel('Actual Disease Progression')\n",
        "plt.ylabel('Predicted Disease Progression')\n",
        "plt.title(f'Random Forest Regression (Test R\u00b2 = {test_r2:.3f})')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Feature importance\n",
        "importances_db = rf_diabetes.feature_importances_\n",
        "feature_importance_db = pd.DataFrame({\n",
        "    'Feature': diabetes.feature_names,\n",
        "    'Importance': importances_db\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importance: Diabetes Progression\")\n",
        "print(\"=\"*70)\n",
        "print(feature_importance_db.to_string(index=False))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_db['Feature'], \n",
        "        feature_importance_db['Importance'], alpha=0.8)\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importance: Diabetes Progression Prediction')\n",
        "plt.grid(alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hyperparameters",
      "metadata": {},
      "source": [
        "## 5. Hyperparameter Tuning\n",
        "\n",
        "### 5.1 Key Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hyperparameter-analysis",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Random Forest Hyperparameters\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "hyperparams = [\n",
        "    (\"n_estimators\", \"Number of trees\", \"More = better (diminishing returns)\"),\n",
        "    (\"max_depth\", \"Maximum tree depth\", \"Control individual tree complexity\"),\n",
        "    (\"min_samples_split\", \"Min samples to split\", \"Prevent overfitting\"),\n",
        "    (\"min_samples_leaf\", \"Min samples per leaf\", \"Smoother predictions\"),\n",
        "    (\"max_features\", \"Features per split\", \"sqrt (clf), 1/3 (reg) default\"),\n",
        "    (\"max_samples\", \"Samples per tree\", \"Bootstrap sample size\"),\n",
        "    (\"bootstrap\", \"Use bootstrap sampling\", \"True = bagging, False = pasting\"),\n",
        "]\n",
        "\n",
        "for param, desc, effect in hyperparams:\n",
        "    print(f\"\\n{param}:\")\n",
        "    print(f\"  {desc}\")\n",
        "    print(f\"  {effect}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n-estimators",
      "metadata": {},
      "source": [
        "### 5.2 Effect of Number of Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n-estimators-analysis",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different numbers of trees\n",
        "n_estimators_range = [1, 5, 10, 20, 50, 100, 200, 300]\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "oob_scores = []\n",
        "\n",
        "for n in n_estimators_range:\n",
        "    rf_temp = RandomForestClassifier(\n",
        "        n_estimators=n,\n",
        "        oob_score=True,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    rf_temp.fit(X_train, y_train)\n",
        "    \n",
        "    train_scores.append(rf_temp.score(X_train, y_train))\n",
        "    test_scores.append(rf_temp.score(X_test, y_test))\n",
        "    oob_scores.append(rf_temp.oob_score_)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(n_estimators_range, train_scores, 'o-', label='Train', linewidth=2)\n",
        "plt.plot(n_estimators_range, test_scores, 's-', label='Test', linewidth=2)\n",
        "plt.plot(n_estimators_range, oob_scores, '^-', label='OOB', linewidth=2)\n",
        "plt.xlabel('Number of Trees')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Effect of Number of Trees')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.xscale('log')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Observations:\")\n",
        "print(\"  - Performance improves rapidly up to ~50 trees\")\n",
        "print(\"  - Diminishing returns after ~100 trees\")\n",
        "print(\"  - More trees = more computation but no overfitting\")\n",
        "print(\"  - OOB score tracks test performance well\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "grid-search",
      "metadata": {},
      "source": [
        "### 5.3 Grid Search for Optimal Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "grid-search-rf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Randomized search (faster for Random Forests)\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 15, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Randomized Search for Optimal Hyperparameters\")\n",
        "print(\"=\"*70)\n",
        "print(\"Searching 20 random parameter combinations...\\n\")\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nBest Parameters:\")\n",
        "for param, value in random_search.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "print(f\"\\nBest CV Score: {random_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "best_rf = random_search.best_estimator_\n",
        "test_acc = best_rf.score(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "breast-cancer",
      "metadata": {},
      "source": [
        "## 6. Real-World Application: Breast Cancer Detection\n",
        "\n",
        "### 6.1 Complete Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "breast-cancer-rf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load breast cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X_cancer = cancer.data\n",
        "y_cancer = cancer.target\n",
        "\n",
        "print(\"Breast Cancer Detection with Random Forest\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Samples: {X_cancer.shape[0]}\")\n",
        "print(f\"Features: {X_cancer.shape[1]}\")\n",
        "print(f\"Classes: {cancer.target_names}\")\n",
        "\n",
        "# Split\n",
        "X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = train_test_split(\n",
        "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
        ")\n",
        "\n",
        "# Train optimized random forest\n",
        "rf_cancer = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    oob_score=True,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_cancer.fit(X_train_cancer, y_train_cancer)\n",
        "\n",
        "# Evaluate\n",
        "y_pred_cancer = rf_cancer.predict(X_test_cancer)\n",
        "y_proba_cancer = rf_cancer.predict_proba(X_test_cancer)[:, 1]\n",
        "\n",
        "test_acc = accuracy_score(y_test_cancer, y_pred_cancer)\n",
        "test_auc = roc_auc_score(y_test_cancer, y_proba_cancer)\n",
        "oob_score = rf_cancer.oob_score_\n",
        "\n",
        "print(f\"\\nPerformance:\")\n",
        "print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"  AUC: {test_auc:.4f}\")\n",
        "print(f\"  OOB Score: {oob_score:.4f}\")\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_cancer, y_pred_cancer, \n",
        "                          target_names=cancer.target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cancer-visualizations",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizations\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test_cancer, y_pred_cancer)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=cancer.target_names,\n",
        "            yticklabels=cancer.target_names,\n",
        "            ax=axes[0])\n",
        "axes[0].set_ylabel('True Label')\n",
        "axes[0].set_xlabel('Predicted Label')\n",
        "axes[0].set_title('Confusion Matrix')\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test_cancer, y_proba_cancer)\n",
        "axes[1].plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {test_auc:.3f}')\n",
        "axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "axes[1].set_xlabel('False Positive Rate')\n",
        "axes[1].set_ylabel('True Positive Rate')\n",
        "axes[1].set_title('ROC Curve')\n",
        "axes[1].legend(loc='lower right')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cancer-feature-importance",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Top features\n",
        "importances_cancer = rf_cancer.feature_importances_\n",
        "std_cancer = np.std([tree.feature_importances_ for tree in rf_cancer.estimators_], axis=0)\n",
        "\n",
        "feature_importance_cancer = pd.DataFrame({\n",
        "    'Feature': cancer.feature_names,\n",
        "    'Importance': importances_cancer,\n",
        "    'Std': std_cancer\n",
        "}).sort_values('Importance', ascending=False).head(15)\n",
        "\n",
        "print(\"Top 15 Features for Cancer Detection\")\n",
        "print(\"=\"*70)\n",
        "print(feature_importance_cancer.to_string(index=False))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.barh(feature_importance_cancer['Feature'], \n",
        "        feature_importance_cancer['Importance'],\n",
        "        xerr=feature_importance_cancer['Std'],\n",
        "        alpha=0.8)\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Top 15 Features: Breast Cancer Detection')\n",
        "plt.grid(alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "permutation-importance",
      "metadata": {},
      "source": [
        "### 6.2 Permutation Importance (Model-Agnostic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "permutation-importance-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Permutation importance\n",
        "perm_importance = permutation_importance(\n",
        "    rf_cancer, X_test_cancer, y_test_cancer,\n",
        "    n_repeats=10,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "perm_importance_df = pd.DataFrame({\n",
        "    'Feature': cancer.feature_names,\n",
        "    'Importance': perm_importance.importances_mean,\n",
        "    'Std': perm_importance.importances_std\n",
        "}).sort_values('Importance', ascending=False).head(15)\n",
        "\n",
        "print(\"Permutation Importance (Top 15)\")\n",
        "print(\"=\"*70)\n",
        "print(perm_importance_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Permutation Importance:\")\n",
        "print(\"  - Shuffle each feature and measure performance drop\")\n",
        "print(\"  - More reliable than impurity-based importance\")\n",
        "print(\"  - Works with any model (model-agnostic)\")\n",
        "print(\"  - Accounts for feature interactions\")\n",
        "\n",
        "# Compare both importance methods\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "# Impurity-based\n",
        "axes[0].barh(feature_importance_cancer['Feature'], \n",
        "            feature_importance_cancer['Importance'],\n",
        "            xerr=feature_importance_cancer['Std'],\n",
        "            alpha=0.8, color='skyblue')\n",
        "axes[0].set_xlabel('Importance')\n",
        "axes[0].set_title('Impurity-Based Feature Importance')\n",
        "axes[0].grid(alpha=0.3, axis='x')\n",
        "\n",
        "# Permutation-based\n",
        "axes[1].barh(perm_importance_df['Feature'], \n",
        "            perm_importance_df['Importance'],\n",
        "            xerr=perm_importance_df['Std'],\n",
        "            alpha=0.8, color='lightcoral')\n",
        "axes[1].set_xlabel('Importance')\n",
        "axes[1].set_title('Permutation-Based Feature Importance')\n",
        "axes[1].grid(alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Differences:\")\n",
        "print(\"  - Impurity: Fast, based on training data splits\")\n",
        "print(\"  - Permutation: Slower, based on actual predictions\")\n",
        "print(\"  - Both provide similar rankings but different magnitudes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Summary and Best Practices\n",
        "\n",
        "### Quick Reference\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "\n",
        "# Classification\n",
        "rf_clf = RandomForestClassifier(\n",
        "    n_estimators=100,       # Number of trees\n",
        "    max_depth=10,          # Tree depth (None = unlimited)\n",
        "    min_samples_split=5,   # Min samples to split\n",
        "    min_samples_leaf=2,    # Min samples per leaf\n",
        "    max_features='sqrt',   # Features per split\n",
        "    oob_score=True,        # Out-of-bag validation\n",
        "    n_jobs=-1,             # Parallel processing\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Regression\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_features=0.33,     # 1/3 of features\n",
        "    oob_score=True,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_clf.fit(X_train, y_train)\n",
        "predictions = rf_clf.predict(X_test)\n",
        "```\n",
        "\n",
        "### Advantages\n",
        "\n",
        "\u2713 **Robust**: Less overfitting than single trees\n",
        "\u2713 **Accurate**: Often best out-of-box performance\n",
        "\u2713 **Feature importance**: Built-in importance ranking\n",
        "\u2713 **Handles missing values**: Can handle NaNs (implementation-dependent)\n",
        "\u2713 **No scaling needed**: Tree-based, scale-invariant\n",
        "\u2713 **Parallel**: Trees trained independently\n",
        "\u2713 **OOB validation**: Built-in validation without separate set\n",
        "\u2713 **Versatile**: Classification and regression\n",
        "\n",
        "### Disadvantages\n",
        "\n",
        "\u274c **Black box**: Less interpretable than single tree\n",
        "\u274c **Memory**: Stores multiple trees\n",
        "\u274c **Slow prediction**: Must query all trees\n",
        "\u274c **Not extrapolate**: Cannot predict beyond training range\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. \u2713 **Start with defaults** (100 trees, sqrt features)\n",
        "2. \u2713 **Use OOB score** for quick validation\n",
        "3. \u2713 **Increase n_estimators** (200-500) if time permits\n",
        "4. \u2713 **Tune max_depth and min_samples** to control complexity\n",
        "5. \u2713 **Use RandomizedSearchCV** (faster than GridSearchCV)\n",
        "6. \u2713 **Check permutation importance** for robust feature ranking\n",
        "7. \u2713 **Enable parallel processing** (n_jobs=-1)\n",
        "8. \u2713 **Monitor OOB vs test** to verify generalization\n",
        "\n",
        "### Key Hyperparameters\n",
        "\n",
        "| Parameter | Classification | Regression | Effect |\n",
        "|-----------|----------------|------------|--------|\n",
        "| `n_estimators` | 100-500 | 100-500 | More trees = better (diminishing returns) |\n",
        "| `max_features` | 'sqrt' | 0.33 (p/3) | Controls diversity |\n",
        "| `max_depth` | 5-20 | 5-20 | Limits tree complexity |\n",
        "| `min_samples_split` | 2-20 | 2-20 | Prevents overfitting |\n",
        "| `min_samples_leaf` | 1-10 | 1-10 | Smooths predictions |\n",
        "| `oob_score` | True | True | Built-in validation |\n",
        "\n",
        "### When to Use Random Forest\n",
        "\n",
        "\u2713 **Good for:**\n",
        "- Tabular data with mixed feature types\n",
        "- Need robust baseline model\n",
        "- Feature importance analysis required\n",
        "- Small to medium datasets\n",
        "- Don't want to tune hyperparameters much\n",
        "- Need uncertainty estimates (via prediction variance)\n",
        "\n",
        "\u2717 **Consider alternatives:**\n",
        "- Very large datasets \u2192 LightGBM, XGBoost (faster)\n",
        "- Need highest accuracy \u2192 Gradient Boosting\n",
        "- Need interpretability \u2192 Single Decision Tree, Linear Models\n",
        "- Text/Image data \u2192 Deep Learning\n",
        "- Time series \u2192 ARIMA, LSTM\n",
        "\n",
        "### Comparison: Random Forest vs Gradient Boosting\n",
        "\n",
        "| Aspect | Random Forest | Gradient Boosting |\n",
        "|--------|---------------|-------------------|\n",
        "| Training | Parallel (fast) | Sequential (slower) |\n",
        "| Overfitting | Resistant | Prone (needs tuning) |\n",
        "| Hyperparameters | Few to tune | Many to tune |\n",
        "| Accuracy | Good | Often better |\n",
        "| Robustness | More robust | Less robust |\n",
        "| Use case | Quick baseline | Competition winning |\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Gradient Boosting (GBM, XGBoost, LightGBM)\n",
        "- SHAP values for detailed interpretation\n",
        "- Handling imbalanced data\n",
        "- Stacking and blending ensembles\n",
        "- Feature engineering for trees"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}