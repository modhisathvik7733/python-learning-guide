{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ecde6a8",
   "metadata": {},
   "source": [
    "# Audio and Advanced Signal Processing\n",
    "- Audio I/O, spectral features, advanced techniques\n",
    "- Real examples: Audio processing, Voice activity detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "318d1b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio processing module loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "print('Audio processing module loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce43f4b",
   "metadata": {},
   "source": [
    "## Audio Signal Basics\n",
    "- **Sample rate**: 44.1 kHz (CD quality), 48 kHz (professional)\n",
    "- **Bit depth**: 16-bit (CD), 24-bit (professional)\n",
    "- **Channels**: Mono (1), Stereo (2), Surround (5.1, 7.1)\n",
    "\n",
    "**Nyquist**: Human hearing ~20 Hz to 20 kHz\n",
    "Sample at 40+ kHz to capture all frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7a7c4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio signal:\n",
      "  Sample rate: 44100 Hz\n",
      "  Duration: 2 s\n",
      "  Samples: 88200\n",
      "  Frequency: 440 Hz (A4 note)\n"
     ]
    }
   ],
   "source": [
    "# Generate audio tone\n",
    "fs = 44100  # CD quality\n",
    "duration = 2\n",
    "t = np.linspace(0, duration, int(fs*duration))\n",
    "\n",
    "# A440 note (concert A)\n",
    "freq = 440\n",
    "audio_tone = np.sin(2*np.pi*freq*t)\n",
    "\n",
    "# Apply envelope (attack-decay)\n",
    "envelope = np.exp(-3*t)\n",
    "audio_tone *= envelope\n",
    "\n",
    "print(f'Audio signal:')\n",
    "print(f'  Sample rate: {fs} Hz')\n",
    "print(f'  Duration: {duration} s')\n",
    "print(f'  Samples: {len(audio_tone)}')\n",
    "print(f'  Frequency: {freq} Hz (A4 note)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90876059",
   "metadata": {},
   "source": [
    "## Spectral Features\n",
    "Extract features for audio analysis/classification\n",
    "\n",
    "**Common features**:\n",
    "- **Spectral centroid**: Brightness (center of mass)\n",
    "- **Spectral rolloff**: Frequency below which X% of energy\n",
    "- **Zero-crossing rate**: Percussiveness, noisiness\n",
    "- **MFCCs**: Mel-frequency cepstral coefficients (speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "066d5aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio features:\n",
      "  Spectral centroid: 605.1 Hz\n",
      "  Zero-crossing rate: 0.0200\n"
     ]
    }
   ],
   "source": [
    "# Spectral centroid\n",
    "def spectral_centroid(sig, fs):\n",
    "    freqs = np.fft.rfftfreq(len(sig), 1/fs)\n",
    "    fft_vals = np.abs(np.fft.rfft(sig))\n",
    "    return np.sum(freqs * fft_vals) / np.sum(fft_vals)\n",
    "\n",
    "# Zero-crossing rate\n",
    "def zero_crossing_rate(sig):\n",
    "    return np.sum(np.abs(np.diff(np.sign(sig)))) / (2 * len(sig))\n",
    "\n",
    "# Calculate for our tone\n",
    "centroid = spectral_centroid(audio_tone, fs)\n",
    "zcr = zero_crossing_rate(audio_tone)\n",
    "\n",
    "print(f'Audio features:')\n",
    "print(f'  Spectral centroid: {centroid:.1f} Hz')\n",
    "print(f'  Zero-crossing rate: {zcr:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36add8cf",
   "metadata": {},
   "source": [
    "## Noise Reduction: Spectral Subtraction\n",
    "Estimate noise spectrum and subtract from signal\n",
    "\n",
    "**Steps**:\n",
    "1. Estimate noise (from silent portion)\n",
    "2. FFT of noisy signal\n",
    "3. Subtract noise spectrum\n",
    "4. Inverse FFT\n",
    "\n",
    "Used in: Voice enhancement, audio cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bb96403",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (8001,) (801,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m noisy_phase \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mangle(noisy_spectrum)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Subtract noise\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m clean_mag \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(noisy_mag \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(noise_spectrum), \u001b[38;5;241m0.1\u001b[39m\u001b[38;5;241m*\u001b[39mnoisy_mag)\n\u001b[1;32m     21\u001b[0m clean_spectrum \u001b[38;5;241m=\u001b[39m clean_mag \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m1\u001b[39mj \u001b[38;5;241m*\u001b[39m noisy_phase)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Inverse FFT\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (8001,) (801,) "
     ]
    }
   ],
   "source": [
    "# Clean speech simulation\n",
    "fs = 16000  # Speech sample rate\n",
    "t = np.linspace(0, 1, fs)\n",
    "speech = np.sin(2*np.pi*300*t) + 0.5*np.sin(2*np.pi*800*t)  # Simple speech\n",
    "\n",
    "# Add noise\n",
    "noise = np.random.randn(len(speech)) * 0.3\n",
    "noisy_speech = speech + noise\n",
    "\n",
    "# Estimate noise spectrum (from first 0.1s)\n",
    "noise_sample = noisy_speech[:int(0.1*fs)]\n",
    "noise_spectrum = np.abs(np.fft.rfft(noise_sample))**2\n",
    "\n",
    "# Process with spectral subtraction (simplified)\n",
    "noisy_spectrum = np.fft.rfft(noisy_speech)\n",
    "noisy_mag = np.abs(noisy_spectrum)\n",
    "noisy_phase = np.angle(noisy_spectrum)\n",
    "\n",
    "# Subtract noise\n",
    "clean_mag = np.maximum(noisy_mag - np.sqrt(noise_spectrum), 0.1*noisy_mag)\n",
    "clean_spectrum = clean_mag * np.exp(1j * noisy_phase)\n",
    "\n",
    "# Inverse FFT\n",
    "clean_speech = np.fft.irfft(clean_spectrum)\n",
    "\n",
    "# Calculate SNR improvement\n",
    "snr_before = 10*np.log10(np.var(speech) / np.var(noise))\n",
    "snr_after = 10*np.log10(np.var(speech) / np.var(clean_speech - speech))\n",
    "\n",
    "print(f'Noise reduction:')\n",
    "print(f'  SNR before: {snr_before:.2f} dB')\n",
    "print(f'  SNR after: {snr_after:.2f} dB')\n",
    "print(f'  Improvement: {snr_after - snr_before:.2f} dB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deef29cd",
   "metadata": {},
   "source": [
    "## Real Example: Voice Activity Detection (VAD)\n",
    "Detect speech vs silence in audio\n",
    "Used in: Speech recognition, teleconferencing, compression\n",
    "\n",
    "**Method**: Energy + zero-crossing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c773d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate audio with speech and silence\n",
    "fs = 16000\n",
    "t_total = np.linspace(0, 5, 5*fs)\n",
    "\n",
    "# Create segments: silence-speech-silence-speech\n",
    "audio_vad = np.zeros(len(t_total))\n",
    "\n",
    "# Speech segments (higher energy, more zero crossings)\n",
    "speech_seg1 = slice(int(0.5*fs), int(1.5*fs))\n",
    "speech_seg2 = slice(int(3.0*fs), int(4.0*fs))\n",
    "\n",
    "audio_vad[speech_seg1] = np.random.randn(len(range(*speech_seg1.indices(len(audio_vad))))) * 0.5\n",
    "audio_vad[speech_seg2] = np.random.randn(len(range(*speech_seg2.indices(len(audio_vad))))) * 0.5\n",
    "\n",
    "# Add background noise everywhere\n",
    "audio_vad += np.random.randn(len(audio_vad)) * 0.05\n",
    "\n",
    "# VAD: Compute energy in frames\n",
    "frame_length = int(0.025 * fs)  # 25ms frames\n",
    "step = int(0.010 * fs)  # 10ms step\n",
    "\n",
    "energy = []\n",
    "for i in range(0, len(audio_vad) - frame_length, step):\n",
    "    frame = audio_vad[i:i+frame_length]\n",
    "    energy.append(np.sum(frame**2))\n",
    "\n",
    "energy = np.array(energy)\n",
    "\n",
    "# Threshold for voice detection\n",
    "threshold = np.percentile(energy, 75)\n",
    "voice_detected = energy > threshold\n",
    "\n",
    "print(f'Voice Activity Detection:')\n",
    "print(f'  Total frames: {len(energy)}')\n",
    "print(f'  Voice frames: {voice_detected.sum()}')\n",
    "print(f'  Silence frames: {(~voice_detected).sum()}')\n",
    "print(f'  Voice activity: {voice_detected.sum()/len(voice_detected)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6e30fd",
   "metadata": {},
   "source": [
    "## Advanced: Spectrogram Manipulation\n",
    "Process signals in time-frequency domain\n",
    "\n",
    "**Applications**:\n",
    "- Time stretching (change speed without pitch)\n",
    "- Pitch shifting (change pitch without speed)\n",
    "- Source separation\n",
    "- Audio effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbefc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate chirp\n",
    "fs = 8000\n",
    "t = np.linspace(0, 2, 2*fs)\n",
    "chirp_audio = signal.chirp(t, f0=200, f1=2000, t1=2)\n",
    "\n",
    "# STFT\n",
    "f, t_spec, Zxx = signal.stft(chirp_audio, fs=fs, nperseg=256)\n",
    "\n",
    "print(f'STFT analysis:')\n",
    "print(f'  Frequency bins: {len(f)}')\n",
    "print(f'  Time frames: {len(t_spec)}')\n",
    "print(f'  Spectrogram shape: {Zxx.shape}')\n",
    "\n",
    "# Manipulate: Pitch shift up by multiplying frequencies\n",
    "# (Simplified - real pitch shifting is more complex)\n",
    "Zxx_shifted = np.vstack([Zxx[::2, :], np.zeros((Zxx.shape[0]//2, Zxx.shape[1]))])\n",
    "\n",
    "# Inverse STFT\n",
    "_, audio_shifted = signal.istft(Zxx_shifted, fs=fs)\n",
    "\n",
    "print(f'\n",
    "Pitch shift applied (simplified version)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27d64a3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Audio Processing Pipeline:\n",
    "1. **Load audio**: Read file, resample if needed\n",
    "2. **Preprocess**: Normalize, remove DC, pre-emphasis\n",
    "3. **Feature extraction**: STFT, MFCCs, spectral features\n",
    "4. **Processing**: Filtering, enhancement, effects\n",
    "5. **Analysis**: Classification, detection, recognition\n",
    "\n",
    "### Key Techniques:\n",
    "\n",
    "**Time domain**:\n",
    "- Amplitude envelope\n",
    "- Zero-crossing rate\n",
    "- Energy/RMS\n",
    "\n",
    "**Frequency domain**:\n",
    "- FFT spectrum\n",
    "- Spectral centroid\n",
    "- Spectral rolloff\n",
    "- Spectral flux\n",
    "\n",
    "**Time-frequency**:\n",
    "- STFT/Spectrogram\n",
    "- Mel-spectrogram\n",
    "- Wavelet transform\n",
    "\n",
    "### Applications:\n",
    "\n",
    "**Speech**:\n",
    "- Voice activity detection\n",
    "- Speech recognition\n",
    "- Speaker identification\n",
    "- Noise reduction\n",
    "\n",
    "**Music**:\n",
    "- Beat tracking\n",
    "- Tempo estimation\n",
    "- Chord recognition\n",
    "- Genre classification\n",
    "\n",
    "**General**:\n",
    "- Audio compression\n",
    "- Sound effects\n",
    "- Acoustic analysis\n",
    "- Environmental sound classification\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "✓ **Sample rate**: 44.1/48 kHz for audio, 16 kHz for speech\n",
    "✓ **Framing**: 20-40ms windows, 50% overlap\n",
    "✓ **Windowing**: Hann/Hamming for STFT\n",
    "✓ **Normalize**: Scale audio to [-1, 1]\n",
    "✓ **Pre-emphasis**: Boost high frequencies for speech\n",
    "\n",
    "### Advanced Topics:\n",
    "- Deep learning (neural audio processing)\n",
    "- Source separation (isolate instruments)\n",
    "- Audio synthesis (generate sounds)\n",
    "- Spatial audio (3D sound)\n",
    "- Real-time processing (low latency)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
