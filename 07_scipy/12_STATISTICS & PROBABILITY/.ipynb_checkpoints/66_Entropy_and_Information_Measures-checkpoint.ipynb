{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["# Entropy and Information Measures\n- Shannon entropy, Mutual information, KL divergence\n- Real examples: Feature selection, Compression"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["import numpy as np\nfrom scipy import stats\nfrom scipy.special import rel_entr\nprint('Entropy and information module loaded')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Shannon Entropy\n**Definition**: H(X) = -Σ p(x) log₂ p(x)\n**Interpretation**: Average information content"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Entropy calculation\nprobs1 = [0.5, 0.5]  # Fair coin\nprobs2 = [0.9, 0.1]  # Biased coin\nprobs3 = [0.25, 0.25, 0.25, 0.25]  # Uniform\n\nentropy1 = stats.entropy(probs1, base=2)\nentropy2 = stats.entropy(probs2, base=2)\nentropy3 = stats.entropy(probs3, base=2)\n\nprint('Shannon Entropy (bits)\\n')\nprint(f'Fair coin [0.5, 0.5]: H = {entropy1:.4f}')\nprint(f'Biased coin [0.9, 0.1]: H = {entropy2:.4f}')\nprint(f'4-sided die [0.25, ...]: H = {entropy3:.4f}\\n')\nprint('Higher entropy = more uncertainty')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Kullback-Leibler Divergence\n**Definition**: KL(P||Q) = Σ p(x) log(p(x)/q(x))\n**Use**: Measure distance between distributions"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["p = np.array([0.4, 0.3, 0.3])\nq1 = np.array([0.35, 0.35, 0.3])  # Close\nq2 = np.array([0.1, 0.1, 0.8])   # Far\n\nkl1 = stats.entropy(p, q1)\nkl2 = stats.entropy(p, q2)\n\nprint('KL Divergence\\n')\nprint(f'P: {p}')\nprint(f'Q1: {q1} → KL(P||Q1) = {kl1:.4f}')\nprint(f'Q2: {q2} → KL(P||Q2) = {kl2:.4f}\\n')\nprint('Larger KL = more different')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Real Example: Text Compression\n**Scenario**: Estimate compression potential"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["print('Text Compression Analysis\\n')\ntext = 'the quick brown fox jumps over the lazy dog'\n\n# Character frequency\nchars, counts = np.unique(list(text), return_counts=True)\nprobs = counts / counts.sum()\n\n# Entropy\nentropy = stats.entropy(probs, base=2)\nprint(f'Text: \"{text}\"')\nprint(f'Unique chars: {len(chars)}')\nprint(f'Entropy: {entropy:.4f} bits/char\\n')\n\n# Compression potential\nmax_entropy = np.log2(len(chars))\ncompression_ratio = entropy / max_entropy\nprint(f'Max entropy (uniform): {max_entropy:.4f} bits/char')\nprint(f'Compression ratio: {compression_ratio:.2%}')\nprint(f'Potential savings: {(1-compression_ratio)*100:.1f}%')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Summary\n```python\n# Entropy\nH = stats.entropy(probs, base=2)\n\n# KL Divergence\nKL = stats.entropy(p, q)\n\n# Mutual information\n# Use sklearn.feature_selection.mutual_info_*\n```"]
    }
  ],
  "metadata": {
    "kernelspec": {"display_name": "Python 3", "name": "python3"},
    "language_info": {"version": "3.8.0"}
  },
  "nbformat": 4,
  "nbformat_minor": 5
}