{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Univariate Optimization (1D)\n",
        "- **Purpose**: Find minimum or maximum of single-variable functions\n",
        "- **scipy.optimize**: minimize_scalar, brent, golden, bracket\n",
        "- **Methods**: Brent, Golden Section, Bounded\n",
        "\n",
        "Key concepts:\n",
        "- **Univariate**: Function of one variable f(x)\n",
        "- **Local minimum**: f(x*) \u2264 f(x) for nearby x\n",
        "- **Global minimum**: f(x*) \u2264 f(x) for all x\n",
        "- **Bracketing**: Find interval containing minimum\n",
        "\n",
        "Real applications:\n",
        "- Hyperparameter tuning (learning rate, regularization)\n",
        "- Line search in optimization algorithms\n",
        "- Cost minimization (pricing, inventory)\n",
        "- Physics: projectile range, optimal angle\n",
        "- Economics: profit maximization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import optimize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.set_printoptions(precision=4, suppress=True)\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "\n",
        "print(\"Univariate optimization module loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Example: Parabola\n",
        "\n",
        "**Simple function**: \\( f(x) = (x-3)^2 + 1 \\)\n",
        "\n",
        "**Analytical minimum**: x = 3, f(3) = 1\n",
        "\n",
        "**scipy.optimize.minimize_scalar()**: Main function for 1D optimization\n",
        "\n",
        "**Syntax**:\n",
        "```python\n",
        "result = optimize.minimize_scalar(f)\n",
        "result.x      # Optimal x\n",
        "result.fun    # Minimum value f(x)\n",
        "result.nfev   # Number of function evaluations\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define function\n",
        "def f(x):\n",
        "    return (x - 3)**2 + 1\n",
        "\n",
        "# Minimize\n",
        "result = optimize.minimize_scalar(f)\n",
        "\n",
        "print(\"Minimize f(x) = (x-3)\u00b2 + 1\")\n",
        "print(\"\\nOptimization result:\")\n",
        "print(f\"  Minimum at x = {result.x:.6f}\")\n",
        "print(f\"  Minimum value f(x) = {result.fun:.6f}\")\n",
        "print(f\"  Function evaluations: {result.nfev}\")\n",
        "print(f\"  Method: {result.method}\")\n",
        "print(f\"\\nAnalytical solution: x = 3, f(3) = 1\")\n",
        "print(f\"Error: {abs(result.x - 3):.2e}\")\n",
        "\n",
        "# Visualize\n",
        "x = np.linspace(0, 6, 300)\n",
        "y = f(x)\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.plot(x, y, 'b-', linewidth=2, label='f(x) = (x-3)\u00b2 + 1')\n",
        "plt.plot(result.x, result.fun, 'ro', markersize=15, \n",
        "         label=f'Minimum: ({result.x:.4f}, {result.fun:.4f})')\n",
        "plt.xlabel('x', fontsize=13)\n",
        "plt.ylabel('f(x)', fontsize=13)\n",
        "plt.title('Univariate Optimization: Simple Parabola', fontsize=15)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nOptimization successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bounded Optimization\n",
        "\n",
        "**Problem**: Find minimum within bounds [a, b]\n",
        "\n",
        "**Use case**: Physical constraints, search regions\n",
        "\n",
        "**Syntax**:\n",
        "```python\n",
        "result = optimize.minimize_scalar(f, bounds=(a, b), method='bounded')\n",
        "```\n",
        "\n",
        "**Methods**:\n",
        "- **'bounded'**: Brent method with bounds\n",
        "- Guaranteed to converge within bounds\n",
        "- Handles boundary minima"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function with minimum outside natural range\n",
        "def f(x):\n",
        "    return x**2 - 10*x + 25  # Minimum at x=5\n",
        "\n",
        "# Case 1: Minimum inside bounds\n",
        "result1 = optimize.minimize_scalar(f, bounds=(0, 10), method='bounded')\n",
        "\n",
        "# Case 2: Minimum outside bounds (at boundary)\n",
        "result2 = optimize.minimize_scalar(f, bounds=(6, 10), method='bounded')\n",
        "\n",
        "print(\"Bounded Optimization: f(x) = x\u00b2 - 10x + 25\")\n",
        "print(\"Analytical minimum: x = 5, f(5) = 0\\n\")\n",
        "\n",
        "print(\"Case 1: Bounds [0, 10] (contains minimum)\")\n",
        "print(f\"  Optimal x = {result1.x:.6f}\")\n",
        "print(f\"  Minimum f(x) = {result1.fun:.6f}\")\n",
        "\n",
        "print(\"\\nCase 2: Bounds [6, 10] (minimum outside)\")\n",
        "print(f\"  Optimal x = {result2.x:.6f} (boundary!)\")\n",
        "print(f\"  Minimum f(x) = {result2.fun:.6f}\")\n",
        "\n",
        "# Visualize\n",
        "x = np.linspace(0, 10, 300)\n",
        "y = f(x)\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.plot(x, y, 'b-', linewidth=2, label='f(x) = x\u00b2 - 10x + 25')\n",
        "plt.axvline(5, color='gray', linestyle='--', alpha=0.5, label='True minimum (x=5)')\n",
        "plt.plot(result1.x, result1.fun, 'go', markersize=15, \n",
        "         label=f'Case 1: x={result1.x:.2f} (interior)')\n",
        "plt.plot(result2.x, result2.fun, 'ro', markersize=15, \n",
        "         label=f'Case 2: x={result2.x:.2f} (boundary)')\n",
        "plt.axvspan(6, 10, alpha=0.2, color='red', label='Case 2 bounds')\n",
        "plt.xlabel('x', fontsize=13)\n",
        "plt.ylabel('f(x)', fontsize=13)\n",
        "plt.title('Bounded Optimization: Interior vs Boundary Minimum', fontsize=15)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real Example: Optimal Pricing\n",
        "\n",
        "**Business problem**: Maximize profit by finding optimal price\n",
        "\n",
        "**Model**:\n",
        "- **Demand**: Q(p) = 1000 - 20p (decreases with price)\n",
        "- **Cost**: C = 10 per unit\n",
        "- **Revenue**: R(p) = p \u00d7 Q(p)\n",
        "- **Profit**: \u03a0(p) = R(p) - C \u00d7 Q(p)\n",
        "\n",
        "**Goal**: Find price p* that maximizes profit\n",
        "\n",
        "**Note**: Maximize f(x) = Minimize -f(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Profit function\n",
        "def profit(price):\n",
        "    demand = 1000 - 20*price\n",
        "    if demand <= 0:\n",
        "        return -1e6  # Invalid price\n",
        "    cost_per_unit = 10\n",
        "    revenue = price * demand\n",
        "    cost = cost_per_unit * demand\n",
        "    return revenue - cost\n",
        "\n",
        "# Maximize profit = minimize negative profit\n",
        "def neg_profit(price):\n",
        "    return -profit(price)\n",
        "\n",
        "# Find optimal price (must be positive, max reasonable price 50)\n",
        "result = optimize.minimize_scalar(neg_profit, bounds=(10, 50), method='bounded')\n",
        "\n",
        "optimal_price = result.x\n",
        "max_profit = -result.fun\n",
        "optimal_demand = 1000 - 20*optimal_price\n",
        "revenue = optimal_price * optimal_demand\n",
        "cost = 10 * optimal_demand\n",
        "\n",
        "print(\"Optimal Pricing Problem\")\n",
        "print(\"  Demand: Q(p) = 1000 - 20p\")\n",
        "print(\"  Cost: $10 per unit\")\n",
        "print(\"  Profit: \u03a0(p) = p\u00b7Q(p) - 10\u00b7Q(p)\\n\")\n",
        "\n",
        "print(\"Optimal solution:\")\n",
        "print(f\"  Price: ${optimal_price:.2f}\")\n",
        "print(f\"  Demand: {optimal_demand:.0f} units\")\n",
        "print(f\"  Revenue: ${revenue:.2f}\")\n",
        "print(f\"  Cost: ${cost:.2f}\")\n",
        "print(f\"  Profit: ${max_profit:.2f}\")\n",
        "\n",
        "# Analytical solution: d\u03a0/dp = 0\n",
        "# \u03a0(p) = p(1000-20p) - 10(1000-20p) = 1000p - 20p\u00b2 - 10000 + 200p\n",
        "# \u03a0(p) = -20p\u00b2 + 1200p - 10000\n",
        "# d\u03a0/dp = -40p + 1200 = 0 \u2192 p = 30\n",
        "analytical_price = 30\n",
        "print(f\"\\nAnalytical solution: p* = ${analytical_price}\")\n",
        "print(f\"Error: ${abs(optimal_price - analytical_price):.2e}\")\n",
        "\n",
        "# Visualize\n",
        "prices = np.linspace(10, 50, 300)\n",
        "profits = [profit(p) for p in prices]\n",
        "demands = [1000 - 20*p for p in prices]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "# Profit curve\n",
        "axes[0].plot(prices, profits, 'b-', linewidth=2, label='Profit \u03a0(p)')\n",
        "axes[0].plot(optimal_price, max_profit, 'ro', markersize=15, \n",
        "             label=f'Optimal: p=${optimal_price:.2f}, \u03a0=${max_profit:.2f}')\n",
        "axes[0].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
        "axes[0].set_xlabel('Price ($)', fontsize=13)\n",
        "axes[0].set_ylabel('Profit ($)', fontsize=13)\n",
        "axes[0].set_title('Profit vs Price', fontsize=14)\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Demand curve\n",
        "axes[1].plot(prices, demands, 'g-', linewidth=2, label='Demand Q(p)')\n",
        "axes[1].plot(optimal_price, optimal_demand, 'ro', markersize=15,\n",
        "             label=f'Optimal: Q={optimal_demand:.0f} at p=${optimal_price:.2f}')\n",
        "axes[1].set_xlabel('Price ($)', fontsize=13)\n",
        "axes[1].set_ylabel('Demand (units)', fontsize=13)\n",
        "axes[1].set_title('Demand vs Price', fontsize=14)\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nOptimal price balances revenue and demand!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real Example: Machine Learning - Optimal Learning Rate\n",
        "\n",
        "**Problem**: Find optimal learning rate for gradient descent\n",
        "\n",
        "**Model**: Simple quadratic loss\n",
        "\\[ L(\\theta) = (\\theta - 5)^2 \\]\n",
        "\n",
        "**Gradient descent**: \\( \\theta_{t+1} = \\theta_t - \\alpha \\nabla L(\\theta_t) \\)\n",
        "\n",
        "**Goal**: Find learning rate \u03b1 that minimizes iterations to converge\n",
        "\n",
        "**Consideration**:\n",
        "- Too small \u03b1 \u2192 slow convergence\n",
        "- Too large \u03b1 \u2192 oscillation or divergence\n",
        "- Optimal \u03b1 \u2192 fastest convergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate gradient descent with given learning rate\n",
        "def gradient_descent_iterations(learning_rate, max_iters=1000, tol=1e-6):\n",
        "    \"\"\"Count iterations to converge for given learning rate\"\"\"\n",
        "    theta = 0.0  # Initial parameter\n",
        "    target = 5.0  # True minimum\n",
        "    \n",
        "    for i in range(max_iters):\n",
        "        # Gradient of (theta - 5)^2 is 2(theta - 5)\n",
        "        grad = 2 * (theta - target)\n",
        "        theta = theta - learning_rate * grad\n",
        "        \n",
        "        # Check convergence\n",
        "        if abs(theta - target) < tol:\n",
        "            return i + 1\n",
        "    \n",
        "    return max_iters  # Did not converge\n",
        "\n",
        "# Objective: minimize iterations (or maximize negative iterations)\n",
        "def objective(lr):\n",
        "    iters = gradient_descent_iterations(lr)\n",
        "    return iters\n",
        "\n",
        "# Find optimal learning rate\n",
        "result = optimize.minimize_scalar(objective, bounds=(0.01, 1.0), method='bounded')\n",
        "\n",
        "optimal_lr = result.x\n",
        "min_iters = int(result.fun)\n",
        "\n",
        "print(\"Optimal Learning Rate for Gradient Descent\")\n",
        "print(\"  Loss: L(\u03b8) = (\u03b8 - 5)\u00b2\")\n",
        "print(\"  Update: \u03b8 \u2190 \u03b8 - \u03b1\u00b7\u2207L(\u03b8)\")\n",
        "print(\"  Goal: Minimize iterations to convergence\\n\")\n",
        "\n",
        "print(\"Optimization result:\")\n",
        "print(f\"  Optimal learning rate: \u03b1 = {optimal_lr:.4f}\")\n",
        "print(f\"  Iterations to converge: {min_iters}\")\n",
        "\n",
        "# Compare with other learning rates\n",
        "test_lrs = [0.1, 0.5, optimal_lr, 0.9]\n",
        "print(\"\\nComparison:\")\n",
        "for lr in test_lrs:\n",
        "    iters = gradient_descent_iterations(lr)\n",
        "    print(f\"  \u03b1 = {lr:.4f} \u2192 {iters} iterations\")\n",
        "\n",
        "# Visualize\n",
        "learning_rates = np.linspace(0.01, 1.0, 100)\n",
        "iterations = [gradient_descent_iterations(lr) for lr in learning_rates]\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.plot(learning_rates, iterations, 'b-', linewidth=2, \n",
        "         label='Iterations to converge')\n",
        "plt.plot(optimal_lr, min_iters, 'ro', markersize=15,\n",
        "         label=f'Optimal: \u03b1={optimal_lr:.4f}, iters={min_iters}')\n",
        "plt.xlabel('Learning Rate (\u03b1)', fontsize=13)\n",
        "plt.ylabel('Iterations to Converge', fontsize=13)\n",
        "plt.title('Finding Optimal Learning Rate', fontsize=15)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nOptimal \u03b1 = {optimal_lr:.4f} converges fastest!\")\n",
        "print(\"Too small \u2192 slow, too large \u2192 oscillation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Methods Comparison\n",
        "\n",
        "**scipy.optimize.minimize_scalar()** supports multiple methods:\n",
        "\n",
        "| Method | Description | Use When |\n",
        "|--------|-------------|----------|\n",
        "| **'brent'** | Brent's method (default) | No bounds, smooth function |\n",
        "| **'bounded'** | Bounded Brent | Constraints on x |\n",
        "| **'golden'** | Golden section search | Derivative-free, simple |\n",
        "\n",
        "**Brent's method**:\n",
        "- Combines golden section + parabolic interpolation\n",
        "- Fast convergence (superlinear)\n",
        "- Default choice for most problems\n",
        "\n",
        "**Golden section**:\n",
        "- Uses golden ratio \u03c6 = (1+\u221a5)/2 \u2248 1.618\n",
        "- Guaranteed convergence\n",
        "- Slower but more robust\n",
        "\n",
        "**Bounded**:\n",
        "- Respects bounds strictly\n",
        "- Handles boundary minima\n",
        "- Essential for constrained problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare methods on challenging function\n",
        "def f(x):\n",
        "    return np.sin(x) + np.sin(10*x/3) + np.log(x) - 0.84*x + 3\n",
        "\n",
        "print(\"Method Comparison\")\n",
        "print(\"Function: f(x) = sin(x) + sin(10x/3) + log(x) - 0.84x + 3\")\n",
        "print(\"Search interval: [2.7, 7.5]\\n\")\n",
        "\n",
        "methods = ['brent', 'golden', 'bounded']\n",
        "results = {}\n",
        "\n",
        "for method in methods:\n",
        "    if method == 'bounded':\n",
        "        result = optimize.minimize_scalar(f, bounds=(2.7, 7.5), method=method)\n",
        "    else:\n",
        "        result = optimize.minimize_scalar(f, bracket=(2.7, 7.5), method=method)\n",
        "    \n",
        "    results[method] = result\n",
        "    print(f\"{method.upper()}:\")\n",
        "    print(f\"  Minimum at x = {result.x:.6f}\")\n",
        "    print(f\"  Function value = {result.fun:.6f}\")\n",
        "    print(f\"  Function evaluations: {result.nfev}\")\n",
        "    print()\n",
        "\n",
        "# Visualize\n",
        "x = np.linspace(2.7, 7.5, 500)\n",
        "y = [f(xi) for xi in x]\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.plot(x, y, 'b-', linewidth=2, label='f(x)', alpha=0.7)\n",
        "\n",
        "colors = {'brent': 'red', 'golden': 'green', 'bounded': 'orange'}\n",
        "for method, result in results.items():\n",
        "    plt.plot(result.x, result.fun, 'o', color=colors[method], markersize=12,\n",
        "             label=f'{method}: x={result.x:.4f}')\n",
        "\n",
        "plt.xlabel('x', fontsize=13)\n",
        "plt.ylabel('f(x)', fontsize=13)\n",
        "plt.title('Method Comparison: All converge to same minimum', fontsize=15)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"All methods find the same minimum!\")\n",
        "print(\"Brent is fastest (fewest function evaluations)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Functions:\n",
        "\n",
        "```python\n",
        "# Main function\n",
        "result = optimize.minimize_scalar(f)\n",
        "\n",
        "# With bounds\n",
        "result = optimize.minimize_scalar(f, bounds=(a, b), method='bounded')\n",
        "\n",
        "# With bracket (initial guess)\n",
        "result = optimize.minimize_scalar(f, bracket=(a, b), method='brent')\n",
        "\n",
        "# Access results\n",
        "result.x       # Optimal point\n",
        "result.fun     # Minimum value\n",
        "result.nfev    # Function evaluations\n",
        "result.success # Convergence flag\n",
        "```\n",
        "\n",
        "### When to Use:\n",
        "\n",
        "\u2713 **Single variable** optimization  \n",
        "\u2713 **Line search** in multi-dimensional algorithms  \n",
        "\u2713 **Hyperparameter tuning** (learning rate, regularization)  \n",
        "\u2713 **Physics/engineering** problems  \n",
        "\u2713 **Economics** (pricing, inventory)  \n",
        "\n",
        "### Best Practices:\n",
        "\n",
        "1. **Use bounds** when possible \u2192 faster, more reliable\n",
        "2. **Brent method** (default) is best for most problems\n",
        "3. **Golden section** for noisy/discontinuous functions\n",
        "4. **Bracket** helps if you know approximate location\n",
        "5. **Maximize f**: minimize -f\n",
        "\n",
        "### Limitations:\n",
        "\n",
        "\u26a0\ufe0f Finds **local** minimum (not global)  \n",
        "\u26a0\ufe0f Requires **unimodal** function in search region  \n",
        "\u26a0\ufe0f For multiple minima \u2192 use global optimization  \n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- **Multivariate**: optimize.minimize() for f(x\u2081, x\u2082, ...)\n",
        "- **Constrained**: Subject to g(x) \u2264 0\n",
        "- **Global**: Find global minimum among many local minima"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}