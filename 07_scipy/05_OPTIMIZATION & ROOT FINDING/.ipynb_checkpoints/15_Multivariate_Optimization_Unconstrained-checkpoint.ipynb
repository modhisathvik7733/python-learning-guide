{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multivariate Optimization - Unconstrained\n",
        "- **Purpose**: Minimize functions of multiple variables f(x\u2081, x\u2082, ..., x\u2099)\n",
        "- **scipy.optimize.minimize()**: Main function for multivariate optimization\n",
        "- **Methods**: BFGS, L-BFGS-B, Nelder-Mead, CG, Newton-CG\n",
        "\n",
        "Key concepts:\n",
        "- **Gradient**: \u2207f = [\u2202f/\u2202x\u2081, \u2202f/\u2202x\u2082, ...] (direction of steepest ascent)\n",
        "- **Hessian**: Matrix of second derivatives\n",
        "- **Quasi-Newton**: Approximate Hessian (BFGS, L-BFGS)\n",
        "- **Line search**: Find step size along direction\n",
        "\n",
        "Real applications:\n",
        "- **Machine Learning**: Training models, loss minimization\n",
        "- **Neural Networks**: Backpropagation optimization\n",
        "- **Portfolio optimization**: Asset allocation\n",
        "- **Parameter estimation**: Curve fitting, regression\n",
        "- **Physics**: Energy minimization, equilibrium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import optimize\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "np.set_printoptions(precision=4, suppress=True)\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "\n",
        "print(\"Multivariate optimization module loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Example: 2D Paraboloid\n",
        "\n",
        "**Function**: \\( f(x, y) = (x-3)^2 + (y+1)^2 \\)\n",
        "\n",
        "**Analytical minimum**: (x*, y*) = (3, -1), f = 0\n",
        "\n",
        "**optimize.minimize()** syntax:\n",
        "```python\n",
        "result = optimize.minimize(f, x0, method='BFGS')\n",
        "```\n",
        "\n",
        "Where:\n",
        "- `f`: Objective function f(x) where x is array\n",
        "- `x0`: Initial guess [x0, y0]\n",
        "- `method`: Optimization algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define objective function\n",
        "def f(x):\n",
        "    return (x[0] - 3)**2 + (x[1] + 1)**2\n",
        "\n",
        "# Initial guess\n",
        "x0 = np.array([0.0, 0.0])\n",
        "\n",
        "# Optimize\n",
        "result = optimize.minimize(f, x0, method='BFGS')\n",
        "\n",
        "print(\"Minimize f(x,y) = (x-3)\u00b2 + (y+1)\u00b2\")\n",
        "print(\"\\nOptimization result:\")\n",
        "print(f\"  Minimum at: x = {result.x}\")\n",
        "print(f\"  Minimum value: f = {result.fun:.6f}\")\n",
        "print(f\"  Function evaluations: {result.nfev}\")\n",
        "print(f\"  Success: {result.success}\")\n",
        "print(f\"\\nAnalytical solution: (3, -1)\")\n",
        "print(f\"Error: {np.linalg.norm(result.x - np.array([3, -1])):.2e}\")\n",
        "\n",
        "# Visualize\n",
        "x = np.linspace(-1, 6, 100)\n",
        "y = np.linspace(-4, 2, 100)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = (X - 3)**2 + (Y + 1)**2\n",
        "\n",
        "fig = plt.figure(figsize=(16, 7))\n",
        "\n",
        "# 3D surface\n",
        "ax1 = fig.add_subplot(121, projection='3d')\n",
        "surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
        "ax1.plot([result.x[0]], [result.x[1]], [result.fun], 'ro', markersize=15,\n",
        "         label='Minimum')\n",
        "ax1.set_xlabel('x', fontsize=12)\n",
        "ax1.set_ylabel('y', fontsize=12)\n",
        "ax1.set_zlabel('f(x,y)', fontsize=12)\n",
        "ax1.set_title('3D Surface', fontsize=14)\n",
        "\n",
        "# Contour plot\n",
        "ax2 = fig.add_subplot(122)\n",
        "contour = ax2.contour(X, Y, Z, levels=20, cmap='viridis')\n",
        "ax2.plot(result.x[0], result.x[1], 'ro', markersize=15, \n",
        "         label=f'Minimum: ({result.x[0]:.3f}, {result.x[1]:.3f})')\n",
        "ax2.plot(x0[0], x0[1], 'gx', markersize=15, markeredgewidth=3,\n",
        "         label=f'Initial: ({x0[0]}, {x0[1]})')\n",
        "ax2.set_xlabel('x', fontsize=12)\n",
        "ax2.set_ylabel('y', fontsize=12)\n",
        "ax2.set_title('Contour Plot', fontsize=14)\n",
        "ax2.legend(fontsize=11)\n",
        "plt.colorbar(contour, ax=ax2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nBFGS successfully found the minimum!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Providing Gradient (Jacobian)\n",
        "\n",
        "**Why provide gradient?**\n",
        "- Faster convergence (fewer iterations)\n",
        "- More accurate optimization\n",
        "- Some methods require it\n",
        "\n",
        "**Gradient**: \\( \\nabla f = \\begin{bmatrix} \\partial f/\\partial x_1 \\\\ \\partial f/\\partial x_2 \\\\ \\vdots \\end{bmatrix} \\)\n",
        "\n",
        "**Syntax**:\n",
        "```python\n",
        "def gradient(x):\n",
        "    return np.array([df_dx1, df_dx2, ...])\n",
        "\n",
        "result = optimize.minimize(f, x0, jac=gradient, method='BFGS')\n",
        "```\n",
        "\n",
        "**Auto-differentiation**: Use `jac='2-point'` or `'3-point'` for numerical gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rosenbrock function: classic test case\n",
        "def rosenbrock(x):\n",
        "    \"\"\"f(x,y) = (1-x)\u00b2 + 100(y-x\u00b2)\u00b2\"\"\"\n",
        "    return (1 - x[0])**2 + 100*(x[1] - x[0]**2)**2\n",
        "\n",
        "def rosenbrock_grad(x):\n",
        "    \"\"\"Analytical gradient\"\"\"\n",
        "    df_dx = -2*(1 - x[0]) - 400*x[0]*(x[1] - x[0]**2)\n",
        "    df_dy = 200*(x[1] - x[0]**2)\n",
        "    return np.array([df_dx, df_dy])\n",
        "\n",
        "x0 = np.array([0.0, 0.0])\n",
        "\n",
        "print(\"Rosenbrock Function: (1-x)\u00b2 + 100(y-x\u00b2)\u00b2\")\n",
        "print(\"Known minimum: (1, 1), f = 0\\n\")\n",
        "\n",
        "# Case 1: Without gradient (numerical)\n",
        "result1 = optimize.minimize(rosenbrock, x0, method='BFGS')\n",
        "\n",
        "# Case 2: With analytical gradient\n",
        "result2 = optimize.minimize(rosenbrock, x0, jac=rosenbrock_grad, method='BFGS')\n",
        "\n",
        "print(\"WITHOUT gradient (numerical derivatives):\")\n",
        "print(f\"  Minimum: {result1.x}\")\n",
        "print(f\"  f(x): {result1.fun:.8f}\")\n",
        "print(f\"  Function evaluations: {result1.nfev}\")\n",
        "print(f\"  Iterations: {result1.nit}\")\n",
        "\n",
        "print(\"\\nWITH analytical gradient:\")\n",
        "print(f\"  Minimum: {result2.x}\")\n",
        "print(f\"  f(x): {result2.fun:.8f}\")\n",
        "print(f\"  Function evaluations: {result2.nfev}\")\n",
        "print(f\"  Iterations: {result2.nit}\")\n",
        "\n",
        "print(f\"\\nImprovement: {result1.nfev/result2.nfev:.1f}x fewer evaluations!\")\n",
        "\n",
        "# Visualize Rosenbrock (banana-shaped valley)\n",
        "x = np.linspace(-2, 2, 200)\n",
        "y = np.linspace(-1, 3, 200)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = (1 - X)**2 + 100*(Y - X**2)**2\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "# Use log scale for better visualization\n",
        "contour = plt.contour(X, Y, np.log10(Z + 1), levels=30, cmap='viridis')\n",
        "plt.plot(1, 1, 'r*', markersize=20, label='True minimum (1,1)')\n",
        "plt.plot(result2.x[0], result2.x[1], 'go', markersize=12, \n",
        "         label=f'Found: ({result2.x[0]:.3f}, {result2.x[1]:.3f})')\n",
        "plt.plot(x0[0], x0[1], 'kx', markersize=15, markeredgewidth=3,\n",
        "         label='Initial (0,0)')\n",
        "plt.xlabel('x', fontsize=13)\n",
        "plt.ylabel('y', fontsize=13)\n",
        "plt.title('Rosenbrock Function (log scale): Banana-shaped valley', fontsize=14)\n",
        "plt.legend(fontsize=12)\n",
        "plt.colorbar(contour, label='log\u2081\u2080(f + 1)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nProviding gradient significantly improves performance!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real Example: Linear Regression\n",
        "\n",
        "**Problem**: Fit line y = mx + b to minimize squared error\n",
        "\n",
        "**Loss function**: \\( L(m, b) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - mx_i - b)^2 \\)\n",
        "\n",
        "**Gradient**:\n",
        "\\[ \\frac{\\partial L}{\\partial m} = -\\frac{2}{n}\\sum x_i(y_i - mx_i - b) \\]\n",
        "\\[ \\frac{\\partial L}{\\partial b} = -\\frac{2}{n}\\sum (y_i - mx_i - b) \\]\n",
        "\n",
        "**Goal**: Find optimal parameters [m, b]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "n = 50\n",
        "x_data = np.linspace(0, 10, n)\n",
        "true_m, true_b = 2.5, 1.0\n",
        "y_data = true_m * x_data + true_b + np.random.randn(n) * 2  # Add noise\n",
        "\n",
        "# Mean Squared Error loss\n",
        "def mse_loss(params):\n",
        "    m, b = params\n",
        "    predictions = m * x_data + b\n",
        "    return np.mean((y_data - predictions)**2)\n",
        "\n",
        "# Gradient\n",
        "def mse_gradient(params):\n",
        "    m, b = params\n",
        "    predictions = m * x_data + b\n",
        "    errors = y_data - predictions\n",
        "    dm = -2 * np.mean(x_data * errors)\n",
        "    db = -2 * np.mean(errors)\n",
        "    return np.array([dm, db])\n",
        "\n",
        "# Initial guess\n",
        "params0 = np.array([0.0, 0.0])\n",
        "\n",
        "# Optimize\n",
        "result = optimize.minimize(mse_loss, params0, jac=mse_gradient, method='BFGS')\n",
        "\n",
        "m_opt, b_opt = result.x\n",
        "mse_opt = result.fun\n",
        "\n",
        "print(\"Linear Regression via Optimization\")\n",
        "print(f\"  Data: {n} points with noise\")\n",
        "print(f\"  Model: y = mx + b\")\n",
        "print(f\"  Loss: Mean Squared Error\\n\")\n",
        "\n",
        "print(\"True parameters:\")\n",
        "print(f\"  m = {true_m}, b = {true_b}\")\n",
        "\n",
        "print(\"\\nOptimized parameters:\")\n",
        "print(f\"  m = {m_opt:.4f}\")\n",
        "print(f\"  b = {b_opt:.4f}\")\n",
        "print(f\"  MSE = {mse_opt:.4f}\")\n",
        "print(f\"  Iterations: {result.nit}\")\n",
        "\n",
        "# Compare with numpy's polyfit (analytical solution)\n",
        "m_np, b_np = np.polyfit(x_data, y_data, 1)\n",
        "print(f\"\\nNumPy polyfit (analytical):\")\n",
        "print(f\"  m = {m_np:.4f}, b = {b_np:.4f}\")\n",
        "print(f\"  Match: {np.allclose([m_opt, b_opt], [m_np, b_np])}\")\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.scatter(x_data, y_data, alpha=0.6, s=50, label='Data')\n",
        "x_line = np.linspace(0, 10, 100)\n",
        "plt.plot(x_line, true_m*x_line + true_b, 'g--', linewidth=2, \n",
        "         label=f'True: y = {true_m}x + {true_b}')\n",
        "plt.plot(x_line, m_opt*x_line + b_opt, 'r-', linewidth=2,\n",
        "         label=f'Fitted: y = {m_opt:.2f}x + {b_opt:.2f}')\n",
        "plt.xlabel('x', fontsize=13)\n",
        "plt.ylabel('y', fontsize=13)\n",
        "plt.title('Linear Regression via scipy.optimize', fontsize=14)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nOptimization matches analytical solution!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real Example: Logistic Regression (Classification)\n",
        "\n",
        "**Problem**: Binary classification with logistic function\n",
        "\n",
        "**Model**: \\( \\hat{y} = \\sigma(w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}} \\)\n",
        "\n",
        "**Loss (Cross-entropy)**:\n",
        "\\[ L(w, b) = -\\frac{1}{n}\\sum [y_i\\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)] \\]\n",
        "\n",
        "**Goal**: Find optimal weights w and bias b\n",
        "\n",
        "**Application**: Medical diagnosis, spam detection, credit scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate binary classification data\n",
        "np.random.seed(42)\n",
        "n = 100\n",
        "X = np.random.randn(n, 2)  # 2 features\n",
        "true_w = np.array([2.0, -1.5])\n",
        "true_b = 0.5\n",
        "y = (X @ true_w + true_b + np.random.randn(n)*0.5 > 0).astype(float)\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # Clip for numerical stability\n",
        "\n",
        "# Cross-entropy loss\n",
        "def cross_entropy_loss(params):\n",
        "    w = params[:-1]\n",
        "    b = params[-1]\n",
        "    z = X @ w + b\n",
        "    y_pred = sigmoid(z)\n",
        "    # Add small epsilon to avoid log(0)\n",
        "    epsilon = 1e-10\n",
        "    loss = -np.mean(y * np.log(y_pred + epsilon) + \n",
        "                    (1 - y) * np.log(1 - y_pred + epsilon))\n",
        "    return loss\n",
        "\n",
        "# Gradient\n",
        "def cross_entropy_gradient(params):\n",
        "    w = params[:-1]\n",
        "    b = params[-1]\n",
        "    z = X @ w + b\n",
        "    y_pred = sigmoid(z)\n",
        "    error = y_pred - y\n",
        "    dw = X.T @ error / n\n",
        "    db = np.mean(error)\n",
        "    return np.concatenate([dw, [db]])\n",
        "\n",
        "# Initial guess\n",
        "params0 = np.zeros(3)  # [w1, w2, b]\n",
        "\n",
        "# Optimize\n",
        "result = optimize.minimize(cross_entropy_loss, params0, \n",
        "                          jac=cross_entropy_gradient, method='BFGS')\n",
        "\n",
        "w_opt = result.x[:-1]\n",
        "b_opt = result.x[-1]\n",
        "\n",
        "# Accuracy\n",
        "y_pred = sigmoid(X @ w_opt + b_opt) > 0.5\n",
        "accuracy = np.mean(y_pred == y) * 100\n",
        "\n",
        "print(\"Logistic Regression (Binary Classification)\")\n",
        "print(f\"  Data: {n} samples, 2 features\")\n",
        "print(f\"  Model: \u03c3(w\u00b7x + b)\")\n",
        "print(f\"  Loss: Cross-entropy\\n\")\n",
        "\n",
        "print(\"True parameters:\")\n",
        "print(f\"  w = {true_w}\")\n",
        "print(f\"  b = {true_b:.2f}\")\n",
        "\n",
        "print(\"\\nOptimized parameters:\")\n",
        "print(f\"  w = {w_opt}\")\n",
        "print(f\"  b = {b_opt:.4f}\")\n",
        "print(f\"  Loss = {result.fun:.4f}\")\n",
        "print(f\"  Accuracy = {accuracy:.1f}%\")\n",
        "print(f\"  Iterations: {result.nit}\")\n",
        "\n",
        "# Visualize decision boundary\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', alpha=0.6, s=50, label='Class 0')\n",
        "plt.scatter(X[y==1, 0], X[y==1, 1], c='red', alpha=0.6, s=50, label='Class 1')\n",
        "\n",
        "# Decision boundary: w1*x1 + w2*x2 + b = 0\n",
        "x1_boundary = np.linspace(X[:, 0].min()-1, X[:, 0].max()+1, 100)\n",
        "x2_boundary = -(w_opt[0]*x1_boundary + b_opt) / w_opt[1]\n",
        "plt.plot(x1_boundary, x2_boundary, 'g-', linewidth=3, \n",
        "         label='Decision boundary')\n",
        "\n",
        "plt.xlabel('Feature 1', fontsize=13)\n",
        "plt.ylabel('Feature 2', fontsize=13)\n",
        "plt.title(f'Logistic Regression: {accuracy:.1f}% Accuracy', fontsize=14)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nSuccessfully trained logistic regression classifier!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Methods Comparison\n",
        "\n",
        "**scipy.optimize.minimize()** supports many methods:\n",
        "\n",
        "| Method | Gradient | Hessian | Speed | Use When |\n",
        "|--------|----------|---------|-------|----------|\n",
        "| **BFGS** | Yes | Approx | Fast | General purpose, medium size |\n",
        "| **L-BFGS-B** | Yes | Approx | Fast | Large scale, with bounds |\n",
        "| **Newton-CG** | Yes | Yes | V. Fast | Hessian available |\n",
        "| **CG** | Yes | No | Medium | No Hessian, large scale |\n",
        "| **Nelder-Mead** | No | No | Slow | Noisy, non-smooth |\n",
        "| **Powell** | No | No | Medium | Derivative-free |\n",
        "\n",
        "**Recommendations**:\n",
        "- **Default**: BFGS (balanced, reliable)\n",
        "- **Large problems**: L-BFGS-B (memory efficient)\n",
        "- **With bounds**: L-BFGS-B, TNC\n",
        "- **No gradient**: Nelder-Mead, Powell\n",
        "- **Fastest**: Newton-CG (if Hessian available)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare methods on Rosenbrock\n",
        "print(\"Method Comparison on Rosenbrock Function\\n\")\n",
        "print(f\"{'Method':<15} {'Min':<20} {'f(x)':<12} {'nfev':<8} {'nit':<6}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "x0 = np.array([0.0, 0.0])\n",
        "methods = ['BFGS', 'L-BFGS-B', 'CG', 'Nelder-Mead', 'Powell']\n",
        "\n",
        "for method in methods:\n",
        "    if method in ['BFGS', 'CG']:\n",
        "        result = optimize.minimize(rosenbrock, x0, jac=rosenbrock_grad, method=method)\n",
        "    else:\n",
        "        result = optimize.minimize(rosenbrock, x0, method=method)\n",
        "    \n",
        "    x_str = f\"({result.x[0]:.4f}, {result.x[1]:.4f})\"\n",
        "    print(f\"{method:<15} {x_str:<20} {result.fun:<12.6f} {result.nfev:<8} {result.nit:<6}\")\n",
        "\n",
        "print(\"\\nAll methods converge to (1, 1), but with different efficiency!\")\n",
        "print(\"BFGS/L-BFGS-B: Best balance of speed and accuracy\")\n",
        "print(\"Nelder-Mead: Slowest but needs no gradient\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Function:\n",
        "\n",
        "```python\n",
        "result = optimize.minimize(\n",
        "    fun=f,              # Objective function\n",
        "    x0=x0,              # Initial guess\n",
        "    method='BFGS',      # Optimization method\n",
        "    jac=gradient,       # Gradient (optional but recommended)\n",
        "    options={'maxiter': 1000}  # Algorithm options\n",
        ")\n",
        "\n",
        "# Access results\n",
        "result.x        # Optimal point\n",
        "result.fun      # Minimum value\n",
        "result.success  # Convergence flag\n",
        "result.nfev     # Function evaluations\n",
        "```\n",
        "\n",
        "### Best Practices:\n",
        "\n",
        "\u2713 **Provide gradient** when possible (2-5x faster)  \n",
        "\u2713 **BFGS** is best default choice  \n",
        "\u2713 **L-BFGS-B** for large problems or with bounds  \n",
        "\u2713 **Good initial guess** helps convergence  \n",
        "\u2713 **Scale variables** to similar magnitude  \n",
        "\n",
        "### Applications:\n",
        "\n",
        "- **Machine Learning**: Train models (regression, classification)\n",
        "- **Deep Learning**: Optimize network weights\n",
        "- **Statistics**: Maximum likelihood estimation\n",
        "- **Finance**: Portfolio optimization, risk management\n",
        "- **Engineering**: Design optimization, control systems\n",
        "\n",
        "### Common Issues:\n",
        "\n",
        "\u26a0\ufe0f **Local minima**: May not find global minimum  \n",
        "\u26a0\ufe0f **Poor scaling**: Normalize features  \n",
        "\u26a0\ufe0f **Bad initial guess**: Try multiple starting points  \n",
        "\u26a0\ufe0f **Numerical instability**: Check gradient correctness  \n",
        "\n",
        "### Next:\n",
        "\n",
        "- **Constrained optimization**: Add constraints g(x) \u2264 0\n",
        "- **Global optimization**: Find global minimum\n",
        "- **Specialized**: Trust region, SQP methods"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}